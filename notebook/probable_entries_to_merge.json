[
    {
        "id": "HYPERPARAMETER_epoch",
        "name": "epoch",
        "also_known_as": [
            "Number of iterations"
        ],
        "description": "An epoch describes the number of times the algorithm sees the entire data set. So, each time the algorithm has seen all samples in the dataset, an epoch has been completed.",
        "type": "Hyperparameter",
        "probable_wikipedia": "Epoch",
        "score": "8.283784",
        "probable_description": " An epoch, for the purposes of chronology and periodization, is an instant in time chosen as the origin of a particular calendar era. The \"epoch\" serves as a reference point from which time is measured.  The moment of epoch is usually decided by congruity, or by following conventions understood from the epoch in question. The epoch moment or date is usually defined from a specific, clear event of change, \"epoch event\". In a more gradual change, a deciding moment is chosen when the \"epoch criterion\" was reached. "
    },
    {
        "id": "HYPERPARAMETER_Learning_rate",
        "name": "learning rate",
        "description": "In machine learning and statistics, the learning rate is a tuning parameter in an optimization algorithm that determines the step size at each iteration while moving toward a minimum of a loss function.",
        "wikipedia": "Learning_rate",
        "type": "Hyperparameter",
        "probable_wikipedia": "Learning rate",
        "score": "12.126115",
        "probable_description": " The learning rate or \"step size\" in machine learning is a hyperparameter which determines to what extent newly acquired information overrides old information. The learning rate is often denoted by the character \u03b7 or \u03b1. A too high learning rate will make the learning jump over minima but a too low learning rate will either take too long to converge or get stuck in an undesirable local minima. Constant learning rates are always smaller than 1 as otherwise the learning will not converge and typically take on values ranging from 0.0001 to 0.4 but this is highly dependent upon the problem at hand. In order to achieve faster convergence, prevent oscillations and getting stuck in undesirable local minima the learning rate is often varied during training either in accordance to a learning rate schedule or by using an adaptive learning rate. "
    },
    {
        "id": "METHOD_weight-decay",
        "name": "Weight Decay",
        "full_name": "Weight Decay",
        "description": "Weight Decay, or $L_{2}$ Regularization, is a regularization technique applied to the weights of a neural network. We minimize a loss function compromising both the primary loss function and a penalty on the $L_{2}$ Norm of the weights:\n$$L_{new}\\left(w\\right) = L_{original}\\left(w\\right) + \\lambda{w^{T}w}$$\nwhere $\\lambda$ is a value determining the strength of the penalty (encouraging smaller weights). \nWeight decay can be incorporated directly into the weight update rule, rather than just implicitly by defining it through to objective function. Often weight decay refers to the implementation where we specify it directly in the weight update rule (whereas L2 regularization is usually the implementation which is specified in the objective function).\nImage Source: Deep Learning, Goodfellow et al",
        "paper": null,
        "category": [
            "Parameter Norm Penalties",
            "Regularization"
        ],
        "type": "Method",
        "probable_wikipedia": "Tikhonov regularization",
        "score": "0.4231091",
        "probable_description": " Tikhonov regularization, named for Andrey Tikhonov, is the most commonly used method of regularization of ill-posed problems. In statistics, the method is known as ridge regression, in machine learning it is known as weight decay, and with multiple independent discoveries, it is also variously known as the Tikhonov\u2013Miller method, the Phillips\u2013Twomey method, the constrained linear inversion method, and the method of linear regularization. It is related to the Levenberg\u2013Marquardt algorithm for non-linear least-squares problems.  Suppose that for a known matrix formula_1 and vector formula_2, we wish to find a vector formula_3 such that The standard approach is ordinary least squares linear regression. However, if no formula_3 satisfies the equation or more than one formula_3 does\u2014that is, the solution is not unique\u2014the problem is said to be ill posed. In such cases, ordinary least squares estimation leads to an overdetermined, or more often an underdetermined system of equations. Most real-world phenomena have the effect of low-pass filters in the forward direction where formula_1 maps formula_3 to formula_2. Therefore, in solving the inverse-problem, the inverse mapping operates as a high-pass filter that has the undesirable tendency of amplifying noise (eigenvalues / singular values are largest in the reverse mapping where they were smallest in the forward mapping). In addition, ordinary least squares implicitly nullifies every element of the reconstructed version of formula_3 that is in the null-space of formula_1, rather than allowing for a model to be used as a prior for formula_3. Ordinary least squares seeks to minimize the sum of squared residuals, which can"
    },
    {
        "id": "HYPERPARAMETER_cost-function",
        "name": "cost function",
        "also_known_as": [
            "loss function"
        ],
        "description": "The cost function is the technique of evaluating \u201cthe performance of our algorithm/model\u201d. It takes both predicted outputs by the model and actual outputs and calculates how much wrong the model was in its prediction.",
        "type": "Hyperparameter",
        "probable_wikipedia": "Cost function",
        "score": "6.303031",
        "probable_description": " Cost function   "
    },
    {
        "id": "METHOD_adapter",
        "name": "Adapter",
        "full_name": "Adapter",
        "description": "We introduce Trankit, a light-weight Transformer-based Toolkit for multilingual Natural Language Processing (NLP). It provides a trainable pipeline for fundamental NLP tasks over 100 languages, and 90 pretrained pipelines for 56 languages. Built on a state-of-the-art pretrained language model, Trankit significantly outperforms prior multilingual NLP pipelines over sentence segmentation, part-of-speech tagging, morphological feature tagging, and dependency parsing while maintaining competitive performance for tokenization, multi-word token expansion, and lemmatization over 90 Universal Dependencies treebanks. Despite the use of a large pretrained transformer, our toolkit is still efficient in memory usage and speed. This is achieved by our novel plug-and-play mechanism with Adapters where a multilingual pretrained transformer is shared across pipelines for different languages. Our toolkit along with pretrained models and code are publicly available at: https://github.com/nlp-uoregon/trankit. A demo website for our toolkit is also available at: http://nlp.uoregon.edu/trankit. Finally, we create a demo video for Trankit at: https://youtu.be/q0KGP3zGjGc.",
        "category": [
            "Feedforward Networks"
        ],
        "proposed_in": {
            "paper_id": "trankit-a-light-weight-transformer-based",
            "s2_paper_id": "b53c386b7c65af80905dc05a9b27e98e03324739"
        },
        "type": "Method",
        "probable_wikipedia": "Adapter",
        "score": "9.715766",
        "probable_description": " An adapter or adaptor is a device that converts attributes of one device or system to those of an otherwise incompatible device or system. Some modify power or signal attributes, while others merely adapt the physical form of one connector to another. "
    },
    {
        "id": "METHOD_ae",
        "name": "AE",
        "full_name": "Autoencoders",
        "description": "An autoencoder is a type of artificial neural network used to learn efficient data codings in an unsupervised manner. The aim of an autoencoder is to learn a representation (encoding) for a set of data, typically for dimensionality reduction, by training the network to ignore signal \u201cnoise\u201d. Along with the reduction side, a reconstructing side is learnt, where the autoencoder tries to generate from the reduced encoding a representation as close as possible to its original input, hence its name. \nExtracted from: Wikipedia\nImage source: Wikipedia",
        "paper": null,
        "category": [
            "Dimensionality Reduction"
        ],
        "type": "Method",
        "probable_wikipedia": "Autoencoder",
        "score": "12.342483",
        "probable_description": " An autoencoder is a type of artificial neural network used to learn efficient data codings in an unsupervised manner. The aim of an autoencoder is to learn a representation (encoding) for a set of data, typically for dimensionality reduction, by training the network to ignore signal \u201cnoise\u201d. Along with the reduction side, a reconstructing side is learnt, where the autoencoder tries to generate from the reduced encoding a representation as close as possible to its original input, hence its name. Several variants exist to the basic model, with the aim of forcing the learned representations of the input to assume useful properties. Examples are the regularized autoencoders (\"Sparse\", \"Denoising\" and \"Contractive\" autoencoders), proven effective in learning representations for subsequent classification tasks, and \"Variational\" autoencoders, with their recent applications as generative models. "
    },
    {
        "id": "METHOD_alexnet",
        "name": "AlexNet",
        "full_name": "AlexNet",
        "description": "AlexNet is a classic convolutional neural network architecture. It consists of convolutions, max pooling and dense layers as the basic building blocks. Grouped convolutions are used in order to fit the model across two GPUs.",
        "category": [
            "Convolutional Neural Networks",
            "Image Models"
        ],
        "proposed_in": {
            "paper_id": "imagenet-classification-with-deep",
            "s2_paper_id": "abd1c342495432171beb7ca8fd9551ef13cbd0ff"
        },
        "type": "Method",
        "probable_wikipedia": "AlexNet",
        "score": "10.805111",
        "probable_description": " AlexNet is the name of a convolutional neural network, designed by Alex Krizhevsky, and published with Ilya Sutskever and Krizhevsky's PhD advisor Geoffrey Hinton, who was originally resistant to the idea of his student.  AlexNet competed in the ImageNet Large Scale Visual Recognition Challenge on September 30, 2012. The network achieved a top-5 error of 15.3%, more than 10.8 percentage points lower than that of the runner up. The original paper's primary result was that the depth of the model was essential for its high performance, which was computationally expensive, but made feasible due to the utilization of graphics processing units (GPUs) during training. "
    },
    {
        "id": "METHOD_alphazero",
        "name": "AlphaZero",
        "full_name": "AlphaZero",
        "description": "AlphaZero is a reinforcement learning agent for playing board games such as Go, chess, and shogi. ",
        "category": [
            "Board Game Models"
        ],
        "proposed_in": {
            "paper_id": "mastering-chess-and-shogi-by-self-play-with-a",
            "s2_paper_id": "38fb1902c6a2ab4f767d4532b28a92473ea737aa"
        },
        "type": "Method",
        "probable_wikipedia": "AlphaZero",
        "score": "12.397048",
        "probable_description": " AlphaZero is a computer program developed by artificial intelligence research company DeepMind to master the games of chess, shogi and go. The algorithm uses an approach similar to AlphaGo Zero. On December 5, 2017, the DeepMind team released a preprint introducing AlphaZero, which within 24 hours achieved a superhuman level of play in these three games by defeating world-champion programs, Stockfish, elmo, and the 3-day version of AlphaGo Zero. In each case it made use of custom tensor processing units (TPUs) that the Google programs were optimized to use. AlphaZero was trained solely via \"self-play\" using 5,000 first-generation TPUs to generate the games and 64 second-generation TPUs to train the neural networks, all in parallel, with no access to opening books or endgame tables. After four hours of training, DeepMind estimated AlphaZero was playing at a higher Elo rating than Stockfish 8; after 9 hours of training, the algorithm decisively defeated Stockfish 8 in a time-controlled 100-game tournament (28 wins, 0 losses, and 72 draws). The trained algorithm played on a single machine with four TPUs. DeepMind's paper on AlphaZero was published in the journal \"Science\" on 7 December 2018. "
    },
    {
        "id": "METHOD_alternet",
        "name": "AlterNet",
        "full_name": "AlterNet",
        "description": "The success of multi-head self-attentions (MSAs) for computer vision is now indisputable. However, little is known about how MSAs work. We present fundamental explanations to help better understand the nature of MSAs. In particular, we demonstrate the following properties of MSAs and Vision Transformers (ViTs): 1 MSAs improve not only accuracy but also generalization by flattening the loss landscapes. Such improvement is primarily attributable to their data specificity, not long-range dependency. On the other hand, ViTs suffer from non-convex losses. Large datasets and loss landscape smoothing methods alleviate this problem; 2 MSAs and Convs exhibit opposite behaviors. For example, MSAs are low-pass filters, but Convs are high-pass filters. Therefore, MSAs and Convs are complementary; 3 Multi-stage neural networks behave like a series connection of small individual models. In addition, MSAs at the end of a stage play a key role in prediction. Based on these insights, we propose AlterNet, a model in which Conv blocks at the end of a stage are replaced with MSA blocks. AlterNet outperforms CNNs not only in large data regimes but also in small data regimes.",
        "category": [
            "Convolutional Neural Networks",
            "Image Models"
        ],
        "proposed_in": {
            "paper_id": "how-do-vision-transformers-work-1",
            "s2_paper_id": "c5e658f9b4606e274946fdff4e1f9e90700fdd47"
        },
        "type": "Method",
        "probable_wikipedia": "AlterNet",
        "score": "8.742052",
        "probable_description": " AlterNet is a politically left-leaning website that was launched in 1998 by the non-profit now known as the Independent Media Institute. In 2018, the website was acquired by owners of The Raw Story. Some AlterNet content is republished on \"Salon\". "
    },
    {
        "id": "METHOD_autoencoder",
        "name": "AutoEncoder",
        "full_name": "AutoEncoder",
        "description": "An Autoencoder is a bottleneck architecture that turns a high-dimensional input into a latent low-dimensional code (encoder), and then performs a reconstruction of the input with this latent code (the decoder).\nImage: Michael Massi",
        "category": [
            "Generative Models"
        ],
        "proposed_in": {
            "paper_id": "reducing-the-dimensionality-of-data-with",
            "s2_paper_id": "46eb79e5eec8a4e2b2f5652b66441e8a4c921c3e"
        },
        "type": "Method",
        "probable_wikipedia": "Autoencoder",
        "score": "11.268999",
        "probable_description": " An autoencoder is a type of artificial neural network used to learn efficient data codings in an unsupervised manner. The aim of an autoencoder is to learn a representation (encoding) for a set of data, typically for dimensionality reduction, by training the network to ignore signal \u201cnoise\u201d. Along with the reduction side, a reconstructing side is learnt, where the autoencoder tries to generate from the reduced encoding a representation as close as possible to its original input, hence its name. Several variants exist to the basic model, with the aim of forcing the learned representations of the input to assume useful properties. Examples are the regularized autoencoders (\"Sparse\", \"Denoising\" and \"Contractive\" autoencoders), proven effective in learning representations for subsequent classification tasks, and \"Variational\" autoencoders, with their recent applications as generative models. "
    },
    {
        "id": "METHOD_base",
        "name": "BASE",
        "full_name": "Balanced Selection",
        "description": "Active learning (AL) algorithms aim to identify an optimal subset of data for annotation, such that deep neural networks (DNN) can achieve better performance when trained on this labeled subset. AL is especially impactful in industrial scale settings where data labeling costs are high and practitioners use every tool at their disposal to improve model performance. The recent success of self-supervised pretraining (SSP) highlights the importance of harnessing abundant unlabeled data to boost model performance. By combining AL with SSP, we can make use of unlabeled data while simultaneously labeling and training on particularly informative samples. In this work, we study a combination of AL and SSP on ImageNet. We find that performance on small toy datasets \u2013 the typical benchmark setting in the literature \u2013 is not representative of performance on ImageNet due to the class imbalanced samples selected by an active learner. Among the existing baselines we test, popular AL algorithms across a variety of small and large scale settings fail to outperform random sampling. To remedy the class-imbalance problem, we propose Balanced Selection (BASE), a simple, scalable AL algorithm that outperforms random sampling consistently by selecting more balanced samples for annotation than existing methods. Our code is available at: https://github.com/zeyademam/active learning.",
        "category": [
            "Active Learning"
        ],
        "proposed_in": {
            "paper_id": "active-learning-at-the-imagenet-scale",
            "s2_paper_id": "a4cf16a6f22692aae247f38aa9470b467796cf02"
        },
        "type": "Method",
        "probable_wikipedia": "Balancing selection",
        "score": "3.0148757",
        "probable_description": " Balancing selection refers to a number of selective processes by which multiple alleles (different versions of a gene) are actively maintained in the gene pool of a population at frequencies larger than expected from genetic drift alone. This can happen by various mechanisms, in particular, when the heterozygotes for the alleles under consideration have a higher fitness than the homozygote. In this way genetic polymorphism is conserved. Evidence for balancing selection can be found in the number of alleles in a population which are maintained above mutation rate frequencies. All modern research has shown that this significant genetic variation is ubiquitous in panmictic populations.  There are several mechanisms (which are not exclusive within any given population) by which balancing selection works to maintain polymorphism. The two major and most studied are heterozygote advantage and frequency-dependent selection. "
    },
    {
        "id": "METHOD_batch-normalization",
        "name": "Batch Normalization",
        "full_name": "Batch Normalization",
        "description": "Batch Normalization aims to reduce internal covariate shift, and in doing so aims to accelerate the training of deep neural nets. It accomplishes this via a normalization step that fixes the means and variances of layer inputs. Batch Normalization also has a beneficial effect on the gradient flow through the network, by reducing the dependence of gradients on the scale of the parameters or of their initial values. This allows for use of much higher learning rates without the risk of divergence. Furthermore, batch normalization regularizes the model and reduces the need for Dropout.\nWe apply a batch normalization layer as follows for a minibatch $\\mathcal{B}$:\n$$ \\mu_{\\mathcal{B}} = \\frac{1}{m}\\sum^{m}_{i=1}x_{i} $$\n$$ \\sigma^{2}_{\\mathcal{B}} = \\frac{1}{m}\\sum^{m}_{i=1}\\left(x_{i}-\\mu_{\\mathcal{B}}\\right)^{2} $$\n$$ \\hat{x}_{i} = \\frac{x_{i} - \\mu_{\\mathcal{B}}}{\\sqrt{\\sigma^{2}_{\\mathcal{B}}+\\epsilon}} $$\n$$ y_{i} = \\gamma\\hat{x}_{i} + \\beta = \\text{BN}_{\\gamma, \\beta}\\left(x_{i}\\right) $$\nWhere $\\gamma$ and $\\beta$ are learnable parameters.",
        "category": [
            "Normalization"
        ],
        "proposed_in": {
            "paper_id": "batch-normalization-accelerating-deep-network",
            "s2_paper_id": "4d376d6978dad0374edfa6709c9556b42d3594d3"
        },
        "type": "Method",
        "probable_wikipedia": "Batch normalization",
        "score": "12.107431",
        "probable_description": " Batch normalization is a technique for improving the speed, performance, and stability of artificial neural networks. Batch normalization was introduced in a 2015 paper. It is used to normalize the input layer by adjusting and scaling the activations.  While the effect of batch normalization is evident, the reasons behind its effectiveness remain under discussion. It was believed that it can mitigate the problem of internal covariate shift, where parameter initialization and changes in the distribution of the inputs of each layer affects the learning rate of the network. Recently, some scholars have shown that batch normalization does not reduce internal covariate shift, but rather smooths the objective function to improve the performance. Others prove that batch normalization achieves length-direction decoupling, and thereby accelerates neural networks. "
    },
    {
        "id": "METHOD_blender",
        "name": "Blender",
        "full_name": "Blender",
        "description": "Blender is a proposal-based instance mask generation module which incorporates rich instance-level information with accurate dense pixel features. A single convolution layer is added on top of the detection towers to produce attention masks along with each bounding box prediction. For each predicted instance, the blender crops predicted bases with its bounding box and linearly combines them according the learned attention maps.\nThe inputs of the blender module are bottom-level bases $\\mathbf{B}$, the selected top-level attentions $A$ and bounding box proposals $P$. First RoIPool of Mask R-CNN to crop bases with each proposal $\\mathbf{p}_{d}$ and then resize the region to a fixed size $R \\times R$ feature map $\\mathbf{r}_{d}$\n$$\n\\mathbf{r}_{d}=\\operatorname{RoIPool}_{R \\times R}\\left(\\mathbf{B}, \\mathbf{p}_{d}\\right), \\quad \\forall d \\in{1 \\ldots D}\n$$\nMore specifically,  asampling ratio 1 is used for RoIAlign, i.e. one bin for each sampling point. During training, ground truth boxes are used as the proposals. During inference, FCOS prediction results are used.\nThe attention size $M$ is smaller than $R$. We interpolate $\\mathbf{a}_{d}$ from $M \\times M$ to $R \\times R$, into the shapes of $R=\\left(\\mathbf{r}_{d} \\mid d=1 \\ldots D\\right)$\n$$\n\\mathbf{a}_{d}^{\\prime}=\\text { interpolate }_{M \\times M \\rightarrow R \\times R}\\left(\\mathbf{a}_{d}\\right), \\quad \\forall d \\in{1 \\ldots D}\n$$\nThen $\\mathbf{a}_{d}^{\\prime}$ is normalized with a softmax function along the $K$ dimension to make it a set of score maps $\\mathbf{s}_{d}$.\n$$\n\\mathbf{s}_{d}=\\operatorname{softmax}\\left(\\mathbf{a}_{d}^{\\prime}\\right), \\quad \\forall d \\in{1 \\ldots D}\n$$\nThen we apply element-wise product between each entity $\\mathbf{r}_{d}, \\mathbf{s}_{d}$ of the regions $R$ and scores $S$, and sum along the $K$ dimension to get our mask logit $\\mathbf{m}_{d}:$\n$$\n\\mathbf{m}_{d}=\\sum_{k=1}^{K} \\mathbf{s}_{d}^{k} \\circ \\mathbf{r}_{d}^{k}, \\quad \\forall d \\in{1 \\ldots D}\n$$\nwhere $k$ is the index of the basis. The mask blending process with $K=4$ is visualized in the Figure.",
        "category": [
            "Instance Segmentation Modules"
        ],
        "proposed_in": {
            "paper_id": "blendmask-top-down-meets-bottom-up-for",
            "s2_paper_id": "53ab5246a8c1ce99198c7f6184b81a6f8b4b16fe"
        },
        "type": "Method",
        "probable_wikipedia": "Blender (software)",
        "score": "8.567834",
        "probable_description": " Blender is a free and open-source 3D computer graphics software toolset used for creating animated films, visual effects, art, 3D printed models, interactive 3D applications and video games. Blender's features include 3D modeling, UV unwrapping, texturing, raster graphics editing, rigging and skinning, fluid and smoke simulation, particle simulation, soft body simulation, sculpting, animating, match moving, rendering, motion graphics, video editing and compositing.  While previous versions also featured an integrated game engine, it was removed with the release of version 2.80 in 2019. "
    },
    {
        "id": "METHOD_bpe",
        "name": "BPE",
        "full_name": "Byte Pair Encoding",
        "description": "Byte Pair Encoding, or BPE, is a subword segmentation algorithm that encodes rare and unknown words as sequences of subword units. The intuition is that various word classes are translatable via smaller units than words, for instance names (via character copying or transliteration), compounds (via compositional translation), and cognates and loanwords (via phonological and morphological transformations).\nLei Mao has a detailed blog post that explains how this works.",
        "category": [
            "Subword Segmentation"
        ],
        "proposed_in": {
            "paper_id": "neural-machine-translation-of-rare-words-with",
            "s2_paper_id": "1af68821518f03568f913ab03fc02080247a27ff"
        },
        "type": "Method",
        "probable_wikipedia": "Byte pair encoding",
        "score": "12.43847",
        "probable_description": " Byte pair encoding or digram coding is a simple form of data compression in which the most common pair of consecutive bytes of data is replaced with a byte that does not occur within that data. A table of the replacements is required to rebuild the original data. The algorithm was first described publicly by Philip Gage in a February 1994 article \"A New Algorithm for Data Compression\" in the \"C Users Journal\".  A variant of the technique has shown to be useful in several natural language processing applications. "
    },
    {
        "id": "METHOD_canine",
        "name": "CANINE",
        "full_name": "CANINE",
        "description": "CANINE is a pre-trained encoder for language understanding that operates directly on character sequences\u2014without explicit tokenization or vocabulary\u2014and a pre-training strategy with soft inductive biases in place of hard token boundaries. To use its finer-grained input effectively and efficiently, Canine combines downsampling, which reduces the input sequence length, with a deep transformer stack, which encodes context.",
        "category": [
            "Language Models"
        ],
        "proposed_in": {
            "paper_id": "canine-pre-training-an-efficient-tokenization",
            "s2_paper_id": "e02f79b710cdcaa9135b835fad964f6f2c78b1a7"
        },
        "type": "Method",
        "probable_wikipedia": "Dog",
        "score": "0.07094853",
        "probable_description": " The domestic dog (\"Canis lupus familiaris\" when considered a subspecies of the wolf or \"Canis familiaris\" when considered a distinct species) is a member of the genus \"Canis\" (canines), which forms part of the wolf-like canids, and is the most widely abundant terrestrial carnivore. The dog and the extant gray wolf are sister taxa as modern wolves are not closely related to the wolves that were first domesticated, which implies that the direct ancestor of the dog is extinct. The dog was the first species to be domesticated and has been selectively bred over millennia for various behaviors, sensory capabilities, and physical attributes.  Their long association with humans has led dogs to be uniquely attuned to human behavior and they are able to thrive on a starch-rich diet that would be inadequate for other canid species. Dogs vary widely in shape, size and colors. They perform many roles for humans, such as hunting, herding, pulling loads, protection, assisting police and military, companionship and, more recently, aiding disabled people and therapeutic roles. This influence on human society has given them the sobriquet of \"man's best friend\".  "
    },
    {
        "id": "METHOD_carafe",
        "name": "CARAFE",
        "full_name": "CARAFE",
        "description": "Content-Aware ReAssembly of FEatures (CARAFE) is an operator for feature upsampling in convolutional neural networks. CARAFE has several appealing properties: (1) Large field of view. Unlike previous works (e.g. bilinear interpolation) that only exploit subpixel neighborhood, CARAFE can aggregate contextual information within a large receptive field. (2) Content-aware handling. Instead of using a fixed kernel for all samples (e.g. deconvolution), CARAFE enables instance-specific content-aware handling, which generates adaptive kernels on-the-fly. (3) Lightweight and fast to compute.",
        "category": [
            "Feature Upsampling"
        ],
        "proposed_in": {
            "paper_id": "carafe-content-aware-reassembly-of-features",
            "s2_paper_id": "477dbe3139216d5d1af7c0a379ed32a7a4dc4ba7"
        },
        "type": "Method",
        "probable_wikipedia": "Carafe",
        "score": "4.89985",
        "probable_description": " A carafe is a glass container without handles used for serving wine and other drinks. Unlike the related decanter, carafes do not include stoppers.  Coffee pots included in coffee makers are also referred to as carafes in American English.  In France, carafes are commonly used to serve water. To order a \"carafe d'eau\" (\"carafe of water\") is to request to be served (free) tap water rather than bottled water at a cost.  "
    },
    {
        "id": "METHOD_causal-inference",
        "name": "Causal Inference",
        "full_name": "Causal Inference",
        "description": "Causal inference is the process of drawing a conclusion about a causal connection based on the conditions of the occurrence of an effect. The main difference between causal inference and inference of association is that the former analyzes the response of the effect variable when the cause is changed.",
        "paper": null,
        "type": "Method",
        "probable_wikipedia": "Causal inference",
        "score": "12.58503",
        "probable_description": " Causal inference is the process of drawing a conclusion about a causal connection based on the conditions of the occurrence of an effect. The main difference between causal inference and inference of association is that the former analyzes the response of the effect variable when the cause is changed. The science of why things occur is called etiology. Causal inference is an example of causal reasoning. "
    },
    {
        "id": "METHOD_clarinet",
        "name": "ClariNet",
        "full_name": "ClariNet",
        "description": "ClariNet is an end-to-end text-to-speech architecture. Unlike previous TTS systems which use text-to-spectogram models with a separate waveform synthesizer (vocoder), ClariNet is a text-to-wave architecture that is fully convolutional and can be trained from scratch. In ClariNet, the WaveNet module is conditioned on the hidden states instead of the mel-spectogram. The architecture is otherwise based on Deep Voice 3.",
        "category": [
            "Sequence To Sequence Models"
        ],
        "proposed_in": {
            "paper_id": "clarinet-parallel-wave-generation-in-end-to",
            "s2_paper_id": "5e3a59695261f03aa3f09a8a5ac6166fb63e0a2e"
        },
        "type": "Method",
        "probable_wikipedia": "Clarinet",
        "score": "8.751767",
        "probable_description": " The clarinet is a family of woodwind instruments. It has a single-reed mouthpiece, a straight, cylindrical tube with an almost cylindrical bore, and a flared bell. A person who plays a clarinet is called a \"clarinetist\" (sometimes spelled \"clarinettist\").  While the similarity in sound between the earliest clarinets and the trumpet may hold a clue to its name, other factors may have been involved. During the Late Baroque era, composers such as Bach and Handel were making new demands on the skills of their trumpeters, who were often required to play difficult melodic passages in the high, or as it came to be called, \"clarion\" register. Since the trumpets of this time had no valves or pistons, melodic passages would often require the use of the highest part of the trumpet's range, where the harmonics were close enough together to produce scales of adjacent notes as opposed to the gapped scales or arpeggios of the lower register. The trumpet parts that required this specialty were known by the term \"clarino\" and this in turn came to apply to the musicians themselves. It is probable that the term clarinet may stem from the diminutive version of the 'clarion' or 'clarino' and it has been suggested that clarino players may have helped themselves out by playing particularly difficult passages on these newly developed \"mock trumpets\".  Johann Christoph Denner is generally believed to have invented the clarinet in Germany around the year 1700 by adding a register key to the earlier chalumeau. Over time, additional keywork"
    },
    {
        "id": "METHOD_colorization",
        "name": "Colorization",
        "full_name": "Colorization",
        "description": "Colorization is a self-supervision approach that relies on colorization as the pretext task in order to learn image representations.",
        "category": [
            "Self-Supervised Learning"
        ],
        "proposed_in": {
            "paper_id": "colorful-image-colorization",
            "s2_paper_id": "8201e6e687f2de477258e9be53ba7b73ee30d7de"
        },
        "type": "Method",
        "probable_wikipedia": "Film colorization",
        "score": "3.7656262",
        "probable_description": " Film colorization (American English; or colourisation (British English), or colourization (Canadian English)) is any process that adds color to black-and-white, sepia, or other monochrome moving-picture images. It may be done as a special effect, to \"modernize\" black-and-white films, or to restore color films. The first examples date from the early 20th century, but colorization has become common with the advent of digital image processing. "
    },
    {
        "id": "METHOD_composite-fields",
        "name": "Composite Fields",
        "full_name": "Composite Fields",
        "description": "Represent and associate with a composite of primitive fields.",
        "category": [
            "Image Representations"
        ],
        "proposed_in": {
            "paper_id": "pifpaf-composite-fields-for-human-pose",
            "s2_paper_id": "52b377e589220708c4ec8c4b55dda689d1bb00c4"
        },
        "type": "Method",
        "probable_wikipedia": "Composite field",
        "score": "8.223044",
        "probable_description": " In quantum field theory, a composite field is a field defined in terms of other more \"elementary\" fields. It might describe a composite particle (bound state) or it might not. It might be local, or it might be nonlocal. Noether fields are often composite fields and they are local.  In the generalized LSZ formalism, composite fields, which are usually nonlocal, are used to model asymptotic bound states.  "
    },
    {
        "id": "METHOD_convolution",
        "name": "Convolution",
        "full_name": "Convolution",
        "description": "A convolution is a type of matrix operation, consisting of a kernel, a small matrix of weights, that slides over input data performing element-wise multiplication with the part of the input it is on, then summing the results into an output.\nIntuitively, a convolution allows for weight sharing - reducing the number of effective parameters - and image translation (allowing for the same feature to be detected in different parts of the input space).\nImage Source: https://arxiv.org/pdf/1603.07285.pdf",
        "paper": null,
        "category": [
            "Convolutions",
            "Image Feature Extractors"
        ],
        "type": "Method",
        "probable_wikipedia": "Convolution",
        "score": "10.157478",
        "probable_description": " In mathematics (in particular, functional analysis) convolution is a mathematical operation on two functions ( and ) to produce a third function that expresses how the shape of one is modified by the other. The term \"convolution\" refers to both the result function and to the process of computing it. It is defined as the integral of the product of the two functions after one is reversed and shifted.  Some features of convolution are similar to cross-correlation: for real-valued functions, of a continuous or discrete variable, it differs from cross-correlation only in that either or is reflected about the y-axis; thus it is a cross-correlation of and , or and .\u00a0 For continuous functions, the cross-correlation operator is the adjoint of the convolution operator.  Convolution has applications that include probability, statistics, computer vision, natural language processing, image and signal processing, engineering, and differential equations.  The convolution can be defined for functions on Euclidean space, and other groups. For example, periodic functions, such as the discrete-time Fourier transform, can be defined on a circle and convolved by \"periodic convolution\". (See row 18 at DTFT \u00a7 Properties.) A \"discrete convolution\" can be defined for functions on the set of integers.  Generalizations of convolution have applications in the field of numerical analysis and numerical linear algebra, and in the design and implementation of finite impulse response filters in signal processing.  Computing the inverse of the convolution operation is known as deconvolution. "
    },
    {
        "id": "METHOD_coresets",
        "name": "Coresets",
        "full_name": "Coresets",
        "description": "Convolutional neural networks (CNNs) have been successfully applied to many recognition and learning tasks using a universal recipe; training a deep model on a very large dataset of supervised examples. However, this approach is rather restrictive in practice since collecting a large set of labeled images is very expensive. One way to ease this problem is coming up with smart ways for choosing images to be labelled from a very large collection (i.e. active learning) In this paper, we first show that existing active learning heuristics are not effective for CNNs even in an oracle setting. Our counterintuitive empirical results make us question these heuristics and inspire us to come up with a simple but effective method, choosing a set of images to label such that they cover the set of unlabeled images as closely as possible. We further present a theoretical justification for this geometric heuristic by giving a bound over the generalization error of CNNs. Our experiments show that the proposed method significantly outperforms existing approaches in image classification experiments by a large margin.",
        "category": [
            "Clustering"
        ],
        "proposed_in": {
            "paper_id": "active-learning-for-convolutional-neural",
            "s2_paper_id": "82fb7661d892a7412726de6ead14269139d0310c"
        },
        "type": "Method",
        "probable_wikipedia": "Coreset",
        "score": "4.479863",
        "probable_description": " In computational geometry, a coreset is a small set of points that approximates the shape of a larger point set, in the sense that applying some geometric measure to the two sets (such as their minimum bounding box volume) results in approximately equal numbers. Many natural geometric optimization problems have coresets that approximate an optimal solution to within a factor of , that can be found quickly (in linear time or near-linear time), and that have size bounded by a function of independent of the input size, where is an arbitrary positive number. When this is the case, one obtains a linear-time or near-linear time approximation scheme, based on the idea of finding a coreset and then applying an exact optimization algorithm to the coreset. Regardless of how slow the exact optimization algorithm is, for any fixed choice of , the running time of this approximation scheme will be plus the time to find the coreset.  "
    },
    {
        "id": "METHOD_crf",
        "name": "CRF",
        "full_name": "Conditional Random Field",
        "description": "Conditional Random Fields or CRFs are a type of probabilistic graph model that take neighboring sample context into account for tasks like classification. Prediction is modeled as a graphical model, which implements dependencies between the predictions. Graph choice depends on the application, for example linear chain CRFs are popular in natural language processing, whereas in image-based tasks, the graph would connect to neighboring locations in an image to enforce that they have similar predictions.\nImage Credit: Charles Sutton and Andrew McCallum, An Introduction to Conditional Random Fields",
        "paper": null,
        "category": [
            "Structured Prediction"
        ],
        "type": "Method",
        "probable_wikipedia": "Conditional random field",
        "score": "12.426116",
        "probable_description": " Conditional random fields (CRFs) are a class of statistical modeling method often applied in pattern recognition and machine learning and used for structured prediction. CRFs fall into the sequence modeling family. Whereas a discrete classifier predicts a label for a single sample without considering \"neighboring\" samples, a CRF can take context into account; e.g., the linear chain CRF (which is popular in natural language processing) predicts sequences of labels for sequences of input samples.  CRFs are a type of discriminative undirected probabilistic graphical model. They are used to encode known relationships between observations and construct consistent interpretations and are often used for labeling or parsing of sequential data, such as natural language processing or biological sequences and in computer vision. Specifically, CRFs find applications in POS tagging, shallow parsing, named entity recognition, gene finding and peptide critical functional region finding, among other tasks, being an alternative to the related hidden Markov models (HMMs). In computer vision, CRFs are often used for object recognition and image segmentation. "
    },
    {
        "id": "METHOD_crossbow",
        "name": "Crossbow",
        "full_name": "Crossbow",
        "description": "Crossbow is a single-server multi-GPU system for training deep learning models that enables users to freely choose their preferred batch size\u2014however small\u2014while scaling to multiple GPUs. Crossbow uses many parallel model replicas and avoids reduced statistical efficiency through a new synchronous training method. SMA, a synchronous variant of model averaging, is used in which replicas independently explore the solution space with gradient descent, but adjust their search synchronously based on the trajectory of a globally-consistent average model.",
        "category": [
            "Asynchronous Data Parallel",
            "Data Parallel Methods",
            "Distributed Methods"
        ],
        "proposed_in": {
            "paper_id": "crossbow-scaling-deep-learning-with-small",
            "s2_paper_id": "1ca00f6c9359e7fabca4cb60cc2badfeb2d157d6"
        },
        "type": "Method",
        "probable_wikipedia": "Crossbow",
        "score": "7.1990504",
        "probable_description": " A crossbow is a type of elastic ranged weapon in similar principle to a bow, consisting of a bow-like assembly called a \"prod\", mounted horizontally on a main frame called a \"tiller\", which is handheld in a similar fashion to the stock of a long gun. It shoots arrow-like projectiles called bolts or quarrels. The medieval European crossbow was called by many other names including crossbow itself, most of which were derived from the word \"ballista\", an ancient Greek torsion siege engine similar in appearance.  Although having the same launch principle, crossbows differ from bows in that a bow's draw must be maintained manually by the archer pulling the bowstring with fingers, arm and back muscles and holding that same form in order to aim (which demands significant physical strength and stamina), while a crossbow uses a locking mechanism to maintain the draw, limiting the shooter's exertion to only pulling the string into lock and then releasing the shot via depressing a lever/trigger. This not only enables a crossbowman to handle stronger draw weight, but also hold for longer with significant less physical strain, thus capable of better precision.  Historically, crossbows played a significant role in the warfare of East Asia and Medieval Europe. The earliest crossbows in the world were invented in ancient China and caused a major shift in the role of projectile weaponry. The traditional bow and arrow had long been a specialized weapon that required considerable training, physical strength and expertise to operate with any degree of practical efficiency."
    },
    {
        "id": "METHOD_dafne",
        "name": "DAFNe",
        "full_name": "DAFNe",
        "description": "DAFNe is a dense one-stage anchor-free deep model for oriented object detection. It is a deep neural network that performs predictions on a dense grid over the input image, being architecturally simpler in design, as well as easier to optimize than its two-stage counterparts. Furthermore, it reduces the prediction complexity by refraining from employing bounding box anchors. This enables a tighter fit to oriented objects, leading to a better separation of bounding boxes especially in case of dense object distributions. Moreover, it introduces an orientation-aware generalization of the center-ness function to arbitrary quadrilaterals that takes into account the object's orientation and that, accordingly, accurately down-weights low-quality predictions",
        "category": [
            "Oriented Object Detection Models",
            "Object Detection Models"
        ],
        "proposed_in": {
            "paper_id": "dafne-a-one-stage-anchor-free-deep-model-for",
            "s2_paper_id": "0ac63bb32477f27d740d793a625d1064f83a2d60"
        },
        "type": "Method",
        "probable_wikipedia": "DAFNE",
        "score": "5.6296997",
        "probable_description": " DAFNE or DA\u03a6NE (Double Annular \u03a6 Factory for Nice Experiments), is an electron-positron collider at the INFN Frascati National Laboratory in Frascati, Italy. Since 1999 it has been colliding electrons and positrons at a center of mass energy of 1.02\u00a0GeV to create phi mesons (\u03c6). 85% of these decay into kaons (K), whose physics is the subject of most of the experiments at DAFNE.  There are five experiments at DAFNE:   "
    },
    {
        "id": "METHOD_deep-belief-network",
        "name": "DBN",
        "full_name": "Deep Belief Network",
        "description": "A Deep Belief Network (DBN) is a multi-layer generative graphical model. DBNs have bi-directional connections (RBM-type connections) on the top layer while the bottom layers only have top-down connections. They are trained using layerwise pre-training. Pre-training occurs by training the network component by component bottom up: treating the first two layers as an RBM and training, then treating the second layer and third layer as another RBM and training for those parameters.\nSource: Origins of Deep Learning\nImage Source: Wikipedia",
        "paper": null,
        "category": [
            "Generative Models"
        ],
        "type": "Method",
        "probable_wikipedia": "Deep belief network",
        "score": "10.6281805",
        "probable_description": " In machine learning, a deep belief network (DBN) is a generative graphical model, or alternatively a class of deep neural network, composed of multiple layers of latent variables (\"hidden units\"), with connections between the layers but not between units within each layer.  When trained on a set of examples without supervision, a DBN can learn to probabilistically reconstruct its inputs. The layers then act as feature detectors. After this learning step, a DBN can be further trained with supervision to perform classification.  DBNs can be viewed as a composition of simple, unsupervised networks such as restricted Boltzmann machines (RBMs) or autoencoders, where each sub-network's hidden layer serves as the visible layer for the next. An RBM is an undirected, generative energy-based model with a \"visible\" input layer and a hidden layer and connections between but not within layers. This composition leads to a fast, layer-by-layer unsupervised training procedure, where contrastive divergence is applied to each sub-network in turn, starting from the \"lowest\" pair of layers (the lowest visible layer is a training set).  The observation that DBNs can be trained greedily, one layer at a time, led to one of the first effective deep learning algorithms. Overall, there are many attractive implementations and uses of DBNs in real-life applications and scenarios (e.g., electroencephalography, drug discovery). "
    },
    {
        "id": "METHOD_deep-captcha",
        "name": "Deep-CAPTCHA",
        "full_name": "Deep-CAPTCHA",
        "description": "CAPTCHA is a human-centred test to distinguish a human operator from bots, attacking programs, or other computerised agents that tries to imitate human intelligence. In this research, we investigate a way to crack visual CAPTCHA tests by an automated deep learning based solution. The goal of this research is to investigate the weaknesses and vulnerabilities of the CAPTCHA generator systems; hence, developing more robust CAPTCHAs, without taking the risks of manual try and fail efforts. We develop a Convolutional Neural Network called Deep-CAPTCHA to achieve this goal. The proposed platform is able to investigate both numerical and alphanumerical CAPTCHAs. To train and develop an efficient model, we have generated a dataset of 500,000 CAPTCHAs to train our model. In this paper, we present our customised deep neural network model, we review the research gaps, the existing challenges, and the solutions to cope with the issues. Our network's cracking accuracy leads to a high rate of 98.94% and 98.31% for the numerical and the alpha-numerical test datasets, respectively. That means more works is required to develop robust CAPTCHAs, to be non-crackable against automated artificial agents. As the outcome of this research, we identify some efficient techniques to improve the security of the CAPTCHAs, based on the performance analysis conducted on the Deep-CAPTCHA model.",
        "category": [
            "Convolutional Neural Networks",
            "Image Models"
        ],
        "proposed_in": {
            "paper_id": "deep-captcha-a-deep-learning-based-captcha",
            "s2_paper_id": "841f9e05ca03c0653bc912abb25f8b8b66cab531"
        },
        "type": "Method",
        "probable_wikipedia": "CAPTCHA",
        "score": "2.7047708",
        "probable_description": " A CAPTCHA (, an acronym for \"completely automated public Turing test to tell computers and humans apart\") is a type of challenge\u2013response test used in computing to determine whether or not the user is human.  The term was coined in 2003 by Luis von Ahn, Manuel Blum, Nicholas J. Hopper, and John Langford. The most common type of CAPTCHA (displayed as Version 1.0) was first invented in 1997 by two groups working in parallel. This form of CAPTCHA requires that the user type the letters of a distorted image, sometimes with the addition of an obscured sequence of letters or digits that appears on the screen. Because the test is administered by a computer, in contrast to the standard Turing test that is administered by a human, a CAPTCHA is sometimes described as a reverse Turing test.  This user identification procedure has received many criticisms, especially from people with disabilities, but also from other people who feel that their everyday work is slowed down by distorted words that are difficult to read. It takes the average person approximately 10 seconds to solve a typical CAPTCHA. "
    },
    {
        "id": "METHOD_deflation",
        "name": "Deflation",
        "full_name": "Deflation",
        "description": "Deflation is a video-to-image operation to transform a video network into a network that can ingest a single image. In the two types of video networks considered in the original paper, this deflation corresponds to the following operations: for 3D convolutional based networks, summing the 3D spatio-temporal filters over the temporal dimension to obtain 2D filters; for TSM networks,, turning off the channel shifting which results in a standard residual architecture (ResNet50) for images.",
        "category": [
            "Miscellaneous Components"
        ],
        "proposed_in": {
            "paper_id": "self-supervised-multimodal-versatile-networks",
            "s2_paper_id": "10d11f0045dc7f217c7f01bc6cbb47929e9b8808"
        },
        "type": "Method",
        "probable_wikipedia": "DEFLATE",
        "score": "8.750696",
        "probable_description": " In computing, Deflate is a lossless data compression file format that uses a combination of LZSS and Huffman coding. It was designed by Phil Katz, for version 2 of his PKZIP archiving tool. Deflate was later specified in RFC 1951.  Katz also designed the original algorithm used to construct Deflate streams. This algorithm was patented as , and assigned to PKWARE, Inc. As stated in the RFC document, an algorithm producing Deflate files was widely thought to be implementable in a manner not covered by patents. This led to its widespread use, for example in gzip compressed files and PNG image files, in addition to the ZIP file format for which Katz originally designed it. The patent has since expired. "
    },
    {
        "id": "METHOD_discrete-cosine-transform",
        "name": "Discrete Cosine Transform",
        "full_name": "Discrete Cosine Transform",
        "description": "Discrete Cosine Transform (DCT) is an orthogonal transformation method that decomposes an\nimage to its spatial frequency spectrum. It expresses a finite sequence of data points in terms of a sum of cosine functions oscillating at different frequencies. It is used a lot in compression tasks, e..g image compression where for example high-frequency components can be discarded. It is a type of Fourier-related Transform, similar to discrete fourier transforms (DFTs), but only using real numbers.\nImage Credit: Wikipedia",
        "paper": null,
        "category": [
            "Fourier-related Transforms"
        ],
        "type": "Method",
        "probable_wikipedia": "Discrete cosine transform",
        "score": "11.887223",
        "probable_description": " A discrete cosine transform (DCT) expresses a finite sequence of data points in terms of a sum of cosine functions oscillating at different frequencies. DCTs are important to numerous applications in science and engineering, from lossy compression of audio (e.g. MP3), images (e.g. JPEG) (where small high-frequency components can be discarded), and video (e.g. MPEG), to spectral methods for the numerical solution of partial differential equations. The use of cosine rather than sine functions is critical for compression, since it turns out (as described below) that fewer cosine functions are needed to approximate a typical signal, whereas for differential equations the cosines express a particular choice of boundary conditions.  In particular, a DCT is a Fourier-related transform similar to the discrete Fourier transform (DFT), but using only real numbers. The DCTs are generally related to Fourier Series coefficients of a periodically and symmetrically extended sequence whereas DFTs are related to Fourier Series coefficients of a periodically extended sequence. DCTs are equivalent to DFTs of roughly twice the length, operating on real data with even symmetry (since the Fourier transform of a real and even function is real and even), whereas in some variants the input and/or output data are shifted by half a sample. There are eight standard DCT variants, of which four are common.  The most common variant of discrete cosine transform is the type-II DCT, which is often called simply \"the DCT\". Its inverse, the type-III DCT, is correspondingly often called simply \"the inverse DCT\" or \"the IDCT\". Two related transforms"
    },
    {
        "id": "METHOD_dropout",
        "name": "Dropout",
        "full_name": "Dropout",
        "description": "Dropout is a regularization technique for neural networks that drops a unit (along with connections) at training time with a specified probability $p$ (a common value is $p=0.5$). At test time, all units are present, but with weights scaled by $p$ (i.e. $w$ becomes $pw$).\nThe idea is to prevent co-adaptation, where the neural network becomes too reliant on particular connections, as this could be symptomatic of overfitting. Intuitively, dropout can be thought of as creating an implicit ensemble of neural networks.",
        "category": [
            "Regularization"
        ],
        "proposed_in": {
            "paper_id": "dropout-a-simple-way-to-prevent-neural",
            "s2_paper_id": "34f25a8704614163c4095b3ee2fc969b60de4698"
        },
        "type": "Method",
        "probable_wikipedia": "Dropout (neural networks)",
        "score": "10.166489",
        "probable_description": " Dropout is a regularization technique patented by Google for reducing overfitting in neural networks by preventing complex co-adaptations on training data. It is a very efficient way of performing model averaging with neural networks. The term \"dropout\" refers to dropping out units (both hidden and visible) in a neural network.   "
    },
    {
        "id": "METHOD_dtw",
        "name": "DTW",
        "full_name": "Dynamic Time Warping",
        "description": "Dynamic Time Warping (DTW) [1] is one of well-known distance measures between a pairwise of time series. The main idea of DTW is to compute the distance from the matching of similar elements between time series. It uses the dynamic programming technique to find the optimal temporal matching between elements of two time series.\nFor instance, similarities in walking could be detected using DTW, even if one person was walking faster than the other, or if there were accelerations and decelerations during the course of an observation. DTW has been applied to temporal sequences of video, audio, and graphics data \u2014 indeed, any data that can be turned into a linear sequence can be analyzed with DTW. A well known application has been automatic speech recognition, to cope with different speaking speeds. Other applications include speaker recognition and online signature recognition. It can also be used in partial shape matching application.\nIn general, DTW is a method that calculates an optimal match between two given sequences (e.g. time series) with certain restriction and rules:\n\nEvery index from the first sequence must be matched with one or more indices from the other sequence, and vice versa\nThe first index from the first sequence must be matched with the first index from the other sequence (but it does not have to be its only match)\nThe last index from the first sequence must be matched with the last index from the other sequence (but it does not have to be its only match)\nThe mapping of the indices from the first sequence to indices from the other sequence must be monotonically increasing, and vice versa, i.e. if j>i  are indices from the first sequence, then there must not be two indices l>k in the other sequence, such that index i is matched with index l and index j is matched with index k, and vice versa.\n\n[1] Sakoe, Hiroaki, and Seibi Chiba. \"Dynamic programming algorithm optimization for spoken word recognition.\" IEEE transactions on acoustics, speech, and signal processing 26, no. 1 (1978): 43-49.",
        "paper": null,
        "category": [
            "Time Series Analysis"
        ],
        "type": "Method",
        "probable_wikipedia": "Dynamic time warping",
        "score": "11.680642",
        "probable_description": " In time series analysis, dynamic time warping (DTW) is one of the algorithms for measuring similarity between two temporal sequences, which may vary in speed. For instance, similarities in walking could be detected using DTW, even if one person was walking faster than the other, or if there were accelerations and decelerations during the course of an observation. DTW has been applied to temporal sequences of video, audio, and graphics data \u2014 indeed, any data that can be turned into a linear sequence can be analyzed with DTW. A well known application has been automatic speech recognition, to cope with different speaking speeds. Other applications include speaker recognition and online signature recognition. It can also be used in partial shape matching application.  In general, DTW is a method that calculates an optimal match between two given sequences (e.g. time series) with certain restriction and rules:  The optimal match is denoted by the match that satisfies all the restrictions and the rules and that has the minimal cost, where the cost is computed as the sum of absolute differences, for each matched pair of indices, between their values.  The sequences are \"warped\" non-linearly in the time dimension to determine a measure of their similarity independent of certain non-linear variations in the time dimension. This sequence alignment method is often used in time series classification. Although DTW measures a distance-like quantity between two given sequences, it doesn't guarantee the triangle inequality to hold.  In addition to a similarity measure between the two sequences,"
    },
    {
        "id": "METHOD_dueling-network",
        "name": "Dueling Network",
        "full_name": "Dueling Network",
        "description": "A Dueling Network is a type of Q-Network that has two streams to separately estimate (scalar) state-value and the advantages for each action. Both streams share a common convolutional feature learning module. The two streams are combined via a special aggregating layer to produce an\nestimate of the state-action value function Q as shown in the figure to the right.\nThe last module uses the following mapping:\n$$ Q\\left(s, a, \\theta, \\alpha, \\beta\\right) =V\\left(s, \\theta, \\beta\\right) + \\left(A\\left(s, a, \\theta, \\alpha\\right) - \\frac{1}{|\\mathcal{A}|}\\sum_{a'}A\\left(s, a'; \\theta, \\alpha\\right)\\right) $$\nThis formulation is chosen for identifiability so that the advantage function has zero advantage for the chosen action, but instead of a maximum we use an average operator to increase the stability of the optimization.",
        "category": [
            "Q-Learning Networks",
            "Off-Policy TD Control"
        ],
        "proposed_in": {
            "paper_id": "dueling-network-architectures-for-deep",
            "s2_paper_id": "4c05d7caa357148f0bbd61720bdd35f0bc05eb81"
        },
        "type": "Method",
        "probable_wikipedia": "Dueling Network",
        "score": "0.72418714",
        "probable_description": " Dueling Network (commonly abbreviated DN) was an online, unofficial Adobe Flash\u2013based simulation of the \"Yu-Gi-Oh! Trading Card Game\" (TCG). It was created by Christopher Salvarani on March 3, 2006 and officially released on May 17, 2011. Its popularity grew quickly, and it had gained more than four million registered users. The site was run by in-game volunteer administrators and moderators. At its peak, its server allowed for 9,100 players to be online at the same time.  \"Dueling Network\" started on May 8, 2011 and officially released on May 17, 2011. After the release, the site's popularity grew quickly, and as of 2013, had acquired more than three million registered users. The site provided users with various in-game messaging systems. From the main menu, players were able to access the \"Duel Room\", construct Decks, check players' rankings and profiles, edit their own profiles and change their account passwords. The game was intended to be a simulation of the real-life card game, and as such, was set to be played manually with little in-game automation. It followed official Konami policy as closely as possible. "
    },
    {
        "id": "METHOD_early-stopping",
        "name": "Early Stopping",
        "full_name": "Early Stopping",
        "description": "Early Stopping is a regularization technique for deep neural networks that stops training when parameter updates no longer begin to yield improves on a validation set. In essence, we store and update the current best parameters during training, and when parameter updates no longer yield an improvement (after a set number of iterations) we stop training and use the last best parameters. It works as a regularizer by restricting the optimization procedure to a smaller volume of parameter space.\nImage Source: Ramazan Gen\u00e7ay",
        "paper": null,
        "category": [
            "Regularization"
        ],
        "type": "Method",
        "probable_wikipedia": "Early stopping",
        "score": "11.76099",
        "probable_description": " In machine learning, early stopping is a form of regularization used to avoid overfitting when training a learner with an iterative method, such as gradient descent. Such methods update the learner so as to make it better fit the training data with each iteration. Up to a point, this improves the learner's performance on data outside of the training set. Past that point, however, improving the learner's fit to the training data comes at the expense of increased generalization error. Early stopping rules provide guidance as to how many iterations can be run before the learner begins to over-fit. Early stopping rules have been employed in many different machine learning methods, with varying amounts of theoretical foundation. "
    },
    {
        "id": "METHOD_electric",
        "name": "Electric",
        "full_name": "Electric",
        "description": "Electric is an energy-based cloze model for representation learning over text. Like BERT, it is a conditional generative model of tokens given their contexts. However, Electric does not use masking or output a full distribution over tokens that could occur in a context. Instead, it assigns a scalar energy score to each input token indicating how likely it is given its context.\nSpecifically, like BERT, Electric also models $p_{\\text {data }}\\left(x_{t} \\mid \\mathbf{x}_{\\backslash t}\\right)$, but does not use masking or a softmax layer. Electric first maps the unmasked input $\\mathbf{x}=\\left[x_{1}, \\ldots, x_{n}\\right]$ into contextualized vector representations $\\mathbf{h}(\\mathbf{x})=\\left[\\mathbf{h}_{1}, \\ldots, \\mathbf{h}_{n}\\right]$ using a transformer network. The model assigns a given position $t$ an energy score\n$$\nE(\\mathbf{x})_{t}=\\mathbf{w}^{T} \\mathbf{h}(\\mathbf{x})_{t}\n$$\nusing a learned weight vector $w$. The energy function defines a distribution over the possible tokens at position $t$ as\n$$\np_{\\theta}\\left(x_{t} \\mid \\mathbf{x}_{\\backslash t}\\right)=\\exp \\left(-E(\\mathbf{x})_{t}\\right) / Z\\left(\\mathbf{x}_{\\backslash t}\\right) \n$$\n$$\n=\\frac{\\exp \\left(-E(\\mathbf{x})_{t}\\right)}{\\sum_{x^{\\prime} \\in \\mathcal{V}} \\exp \\left(-E\\left(\\operatorname{REPLACE}\\left(\\mathbf{x}, t, x^{\\prime}\\right)\\right)_{t}\\right)}\n$$\nwhere $\\text{REPLACE}\\left(\\mathbf{x}, t, x^{\\prime}\\right)$ denotes replacing the token at position $t$ with $x^{\\prime}$ and $\\mathcal{V}$ is the vocabulary, in practice usually word pieces. Unlike with BERT, which produces the probabilities for all possible tokens $x^{\\prime}$ using a softmax layer, a candidate $x^{\\prime}$ is passed in as input to the transformer. As a result, computing $p_{\\theta}$ is prohibitively expensive because the partition function $Z_{\\theta}\\left(\\mathbf{x}_{\\backslash t}\\right)$ requires running the transformer $|\\mathcal{V}|$ times; unlike most EBMs, the intractability of $Z_{\\theta}(\\mathbf{x} \\backslash t)$ is more due to the expensive scoring function rather than having a large sample space.",
        "category": [
            "Autoencoding Transformers",
            "Transformers",
            "Language Models"
        ],
        "proposed_in": {
            "paper_id": "pre-training-transformers-as-energy-based-1",
            "s2_paper_id": "00b677e971ded11ac4a7da1b80ffda95b4f1ed78"
        },
        "type": "Method",
        "probable_wikipedia": "Electricity",
        "score": "1.1382804",
        "probable_description": " Electricity is the set of physical phenomena associated with the presence and motion of matter that has a property of electric charge. In early days, electricity was considered as being unrelated to magnetism. Later on, many experimental results and the development of Maxwell's equations indicated that both electricity and magnetism are from a single phenomenon: electromagnetism. Various common phenomena are related to electricity, including lightning, static electricity, electric heating, electric discharges and many others.  The presence of an electric charge, which can be either positive or negative, produces an electric field. The movement of electric charges is an electric current and produces a magnetic field.  When a charge is placed in a location with a non-zero electric field, a force will act on it. The magnitude of this force is given by Coulomb's law. Thus, if that charge were to move, the electric field would be doing work on the electric charge. Thus we can speak of electric potential at a certain point in space, which is equal to the work done by an external agent in carrying a unit of positive charge from an arbitrarily chosen reference point to that point without any acceleration and is typically measured in volts.  Electricity is at the heart of many modern technologies, being used for:  Electrical phenomena have been studied since antiquity, though progress in theoretical understanding remained slow until the seventeenth and eighteenth centuries. Even then, practical applications for electricity were few, and it would not be until the late nineteenth century"
    },
    {
        "id": "METHOD_estimation-statistics",
        "name": "Estimation Statistics",
        "full_name": "Estimation Statistics",
        "description": "Estimation statistics is a data analysis framework that uses a combination of effect sizes, confidence intervals, precision planning, and meta-analysis to plan experiments, analyze data and interpret results. It is distinct from null hypothesis significance testing (NHST), which is considered to be less informative. The primary aim of estimation methods is to report an effect size (a point estimate) along with its confidence interval, the latter of which is related to the precision of the estimate. The confidence interval summarizes a range of likely values of the underlying population effect. Proponents of estimation see reporting a P value as an unhelpful distraction from the important business of reporting an effect size with its confidence intervals, and believe that estimation should replace significance testing for data analysis.",
        "paper": null,
        "category": [
            "Statistical Inference"
        ],
        "type": "Method",
        "probable_wikipedia": "Estimation statistics",
        "score": "12.530518",
        "probable_description": " Estimation statistics is a data analysis framework that uses a combination of effect sizes, confidence intervals, precision planning, and meta-analysis to plan experiments, analyze data and interpret results. It is distinct from null hypothesis significance testing (NHST), which is considered to be less informative. Estimation statistics, or simply estimation, is also known as the new statistics, a distinction introduced in the fields of psychology, medical research, life sciences and a wide range of other experimental sciences where NHST still remains prevalent, despite estimation statistics having been recommended as preferable for several decades.  The primary aim of estimation methods is to report an effect size (a point estimate) along with its confidence interval, the latter of which is related to the precision of the estimate. The confidence interval summarizes a range of likely values of the underlying population effect. Proponents of estimation see reporting a \"P\" value as an unhelpful distraction from the important business of reporting an effect size with its confidence intervals, and believe that estimation should replace significance testing for data analysis. "
    },
    {
        "id": "METHOD_exponential-decay",
        "name": "Exponential Decay",
        "full_name": "Exponential Decay",
        "description": "Exponential Decay is a learning rate schedule where we decay the learning rate with more iterations using an exponential function:\n$$ \\text{lr} = \\text{lr}_{0}\\exp\\left(-kt\\right) $$\nImage Credit: Suki Lau",
        "paper": null,
        "category": [
            "Learning Rate Schedules"
        ],
        "type": "Method",
        "probable_wikipedia": "Exponential decay",
        "score": "6.9951463",
        "probable_description": " A quantity is subject to exponential decay if it decreases at a rate proportional to its current value. Symbolically, this process can be expressed by the following differential equation, where \"N\" is the quantity and \u03bb (lambda) is a positive rate called the exponential decay constant:  The solution to this equation (see derivation below) is:  where \"N\"(\"t\") is the quantity at time \"t\", and \"N\" = \"N\"(0) is the initial quantity, i.e. the quantity at time \"t\" = 0. "
    },
    {
        "id": "METHOD_fasttext",
        "name": "fastText",
        "full_name": "fastText",
        "description": "fastText embeddings exploit subword information to construct word embeddings. Representations are learnt of character $n$-grams, and words represented as the sum of the $n$-gram vectors. This extends the word2vec type models with subword information. This helps the embeddings understand suffixes and prefixes. Once a word is represented using character $n$-grams, a skipgram model is trained to learn the embeddings.",
        "category": [
            "Static Word Embeddings",
            "Word Embeddings"
        ],
        "proposed_in": {
            "paper_id": "enriching-word-vectors-with-subword",
            "s2_paper_id": "e2dba792360873aef125572812f3673b1a85d850"
        },
        "type": "Method",
        "probable_wikipedia": "FastText",
        "score": "12.121527",
        "probable_description": " fastText is a library for learning of word embeddings and text classification created by Facebook's AI Research (FAIR) lab. The model allows to create an unsupervised learning or supervised learning algorithm for obtaining vector representations for words. Facebook makes available pretrained models for 294 languages.fastText uses a neural network for word embedding.   "
    },
    {
        "id": "METHOD_feedforward-network",
        "name": "Feedforward Network",
        "full_name": "Feedforward Network",
        "description": "A Feedforward Network, or a Multilayer Perceptron (MLP), is a neural network with solely densely connected layers. This is the classic neural network architecture of the literature. It consists of inputs $x$ passed through units $h$ (of which there can be many layers) to predict a target $y$. Activation functions are generally chosen to be non-linear to allow for flexible functional approximation.\nImage Source: Deep Learning, Goodfellow et al",
        "paper": null,
        "category": [
            "Feedforward Networks"
        ],
        "type": "Method",
        "probable_wikipedia": "Feedforward neural network",
        "score": "8.235459",
        "probable_description": " A feedforward neural network is an artificial neural network wherein connections between the nodes do \"not\" form a cycle. As such, it is different from recurrent neural networks.  The feedforward neural network was the first and simplest type of artificial neural network devised. In this network, the information moves in only one direction, forward, from the input nodes, through the hidden nodes (if any) and to the output nodes. There are no cycles or loops in the network. "
    },
    {
        "id": "METHOD_ga",
        "name": "GA",
        "full_name": "Genetic Algorithms",
        "description": "Genetic Algorithms are search algorithms that mimic Darwinian biological evolution in order to select and propagate better solutions.",
        "category": [
            "Heuristic Search Algorithms"
        ],
        "proposed_in": {
            "paper_id": "genetic-algorithms-and-the-traveling-salesman",
            "s2_paper_id": "a4387cb9defa57861e1f84bbfafa47bdc8856570"
        },
        "type": "Method",
        "probable_wikipedia": "Genetic algorithm",
        "score": "10.661609",
        "probable_description": " In computer science and operations research, a genetic algorithm (GA) is a metaheuristic inspired by the process of natural selection that belongs to the larger class of evolutionary algorithms (EA). Genetic algorithms are commonly used to generate high-quality solutions to optimization and search problems by relying on bio-inspired operators such as mutation, crossover and selection. John Holland introduced genetic algorithms in 1960 based on the concept of Darwin\u2019s theory of evolution; afterwards, his student David E. Goldberg extended GA in 1989. "
    },
    {
        "id": "METHOD_gan",
        "name": "GAN",
        "full_name": "Generative Adversarial Network",
        "description": "A GAN, or Generative Adversarial Network, is a generative model that simultaneously trains\ntwo models: a generative model $G$ that captures the data distribution, and a discriminative model $D$ that estimates the\nprobability that a sample came from the training data rather than $G$.\nThe training procedure for $G$ is to maximize the probability of $D$ making\na mistake. This framework corresponds to a minimax two-player game. In the\nspace of arbitrary functions $G$ and $D$, a unique solution exists, with $G$\nrecovering the training data distribution and $D$ equal to $\\frac{1}{2}$\neverywhere. In the case where $G$ and $D$ are defined by multilayer perceptrons,\nthe entire system can be trained with backpropagation. \n(Image Source: here)",
        "category": [
            "Generative Adversarial Networks",
            "Generative Models"
        ],
        "proposed_in": {
            "paper_id": "generative-adversarial-networks",
            "s2_paper_id": "8388f1be26329fa45e5807e968a641ce170ea078"
        },
        "type": "Method",
        "probable_wikipedia": "Generative adversarial network",
        "score": "11.559689",
        "probable_description": " A generative adversarial network (GAN) is a class of machine learning systems invented by Ian Goodfellow and his colleagues in 2014. Two neural networks contest with each other in a game (in the sense of game theory, often but not always in the form of a zero-sum game). Given a training set, this technique learns to generate new data with the same statistics as the training set. For example, a GAN trained on photographs can generate new photographs that look at least superficially authentic to human observers, having many realistic characteristics. Though originally proposed as a form of generative model for unsupervised learning, GANs have also proven useful for semi-supervised learning, fully supervised learning, and reinforcement learning. In a 2016 seminar, Yann LeCun described GANs as \"the coolest idea in machine learning in the last twenty years\". "
    },
    {
        "id": "METHOD_gaussian-process",
        "name": "Gaussian Process",
        "full_name": "Gaussian Process",
        "description": "Gaussian Processes are non-parametric models for approximating functions. They rely upon a measure of similarity between points (the kernel function) to predict the value for an unseen point from training data. The models are fully probabilistic so uncertainty bounds are baked in with the model.\nImage Source: Gaussian Processes for Machine Learning, C. E. Rasmussen & C. K. I. Williams",
        "paper": null,
        "category": [
            "Non-Parametric Regression"
        ],
        "type": "Method",
        "probable_wikipedia": "Gaussian process",
        "score": "11.026488",
        "probable_description": " In probability theory and statistics, a Gaussian process is a stochastic process (a collection of random variables indexed by time or space), such that every finite collection of those random variables has a multivariate normal distribution, i.e. every finite linear combination of them is normally distributed. The distribution of a Gaussian process is the joint distribution of all those (infinitely many) random variables, and as such, it is a distribution over functions with a continuous domain, e.g. time or space.  A machine-learning algorithm that involves a Gaussian process uses lazy learning and a measure of the similarity between points (the \"kernel function\") to predict the value for an unseen point from training data. The prediction is not just an estimate for that point, but also has uncertainty information\u2014it is a one-dimensional Gaussian distribution (which is the marginal distribution at that point).  For some kernel functions, matrix algebra can be used to calculate the predictions using the technique of kriging. When a parameterised kernel is used, optimisation software is typically used to fit a Gaussian process model.  The concept of Gaussian processes is named after Carl Friedrich Gauss because it is based on the notion of the Gaussian distribution (normal distribution). Gaussian processes can be seen as an infinite-dimensional generalization of multivariate normal distributions.  Gaussian processes are useful in statistical modelling, benefiting from properties inherited from the normal distribution. For example, if a random process is modelled as a Gaussian process, the distributions of various derived quantities can be obtained explicitly. Such"
    },
    {
        "id": "METHOD_gravity",
        "name": "Gravity",
        "full_name": "Gravity",
        "description": "Gravity is a kinematic approach to optimization based on gradients.",
        "category": [
            "Stochastic Optimization",
            "Optimization"
        ],
        "proposed_in": {
            "paper_id": "gravity-optimizer-a-kinematic-approach-on",
            "s2_paper_id": "b26b1d3cd24847e43cbe934294ac571d30ff6bdf"
        },
        "type": "Method",
        "probable_wikipedia": "Gravity",
        "score": "8.123037",
        "probable_description": " Gravity (), or gravitation, is a natural phenomenon by which all things with mass or energy\u2014including planets, stars, galaxies, and even light\u2014are brought toward (or \"gravitate\" toward) one another. On Earth, gravity gives weight to physical objects, and the Moon's gravity causes the ocean tides. The gravitational attraction of the original gaseous matter present in the Universe caused it to begin coalescing, forming stars\u2014and for the stars to group together into galaxies\u2014so gravity is responsible for many of the large-scale structures in the Universe. Gravity has an infinite range, although its effects become increasingly weaker on farther objects.  Gravity is most accurately described by the general theory of relativity (proposed by Albert Einstein in 1915) which describes gravity not as a force, but as a consequence of the curvature of spacetime caused by the uneven distribution of mass. The most extreme example of this curvature of spacetime is a black hole, from which nothing\u2014not even light\u2014can escape once past the black hole's event horizon. However, for most applications, gravity is well approximated by Newton's law of universal gravitation, which describes gravity as a force which causes any two bodies to be attracted to each other, with the force proportional to the product of their masses and inversely proportional to the square of the distance between them.  Gravity is the weakest of the four fundamental interactions of physics, approximately 10 times weaker than the strong interaction, 10 times weaker than the electromagnetic force and 10 times weaker than the weak interaction. As a consequence,"
    },
    {
        "id": "METHOD_gru",
        "name": "GRU",
        "full_name": "Gated Recurrent Unit",
        "description": "A Gated Recurrent Unit, or GRU, is a type of recurrent neural network. It is similar to an LSTM, but only has two gates - a reset gate and an update gate - and notably lacks an output gate. Fewer parameters means GRUs are generally easier/faster to train than their LSTM counterparts.\nImage Source: here",
        "category": [
            "Recurrent Neural Networks"
        ],
        "proposed_in": {
            "paper_id": "learning-phrase-representations-using-rnn",
            "s2_paper_id": "0b544dfe355a5070b60986319a3f51fb45d1348e"
        },
        "type": "Method",
        "probable_wikipedia": "Gated recurrent unit",
        "score": "8.311244",
        "probable_description": " Gated recurrent units (GRUs) are a gating mechanism in recurrent neural networks, introduced in 2014 by Kyunghyun Cho et al. The GRU is like a long short-term memory (LSTM) with forget gate but has fewer parameters than LSTM, as it lacks an output gate. GRU's performance on certain tasks of polyphonic music modeling and speech signal modeling was found to be similar to that of LSTM. GRUs have been shown to exhibit even better performance on certain smaller datasets.  However, as shown by Gail Weiss & Yoav Goldberg & Eran Yahav, the LSTM is \"strictly stronger\" than the GRU as it can easily perform unbounded counting, while the GRU cannot. That's why the GRU fails to learn simple languages that are learnable by the LSTM.  Similarly, as shown by Denny Britz & Anna Goldie & Minh-Thang Luong & Quoc Le of Google Brain, LSTM cells consistently outperform GRU cells in \"the first large-scale analysis of architecture variations for Neural Machine Translation.\" "
    },
    {
        "id": "METHOD_hamburgers",
        "name": "Hamburger",
        "full_name": "Hamburger",
        "description": "Hamburger is a global context module that employs matrix decomposition to factorize the learned representation into sub-matrices so as to recover the clean low-rank signal subspace. The key idea is, if we formulate the inductive bias like the global context into an objective function, the optimization algorithm to minimize the objective function can construct a computational graph, i.e., the architecture we need in the networks.",
        "category": [
            "Global Context Modules"
        ],
        "proposed_in": {
            "paper_id": "is-attention-better-than-matrix-decomposition-1",
            "s2_paper_id": "f829a355de02c08567927154d3045a6eb5425c91"
        },
        "type": "Method",
        "probable_wikipedia": "Hamburger",
        "score": "5.3993897",
        "probable_description": " A hamburger (short: burger) is a sandwich consisting of one or more cooked patties of ground meat, usually beef, placed inside a sliced bread roll or bun. The patty may be pan fried, grilled, or flame broiled. Hamburgers are often served with cheese, lettuce, tomato, onion, pickles, bacon, or chiles; condiments such as ketchup, mayonnaise, mustard, relish, or \"special sauce\"; and are frequently placed on sesame seed buns. A hamburger topped with cheese is called a cheeseburger.  The term \"burger\" can also be applied to the meat patty on its own, especially in the United Kingdom, where the term \"patty\" is rarely used, or the term can even refer simply to ground beef. Since the term hamburger usually implies beef, for clarity \"burger\" may be prefixed with the type of meat or meat substitute used, as in beef burger, turkey burger, bison burger, or veggie burger.  Hamburgers are sold at fast-food restaurants, diners, and specialty and high-end restaurants (where burgers may sell for several times the cost of a fast-food burger, but may be one of the cheaper options on the menu). There are many international and regional variations of the hamburger. "
    },
    {
        "id": "METHOD_hard-sigmoid",
        "name": "Hard Sigmoid",
        "full_name": "Hard Sigmoid",
        "description": "The Hard Sigmoid is an activation function used for neural networks of the form:\n$$f\\left(x\\right) = \\max\\left(0, \\min\\left(1,\\frac{\\left(x+1\\right)}{2}\\right)\\right)$$\nImage Source: Rinat Maksutov",
        "category": [
            "Activation Functions"
        ],
        "proposed_in": {
            "paper_id": "binaryconnect-training-deep-neural-networks",
            "s2_paper_id": "a5733ff08daff727af834345b9cfff1d0aa109ec"
        },
        "type": "Method",
        "probable_wikipedia": "Hard sigmoid",
        "score": "11.867",
        "probable_description": " In artificial intelligence, especially computer vision and artificial neural networks, a hard sigmoid is non-smooth function used in place of a sigmoid function. These retain the basic shape of a sigmoid, rising from 0 to 1, but using simpler functions, especially piecewise linear functions or piecewise constant functions. These are preferred where speed of computation is more important than precision. "
    },
    {
        "id": "METHOD_hho",
        "name": "Harris Hawks optimization (HHO)",
        "full_name": "Harris Hawks optimization",
        "description": "HHO is a popular swarm-based, gradient-free optimization algorithm with several active and time-varying phases of exploration and exploitation. This algorithm initially published by the prestigious Journal of Future Generation Computer Systems (FGCS) in 2019, and from the first day, it has gained increasing attention among researchers due to its flexible structure, high performance, and high-quality results. The main logic of the HHO method is designed based on the cooperative behaviour and chasing styles of Harris' hawks in nature called \"surprise pounce\". Currently, there are many suggestions about how to enhance the functionality of HHO, and there are also several enhanced variants of the HHO in the leading Elsevier and IEEE transaction journals.\nFrom the algorithmic behaviour viewpoint, there are several effective features in HHO :\nEscaping energy parameter has a dynamic randomized time-varying nature, which can further improve and harmonize the exploratory and exploitive patterns of HHO. This factor also supports HHO to conduct a smooth transition between exploration and exploitation.\nDifferent exploration mechanisms with respect to the average location of hawks can increase the exploratory trends of HHO throughout initial iterations.\nDiverse LF-based patterns with short-length jumps enrich the exploitative behaviours of HHO when directing a local search.\nThe progressive selection scheme supports search agents to progressively advance their position and only select a better position, which can improve the superiority of solutions and intensification powers of HHO throughout the optimization procedure.\nHHO shows a series of searching strategies and then, it selects the best movement step. This feature has also a constructive influence on the exploitation inclinations of HHO.\nThe randomized jump strength can assist candidate solutions in harmonising the exploration and exploitation leanings.\nThe application of adaptive and time-varying components allows HHO to handle difficulties of a feature space including local optimal solutions, multi-modality, and deceptive optima.\n\ud83d\udd17 The source codes of HHO are publicly available at https://aliasgharheidari.com/HHO.html",
        "paper": null,
        "category": [
            "Optimization"
        ],
        "type": "Method",
        "probable_wikipedia": "Harris hawks optimization",
        "score": "12.640316",
        "probable_description": " The Harris hawks optimization (HHO) algorithm is a new swarm intelligence optimization paradigm proposed by Ali Asghar Heidari et al. in 2019, which is inspired by the team behaviors and chasing patterns of Harris's hawk in nature called surprise pounce. "
    },
    {
        "id": "METHOD_heatmap",
        "name": "Heatmap",
        "full_name": "Heatmap",
        "description": "This paper proposes a new hybrid architecture that consists of a deep Convolu-tional Network and a Markov Random Field. We show how this architecture is successfully applied to the challenging problem of articulated human pose estimation in monocular images. The architecture can exploit structural domain constraints such as geometric relationships between body joint locations. We show that joint training of these two model paradigms improves performance and allows us to significantly outperform existing state-of-the-art techniques.",
        "category": [
            "Output Functions"
        ],
        "proposed_in": {
            "paper_id": "joint-training-of-a-convolutional-network-and",
            "s2_paper_id": "12ecc2d786080f638a01b9999518e9386baa157d"
        },
        "type": "Method",
        "probable_wikipedia": "Heat map",
        "score": "0.77282804",
        "probable_description": " A heat map (or heatmap) is a graphical representation of data where the individual values contained in a matrix are represented as colors. \"Heat map\" is a newer term but shading matrices have existed for over a century. "
    },
    {
        "id": "METHOD_highway-network",
        "name": "Highway Network",
        "full_name": "Highway Network",
        "description": "A Highway Network is an architecture designed to ease gradient-based training of very deep networks. They allow unimpeded information flow across several layers on \"information highways\". The architecture is characterized by the use of gating units which learn to regulate the flow of information through a network. Highway networks with hundreds of layers can be trained directly using stochastic gradient descent and with a variety of activation functions.",
        "category": [
            "Feedforward Networks"
        ],
        "proposed_in": {
            "paper_id": "highway-networks",
            "s2_paper_id": "e0945081b5b87187a53d4329cf77cd8bff635795"
        },
        "type": "Method",
        "probable_wikipedia": "Highway network",
        "score": "9.100375",
        "probable_description": " In machine learning, a highway network is an approach to optimizing networks and increasing their depth. Highway networks use learned gating mechanisms to regulate information flow, inspired by Long Short-Term Memory (LSTM) recurrent neural networks. The gating mechanisms allow neural networks to have paths for information to follow across different layers (\"information highways\").  Highway networks have been used as part of text sequence labeling and speech recognition tasks.  "
    },
    {
        "id": "METHOD_highway-networks",
        "name": "Highway networks",
        "full_name": "Highway networks",
        "description": "Please enter a description about the method here",
        "category": [
            "Attention Mechanisms",
            "Attention"
        ],
        "proposed_in": {
            "paper_id": "highway-networks",
            "s2_paper_id": "e0945081b5b87187a53d4329cf77cd8bff635795"
        },
        "type": "Method",
        "probable_wikipedia": "Highway network",
        "score": "7.3537173",
        "probable_description": " In machine learning, a highway network is an approach to optimizing networks and increasing their depth. Highway networks use learned gating mechanisms to regulate information flow, inspired by Long Short-Term Memory (LSTM) recurrent neural networks. The gating mechanisms allow neural networks to have paths for information to follow across different layers (\"information highways\").  Highway networks have been used as part of text sequence labeling and speech recognition tasks.  "
    },
    {
        "id": "METHOD_ica",
        "name": "ICA",
        "full_name": "Independent Component Analysis",
        "description": "Independent component analysis (ICA) is a statistical and computational technique for revealing hidden factors that underlie sets of random variables, measurements, or signals.\nICA defines a generative model for the observed multivariate data, which is typically given as a large database of samples. In the model, the data variables are assumed to be linear mixtures of some unknown latent variables, and the mixing system is also unknown. The latent variables are assumed nongaussian and mutually independent, and they are called the independent components of the observed data. These independent components, also called sources or factors, can be found by ICA.\nICA is superficially related to principal component analysis and factor analysis. ICA is a much more powerful technique, however, capable of finding the underlying factors or sources when these classic methods fail completely.\nExtracted from (https://www.cs.helsinki.fi/u/ahyvarin/whatisica.shtml)\nSource papers:\nBlind separation of sources, part I: An adaptive algorithm based on neuromimetic architecture\nIndependent component analysis, A new concept?\nIndependent component analysis: algorithms and applications",
        "paper": null,
        "category": [
            "Dimensionality Reduction"
        ],
        "type": "Method",
        "probable_wikipedia": "Independent component analysis",
        "score": "12.023232",
        "probable_description": " In signal processing, independent component analysis (ICA) is a computational method for separating a multivariate signal into additive subcomponents. This is done by assuming that the subcomponents are non-Gaussian signals and that they are statistically independent from each other. ICA is a special case of blind source separation. A common example application is the \"cocktail party problem\" of listening in on one person's speech in a noisy room. "
    },
    {
        "id": "METHOD_impala",
        "name": "IMPALA",
        "full_name": "IMPALA",
        "description": "IMPALA, or the Importance Weighted Actor Learner Architecture, is an off-policy actor-critic framework that decouples acting from learning and learns from experience trajectories using V-trace. Unlike the popular A3C-based agents, in which workers communicate gradients with respect to the parameters of the policy to a central parameter server, IMPALA actors communicate trajectories of experience (sequences of states, actions, and rewards) to a centralized learner. Since the learner in IMPALA has access to full trajectories of experience we use a GPU to perform updates on mini-batches of trajectories while aggressively parallelising all time independent operations. \nThis type of decoupled architecture can achieve very high throughput. However, because the policy used to generate a trajectory can lag behind the policy on the learner by several updates at the time of gradient calculation, learning becomes off-policy. The V-trace off-policy actor-critic algorithm is used to correct for this harmful discrepancy.",
        "category": [
            "Distributed Methods"
        ],
        "proposed_in": {
            "paper_id": "impala-scalable-distributed-deep-rl-with",
            "s2_paper_id": "80196cdfcd0c6ce2953bf65a7f019971e2026386"
        },
        "type": "Method",
        "probable_wikipedia": "Impala",
        "score": "0.45073944",
        "probable_description": " The impala (, \"Aepyceros melampus\") is a medium-sized antelope found in eastern and southern Africa. The sole member of the genus \"Aepyceros\", it was first described to European audiences by German zoologist Hinrich Lichtenstein in 1812. Two subspecies are recognised\u2014the common impala, and the larger and darker black-faced impala. The impala reaches at the shoulder and weighs . It features a glossy, reddish brown coat. The male's slender, lyre-shaped horns are long.  Active mainly during the day, the impala may be gregarious or territorial depending upon the climate and geography. Three distinct social groups can be observed: the territorial males, bachelor herds and female herds. The impala is known for two characteristic leaps that constitute an anti-predator strategy. Browsers as well as grazers, impala feed on monocots, dicots, forbs, fruits and acacia pods (whenever available). An annual, three-week-long rut takes place toward the end of the wet season, typically in May. Rutting males fight over dominance, and the victorious male courts female in oestrus. Gestation lasts six to seven months, following which a single calf is born and immediately concealed in cover. Calves are suckled for four to six months; young males\u2014forced out of the all-female groups\u2014join bachelor herds, while females may stay back.  The impala is found in woodlands and sometimes on the interface (ecotone) between woodlands and savannahs; it inhabits places close to water. While the black-faced impala is confined to southwestern Angola and Kaokoland in northwestern Namibia, the common impala is widespread across its range and has been reintroduced in"
    },
    {
        "id": "METHOD_pixel-prediction",
        "name": "Inpainting",
        "full_name": "Inpainting",
        "description": "Train a convolutional neural network to generate the contents of an arbitrary image region conditioned on its surroundings.",
        "category": [
            "Self-Supervised Learning"
        ],
        "proposed_in": {
            "paper_id": "context-encoders-feature-learning-by",
            "s2_paper_id": "7d0effebfa4bed19b6ba41f3af5b7e5b6890de87"
        },
        "type": "Method",
        "probable_wikipedia": "Inpainting",
        "score": "9.618477",
        "probable_description": " Inpainting is the process of reconstructing lost or deteriorated parts of images and videos. In the museum world, in the case of a valuable painting, this task would be carried out by a skilled art conservator or art restorer. In the digital world, inpainting (also known as \"image interpolation\" or \"video interpolation\") refers to the application of sophisticated algorithms to replace lost or corrupted parts of the image data (mainly small regions or to remove small defects). "
    },
    {
        "id": "METHOD_jigsaw",
        "name": "Jigsaw",
        "full_name": "Jigsaw",
        "description": "Jigsaw is a self-supervision approach that relies on jigsaw-like puzzles as the pretext task in order to learn image representations.",
        "category": [
            "Self-Supervised Learning"
        ],
        "proposed_in": {
            "paper_id": "unsupervised-learning-of-visual-1",
            "s2_paper_id": "2ec8f7e0257a07d3914322b36072d1bbcd58a1e0"
        },
        "type": "Method",
        "probable_wikipedia": "Jigsaw (teaching technique)",
        "score": "2.0747933",
        "probable_description": " The jigsaw technique is a method of organizing classroom activity that makes students dependent on each other to succeed. It breaks classes into groups and breaks assignments into pieces that the group assembles to complete the (jigsaw) puzzle. It was designed by social psychologist Elliot Aronson to help weaken racial cliques in forcibly integrated schools.  The technique splits classes into mixed groups to work on small problems that the group collates into a final outcome. For example, an in-class assignment is divided into topics. Students are then split into groups with one member assigned to each topic. Working individually, each student learns about his or her topic and presents it to their group. Next, students gather into groups divided by topic. Each member presents again to the topic group. In same-topic groups, students reconcile points of view and synthesize information. They create a final report. Finally, the original groups reconvene and listen to presentations from each member. The final presentations provide all group members with an understanding of their own material, as well as the findings that have emerged from topic-specific group discussion. "
    },
    {
        "id": "METHOD_jukebox",
        "name": "Jukebox",
        "full_name": "Jukebox",
        "description": "Jukebox is a model that generates music with singing in the raw audio domain. It tackles the long context of raw audio using a multi-scale VQ-VAE to compress it to discrete codes, and modeling those using autoregressive Transformers. It can condition on artist and genre to steer the musical and vocal style, and on unaligned lyrics to make the singing more controllable.\nThree separate VQ-VAE models are trained with different temporal resolutions. At each level, the input audio is segmented and encoded into latent vectors $\\mathbf{h}_{t}$, which are then quantized to the closest codebook vectors $\\mathbf{e}_{z_{t}}$. The code $z_{t}$ is a discrete representation of the audio that we later train our prior on. The decoder takes the sequence of codebook vectors and reconstructs the audio. The top level learns the highest degree of abstraction, since it is encoding longer audio per token while keeping the codebook size the same. Audio can be reconstructed using the codes at any one of the abstraction levels, where the least abstract bottom-level codes result in the highest-quality audio.",
        "category": [
            "Generative Audio Models"
        ],
        "proposed_in": {
            "paper_id": "jukebox-a-generative-model-for-music-1",
            "s2_paper_id": "67dea28495cab71703993d0d52ca4733b9a66077"
        },
        "type": "Method",
        "probable_wikipedia": "Jukebox",
        "score": "11.52802",
        "probable_description": " A jukebox is a partially automated music-playing device, usually a coin-operated machine, that will play a patron's selection from self-contained media. The classic jukebox has buttons, with letters and numbers on them, which, when one of each group entered after each other, are used to select a specific record. "
    },
    {
        "id": "METHOD_kaf",
        "name": "KAF",
        "full_name": "Kernel Activation Function",
        "description": "A Kernel Activation Function is a non-parametric activation function defined as a one-dimensional kernel approximator:\n$$ f(s) = \\sum_{i=1}^D \\alpha_i \\kappa( s, d_i) $$\nwhere:\n\nThe dictionary of the kernel elements $d_0, \\ldots, d_D$ is fixed by sampling the $x$-axis with a uniform step around 0.\nThe user selects the kernel function (e.g., Gaussian, ReLU, Softplus) and the number of kernel elements $D$ as a hyper-parameter. A larger dictionary leads to more expressive activation functions and a larger number of trainable parameters.\nThe linear coefficients are adapted independently at every neuron via standard back-propagation.\n\nIn addition, the linear coefficients can be initialized using kernel ridge regression to behave similarly to a known function in the beginning of the optimization process.",
        "category": [
            "Activation Functions"
        ],
        "proposed_in": {
            "paper_id": "kafnets-kernel-based-non-parametric",
            "s2_paper_id": "34ab8f7a017103733a78712239c015f3d79b9c40"
        },
        "type": "Method",
        "probable_wikipedia": "Activation function",
        "score": "0.91809326",
        "probable_description": " In artificial neural networks, the activation function of a node defines the output of that node given an input or set of inputs. A standard computer chip circuit can be seen as a digital network of activation functions that can be \"ON\" (1) or \"OFF\" (0), depending on input. This is similar to the behavior of the linear perceptron in neural networks. However, only \"nonlinear\" activation functions allow such networks to compute nontrivial problems using only a small number of nodes. In artificial neural networks, this function is also called the transfer function. "
    },
    {
        "id": "METHOD_k-means-clustering",
        "name": "k-Means Clustering",
        "full_name": "k-Means Clustering",
        "description": "k-Means Clustering is a clustering algorithm that divides a training set into $k$ different clusters of examples that are near each other. It works by initializing $k$ different centroids {$\\mu\\left(1\\right),\\ldots,\\mu\\left(k\\right)$} to different values, then alternating between two steps until convergence:\n(i) each training example is assigned to cluster $i$ where $i$ is the index of the nearest centroid $\\mu^{(i)}$\n(ii) each centroid $\\mu^{(i)}$ is updated to the mean of all training examples $x^{(j)}$ assigned to cluster $i$.\nText Source: Deep Learning, Goodfellow et al\nImage Source: scikit-learn",
        "paper": null,
        "category": [
            "Clustering"
        ],
        "type": "Method",
        "probable_wikipedia": "K-means clustering",
        "score": "12.056147",
        "probable_description": " \"k\"-means clustering is a method of vector quantization, originally from signal processing, that is popular for cluster analysis in data mining. \"k\"-means clustering aims to partition \"n\" observations into \"k\" clusters in which each observation belongs to the cluster with the nearest mean, serving as a prototype of the cluster. This results in a partitioning of the data space into Voronoi cells.  The problem is computationally difficult (NP-hard); however, efficient heuristic algorithms converge quickly to a local optimum. These are usually similar to the expectation-maximization algorithm for mixtures of Gaussian distributions via an iterative refinement approach employed by both \"k-means\" and \"Gaussian mixture modeling\". They both use cluster centers to model the data; however, \"k\"-means clustering tends to find clusters of comparable spatial extent, while the expectation-maximization mechanism allows clusters to have different shapes.  The algorithm has a loose relationship to the \"k\"-nearest neighbor classifier, a popular machine learning technique for classification that is often confused with \"k\"-means due to the name. Applying the 1-nearest neighbor classifier to the cluster centers obtained by \"k\"-means classifies new data into the existing clusters. This is known as nearest centroid classifier or Rocchio algorithm. "
    },
    {
        "id": "METHOD_k-nn",
        "name": "k-NN",
        "full_name": "k-Nearest Neighbors",
        "description": "$k$-Nearest Neighbors is a clustering-based algorithm for classification and regression. It is a a type of instance-based learning as it does not attempt to construct a general internal model, but simply stores instances of the training data. Prediction is computed from a simple majority vote of the nearest neighbors of each point: a query point is assigned the data class which has the most representatives within the nearest neighbors of the point.\nSource of Description and Image: scikit-learn",
        "paper": null,
        "category": [
            "Non-Parametric Regression"
        ],
        "type": "Method",
        "probable_wikipedia": "K-nearest neighbors algorithm",
        "score": "7.426512",
        "probable_description": " In pattern recognition, the k\"-nearest neighbors algorithm (k\"-NN) is a non-parametric method used for classification and regression. In both cases, the input consists of the \"k\" closest training examples in the feature space. The output depends on whether \"k\"-NN is used for classification or regression:  \"k\"-NN is a type of instance-based learning, or lazy learning, where the function is only approximated locally and all computation is deferred until classification.  Both for classification and regression, a useful technique can be to assign weights to the contributions of the neighbors, so that the nearer neighbors contribute more to the average than the more distant ones. For example, a common weighting scheme consists in giving each neighbor a weight of 1/\"d\", where \"d\" is the distance to the neighbor.  The neighbors are taken from a set of objects for which the class (for \"k\"-NN classification) or the object property value (for \"k\"-NN regression) is known. This can be thought of as the training set for the algorithm, though no explicit training step is required.  A peculiarity of the \"k\"-NN algorithm is that it is sensitive to the local structure of the data. "
    },
    {
        "id": "METHOD_lda",
        "name": "LDA",
        "full_name": "Linear Discriminant Analysis",
        "description": "Linear discriminant analysis (LDA), normal discriminant analysis (NDA), or discriminant function analysis is a generalization of Fisher's linear discriminant, a method used in statistics, pattern recognition, and machine learning to find a linear combination of features that characterizes or separates two or more classes of objects or events. The resulting combination may be used as a linear classifier, or, more commonly, for dimensionality reduction before later classification.\nExtracted from Wikipedia\nSource:\nPaper: Linear Discriminant Analysis: A Detailed Tutorial\nPublic version: Linear Discriminant Analysis: A Detailed Tutorial",
        "paper": null,
        "category": [
            "Dimensionality Reduction"
        ],
        "type": "Method",
        "probable_wikipedia": "Linear discriminant analysis",
        "score": "11.532976",
        "probable_description": " Linear discriminant analysis (LDA), normal discriminant analysis (NDA), or discriminant function analysis is a generalization of Fisher's linear discriminant, a method used in statistics, pattern recognition and machine learning to find a linear combination of features that characterizes or separates two or more classes of objects or events. The resulting combination may be used as a linear classifier, or, more commonly, for dimensionality reduction before later classification.  LDA is closely related to analysis of variance (ANOVA) and regression analysis, which also attempt to express one dependent variable as a linear combination of other features or measurements. However, ANOVA uses categorical independent variables and a continuous dependent variable, whereas discriminant analysis has continuous independent variables and a categorical dependent variable (\"i.e.\" the class label). Logistic regression and probit regression are more similar to LDA than ANOVA is, as they also explain a categorical variable by the values of continuous independent variables. These other methods are preferable in applications where it is not reasonable to assume that the independent variables are normally distributed, which is a fundamental assumption of the LDA method.  LDA is also closely related to principal component analysis (PCA) and factor analysis in that they both look for linear combinations of variables which best explain the data. LDA explicitly attempts to model the difference between the classes of data. PCA on the other hand does not take into account any difference in class, and factor analysis builds the feature combinations based on differences rather than similarities. Discriminant analysis is also different from"
    },
    {
        "id": "METHOD_linear-regression",
        "name": "Linear Regression",
        "full_name": "Linear Regression",
        "description": "Linear Regression is a method for modelling a relationship between a dependent variable and independent variables. These models can be fit with numerous approaches. The most common is least squares, where we minimize the mean square error between the predicted values $\\hat{y} = \\textbf{X}\\hat{\\beta}$ and actual values $y$: $\\left(y-\\textbf{X}\\beta\\right)^{2}$.\nWe can also define the problem in probabilistic terms as a generalized linear model (GLM) where the pdf is a Gaussian distribution, and then perform maximum likelihood estimation to estimate $\\hat{\\beta}$.\nImage Source: Wikipedia",
        "paper": null,
        "category": [
            "Generalized Linear Models"
        ],
        "type": "Method",
        "probable_wikipedia": "Linear regression",
        "score": "12.226955",
        "probable_description": " In statistics, linear regression is a linear approach to modeling the relationship between a scalar response (or dependent variable) and one or more explanatory variables (or independent variables). The case of one explanatory variable is called simple linear regression. For more than one explanatory variable, the process is called multiple linear regression. This term is distinct from multivariate linear regression, where multiple correlated dependent variables are predicted, rather than a single scalar variable.  In linear regression, the relationships are modeled using linear predictor functions whose unknown model parameters are estimated from the data. Such models are called linear models. Most commonly, the conditional mean of the response given the values of the explanatory variables (or predictors) is assumed to be an affine function of those values; less commonly, the conditional median or some other quantile is used. Like all forms of regression analysis, linear regression focuses on the conditional probability distribution of the response given the values of the predictors, rather than on the joint probability distribution of all of these variables, which is the domain of multivariate analysis.  Linear regression was the first type of regression analysis to be studied rigorously, and to be used extensively in practical applications. This is because models which depend linearly on their unknown parameters are easier to fit than models which are non-linearly related to their parameters and because the statistical properties of the resulting estimators are easier to determine.  Linear regression has many practical uses. Most applications fall into one of the following two"
    },
    {
        "id": "METHOD_logistic-regression",
        "name": "Logistic Regression",
        "full_name": "Logistic Regression",
        "description": "Logistic Regression, despite its name, is a linear model for classification rather than regression. Logistic regression is also known in the literature as logit regression, maximum-entropy classification (MaxEnt) or the log-linear classifier. In this model, the probabilities describing the possible outcomes of a single trial are modeled using a logistic function.\nSource: scikit-learn\nImage: Michaelg2015",
        "paper": null,
        "category": [
            "Generalized Linear Models"
        ],
        "type": "Method",
        "probable_wikipedia": "Logistic regression",
        "score": "10.677626",
        "probable_description": " In statistics, the logistic model (or logit model) is used to model the probability of a certain class or event existing such as pass/fail, win/lose, alive/dead or healthy/sick. This can be extended to model several classes of events such as determining whether an image contains a cat, dog, lion, etc... Each object being detected in the image would be assigned a probability between 0 and 1 and the sum adding to one.  Logistic regression is a statistical model that in its basic form uses a logistic function to model a binary dependent variable, although many more complex extensions exist. In regression analysis, logistic regression (or logit regression) is estimating the parameters of a logistic model (a form of binary regression). Mathematically, a binary logistic model has a dependent variable with two possible values, such as pass/fail which is represented by an indicator variable, where the two values are labeled \"0\" and \"1\". In the logistic model, the log-odds (the logarithm of the odds) for the value labeled \"1\" is a linear combination of one or more independent variables (\"predictors\"); the independent variables can each be a binary variable (two classes, coded by an indicator variable) or a continuous variable (any real value). The corresponding probability of the value labeled \"1\" can vary between 0 (certainly the value \"0\") and 1 (certainly the value \"1\"), hence the labeling; the function that converts log-odds to probability is the logistic function, hence the name. The unit of measurement for the log-odds scale is called a \"logit\", from"
    },
    {
        "id": "METHOD_lstm",
        "name": "LSTM",
        "full_name": "Long Short-Term Memory",
        "description": "An LSTM is a type of recurrent neural network that addresses the vanishing gradient problem in vanilla RNNs through additional cells, input and output gates. Intuitively, vanishing gradients are solved through additional additive components, and forget gate activations, that allow the gradients to flow through the network without vanishing as quickly.\n(Image Source here)\n(Introduced by Hochreiter and Schmidhuber)",
        "proposed_in": {
            "s2_paper_id": "44d2abe2175df8153f465f6c39b68b76a0d40ab9"
        },
        "category": [
            "Recurrent Neural Networks"
        ],
        "type": "Method",
        "probable_wikipedia": "Long short-term memory",
        "score": "12.611852",
        "probable_description": " Long short-term memory (LSTM) is an artificial recurrent neural network (RNN) architecture used in the field of deep learning. Unlike standard feedforward neural networks, LSTM has feedback connections that make it a \"general purpose computer\" (that is, it can compute anything that a Turing machine can). It can not only process single data points (such as images), but also entire sequences of data (such as speech or video). For example, LSTM is applicable to tasks such as unsegmented, connected handwriting recognition or speech recognition. Bloomberg Business Week wrote: \"These powers make LSTM arguably the most commercial AI achievement, used for everything from predicting diseases to composing music.\"  A common LSTM unit is composed of a cell, an input gate, an output gate and a forget gate. The cell remembers values over arbitrary time intervals and the three \"gates\" regulate the flow of information into and out of the cell.  LSTM networks are well-suited to classifying, processing and making predictions based on time series data, since there can be lags of unknown duration between important events in a time series. LSTMs were developed to deal with the exploding and vanishing gradient problems that can be encountered when training traditional RNNs. Relative insensitivity to gap length is an advantage of LSTM over RNNs, hidden Markov models and other sequence learning methods in numerous applications. "
    },
    {
        "id": "METHOD_macaw",
        "name": "Macaw",
        "full_name": "Macaw",
        "description": "Macaw is a generative question-answering (QA) system that is built on UnifiedQA, itself built on T5. Macaw has three interesting features. First, it often produces high-quality answers to questions far outside the domain it was trained on, sometimes surprisingly so. Second, Macaw allows different permutations (\u201can gles\u201d) of inputs and outputs to be used. For example, we can give it a question and get an answer; or give it an answer and get a question; or give it a question and answer and get a set of multiple-choice (MC) options for that question. This multi-angle QA capability allows versatility in the way Macaw can be used, include recursively using outputs as new inputs to the system. Finally, Macaw also generates explanations as an optional output (or even input) element.",
        "category": [
            "Question Answering Models"
        ],
        "proposed_in": {
            "paper_id": "general-purpose-question-answering-with-macaw",
            "s2_paper_id": "e3480d9395e692833b722b2e957d51139985f310"
        },
        "type": "Method",
        "probable_wikipedia": "Macaw",
        "score": "3.6562607",
        "probable_description": " Macaws are long-tailed, often colorful New World parrots. "
    },
    {
        "id": "METHOD_matrixnet",
        "name": "MatrixNet",
        "full_name": "MatrixNet",
        "description": "MatrixNet is a scale and aspect ratio aware building block for object detection that seek to handle objects of different sizes and aspect ratios. They have several matrix layers, each layer handles an object of specific size and aspect ratio. They can be seen as an alternative to FPNs. While FPNs are capable of handling objects of different sizes, they do not have a solution for objects of different aspect ratios. Objects such as a high tower, a giraffe, or a knife introduce a design difficulty for FPNs: does one map these objects to layers according to their width or height? Assigning the object to a layer according to its larger dimension would result in loss of information along the smaller dimension due to aggressive downsampling, and vice versa. \nMatrixNets assign objects of different sizes and aspect ratios to layers such that object sizes within their assigned layers are close to uniform. This assignment allows a square output convolution kernel to equally gather information about objects of all aspect ratios and scales. MatrixNets can be applied to any backbone, similar to FPNs. We denote this by appending a \"-X\" to the backbone, i.e. ResNet50-X.",
        "category": [
            "Feature Extractors"
        ],
        "proposed_in": {
            "paper_id": "matrixnets-a-new-scale-and-aspect-ratio-aware",
            "s2_paper_id": "f8b82a28280f5fe0510c03ab1c0b345abd543360"
        },
        "type": "Method",
        "probable_wikipedia": "MatrixNet",
        "score": "11.300986",
        "probable_description": " MatrixNet is a proprietary machine learning algorithm developed by Yandex and used widely throughout the company products. The algorithm is based on gradient boosting and was introduced since 2009. "
    },
    {
        "id": "METHOD_mdl",
        "name": "MDL",
        "full_name": "Minimum Description Length",
        "description": "Minimum Description Length provides a criterion for the selection of models, regardless of their complexity, without the restrictive assumption that the data form a sample from a 'true' distribution.\nExtracted from scholarpedia\nSource:\nPaper: J. Rissanen (1978) Modeling by the shortest data description. Automatica 14, 465-471\nBook: P. D. Gr\u00fcnwald (2007) The Minimum Description Length Principle, MIT Press, June 2007, 570 pages",
        "paper": null,
        "category": [
            "AutoML"
        ],
        "type": "Method",
        "probable_wikipedia": "Minimum description length",
        "score": "12.002715",
        "probable_description": " The minimum description length (MDL) principle is a formalization of Occam's razor in which the best hypothesis (a model and its parameters) for a given set of data is the one that leads to the best compression of the data. MDL was introduced by Jorma Rissanen in 1978. It is an important concept in information theory and computational learning theory. "
    },
    {
        "id": "METHOD_meena",
        "name": "Meena",
        "full_name": "Meena",
        "description": "Meena is a multi-turn open-domain chatbot trained end-to-end on data mined and filtered from public domain social media conversations. This 2.6B parameter neural network is simply trained to minimize perplexity of the next token. A seq2seq model is used with the Evolved Transformer as the main architecture. The model is trained on multi-turn conversations where the input sequence is all turns of the context and the output sequence is the response.",
        "category": [
            "Open-Domain Chatbots"
        ],
        "proposed_in": {
            "paper_id": "towards-a-human-like-open-domain-chatbot",
            "s2_paper_id": "e8961cfbb73313b01f2b768650d8851a015e8d18"
        },
        "type": "Method",
        "probable_wikipedia": "Meena",
        "score": "0.79737294",
        "probable_description": " The Meena () is a tribe found mainly in the Rajasthan and Madhya Pradesh regions of India. Its name is also transliterated as \"Meenanda\" or \"Mina\". The Meenas claim connection to the Matsya avatar of Vishnu, and the ancient Matsya Kingdom. "
    },
    {
        "id": "METHOD_metropolis-hastings",
        "name": "Metropolis Hastings",
        "full_name": "Metropolis Hastings",
        "description": "Metropolis-Hastings is a Markov Chain Monte Carlo (MCMC) algorithm for approximate inference. It allows for sampling from a probability distribution where direct sampling is difficult - usually owing to the presence of an intractable integral.\nM-H consists of a proposal distribution $q\\left(\\theta^{'}\\mid\\theta\\right)$ to draw a parameter value. To decide whether $\\theta^{'}$ is accepted or rejected, we then calculate a ratio:\n$$ \\frac{p\\left(\\theta^{'}\\mid{D}\\right)}{p\\left(\\theta\\mid{D}\\right)} $$\nWe then draw a random number $r \\in \\left[0, 1\\right]$ and accept if it is under the ratio, reject otherwise. If we accept, we set $\\theta_{i} = \\theta^{'}$ and repeat.\nBy the end we have a sample of $\\theta$ values that we can use to form quantities over an approximate posterior, such as the expectation and uncertainty bounds. In practice, we typically have a period of tuning to achieve an acceptable acceptance ratio for the algorithm, as well as a warmup period to reduce bias towards initialization values.\nImage: Samuel Hudec",
        "paper": null,
        "category": [
            "Markov Chain Monte Carlo",
            "Approximate Inference"
        ],
        "type": "Method",
        "probable_wikipedia": "Metropolis\u2013Hastings algorithm",
        "score": "7.988917",
        "probable_description": " In statistics and statistical physics, the Metropolis\u2013Hastings algorithm is a Markov chain Monte Carlo (MCMC) method for obtaining a sequence of random samples from a probability distribution from which direct sampling is difficult. This sequence can be used to approximate the distribution (e.g. to generate a histogram) or to compute an integral (e.g. an expected value). Metropolis\u2013Hastings and other MCMC algorithms are generally used for sampling from multi-dimensional distributions, especially when the number of dimensions is high. For single-dimensional distributions, there are usually other methods (e.g. adaptive rejection sampling) that can directly return independent samples from the distribution, and these are free from the problem of autocorrelated samples that is inherent in MCMC methods. "
    },
    {
        "id": "METHOD_monte-carlo-tree-search",
        "name": "Monte-Carlo Tree Search",
        "full_name": "Monte-Carlo Tree Search",
        "description": "Monte-Carlo Tree Search is a planning algorithm that accumulates value estimates obtained from Monte Carlo simulations in order to successively direct simulations towards more highly-rewarded trajectories. We execute MCTS after encountering each new state to select an agent's action for that state: it is executed again to select the action for the next state. Each execution is an iterative process that simulates many trajectories starting from the current state to the terminal state. The core idea is to successively focus multiple simulations starting at the current state by extending the initial portions of trajectories that have received high evaluations from earlier simulations.\nSource: Sutton and Barto, Reinforcement Learning (2nd Edition)\nImage Credit: Chaslot et al",
        "paper": null,
        "category": [
            "Heuristic Search Algorithms"
        ],
        "type": "Method",
        "probable_wikipedia": "Monte Carlo tree search",
        "score": "10.372815",
        "probable_description": " In computer science, Monte Carlo tree search (MCTS) is a heuristic search algorithm for some kinds of decision processes, most notably those employed in game play. MCTS was introduced in 2006 for computer Go. It has been used in other board games like chess and shogi, games with incomplete information such as bridge and poker, as well as in real-time video games (such as 's implementation in the high level campaign AI). "
    },
    {
        "id": "METHOD_neural-architecture-search",
        "name": "Neural Architecture Search",
        "full_name": "Neural Architecture Search",
        "description": "Neural Architecture Search (NAS) learns a modular architecture which can be transferred from a small dataset to a large dataset. The method does this by reducing the problem of learning best convolutional architectures to the problem of learning a small convolutional cell. The cell can then be stacked in series to handle larger images and more complex datasets.\nNote that this refers to the original method referred to as NAS - there is also a broader category of methods called \"neural architecture search\".",
        "category": [
            "Neural Architecture Search"
        ],
        "proposed_in": {
            "paper_id": "learning-transferable-architectures-for",
            "s2_paper_id": "d0611891b9e8a7c5731146097b6f201578f47b2f"
        },
        "type": "Method",
        "probable_wikipedia": "Neural architecture search",
        "score": "11.825397",
        "probable_description": " Neural architecture search (NAS) is a technique for automating the design of artificial neural networks (ANN), a widely used model in the field of machine learning. NAS has been used to design networks that are on par or outperform hand-designed architectures. Methods for NAS can be categorized according to the search space, search strategy and performance estimation strategy used:   NAS is closely related to hyperparameter optimization and is a subfield of automated machine learning (AutoML). "
    },
    {
        "id": "METHOD_neural-turing-machine",
        "name": "Neural Turing Machine",
        "full_name": "Neural Turing Machine",
        "description": "A Neural Turing Machine is a working memory neural network model. It couples a neural network architecture with external memory resources. The whole architecture is differentiable end-to-end with gradient descent. The models can infer tasks such as copying, sorting and associative recall.\nA Neural Turing Machine (NTM) architecture contains two basic components: a neural\nnetwork controller and a memory bank. The Figure presents a high-level diagram of the NTM\narchitecture. Like most neural networks, the controller interacts with the external world via\ninput and output vectors. Unlike a standard network, it also interacts with a memory matrix\nusing selective read and write operations. By analogy to the Turing machine we refer to the\nnetwork outputs that parameterise these operations as \u201cheads.\u201d\nEvery component of the architecture is differentiable. This is achieved by defining 'blurry' read and write operations that interact to a greater or lesser degree with all the elements in memory (rather\nthan addressing a single element, as in a normal Turing machine or digital computer). The\ndegree of blurriness is determined by an attentional \u201cfocus\u201d mechanism that constrains each\nread and write operation to interact with a small portion of the memory, while ignoring the\nrest. Because interaction with the memory is highly sparse, the NTM is biased towards\nstoring data without interference. The memory location brought into attentional focus is\ndetermined by specialised outputs emitted by the heads. These outputs define a normalised\nweighting over the rows in the memory matrix (referred to as memory \u201clocations\u201d). Each\nweighting, one per read or write head, defines the degree to which the head reads or writes\nat each location. A head can thereby attend sharply to the memory at a single location or\nweakly to the memory at many locations",
        "category": [
            "Recurrent Neural Networks"
        ],
        "proposed_in": {
            "paper_id": "neural-turing-machines",
            "s2_paper_id": "c3823aacea60bc1f2cabb9283144690a3d015db5"
        },
        "type": "Method",
        "probable_wikipedia": "Neural Turing machine",
        "score": "11.950575",
        "probable_description": " A Neural Turing machine (NTMs) is a recurrent neural network model published by Alex Graves et. al. in 2014. NTMs combine the fuzzy pattern matching capabilities of neural networks with the algorithmic power of programmable computers. An NTM has a neural network controller coupled to external memory resources, which it interacts with through attentional mechanisms. The memory interactions are differentiable end-to-end, making it possible to optimize them using gradient descent. An NTM with a long short-term memory (LSTM) network controller can infer simple algorithms such as copying, sorting, and associative recall from input and output examples. They can infer algorithms from input and output examples alone.  The authors of the original NTM paper did not publish the source code for their implementation. The first stable open-source implementation of a Neural Turing Machine was published in 2018 at the 27th International Conference on Artificial Neural Networks, receiving a best-paper award. Other open source implementations of NTMs exist but are not stable for production use. The developers either report that the gradients of their implementation sometimes become NaN during training for unknown reasons and causing training to fail; report slow convergence; or do not report the speed of learning of their implementation at all.  Differentiable neural computers are an outgrowth of neural Turing machines, with attention mechanisms that control where the memory is active, and improved performance.   "
    },
    {
        "id": "METHOD_oasis",
        "name": "OASIS",
        "full_name": "OASIS",
        "description": "OASIS is a GAN-based model to translate semantic label maps into realistic-looking images. The model builds on preceding work such as Pix2Pix and SPADE. OASIS introduces the following innovations:  \n\n\nThe method is not dependent on the perceptual loss, which is commonly used for the semantic image synthesis task. A VGG network trained on ImageNet is routinely employed as the perceptual loss to strongly improve the synthesis quality. The authors show that this perceptual loss also has negative effects: First, it reduces the diversity of the generated images. Second, it negatively influences the color distribution to be more biased towards ImageNet. OASIS eliminates the dependence on the perceptual loss by changing the common discriminator design: The OASIS discriminator segments an image into one of the real classes or an additional fake class. In doing so, it makes more efficient use of the label maps that the discriminator normally receives. This distinguishes the discriminator from the commonly used encoder-shaped discriminators, which concatenate the label maps to the input image and predict a single score per image. With the more fine-grained supervision through the loss of the OASIS discriminator, the perceptual loss is shown to become unnecessary.\n\n\nA user can generate a diverse set of images per label map by simply resampling noise. This is achieved by conditioning the spatially-adaptive denormalization module in each layer of the GAN generator directly on spatially replicated input noise. A side effect of this conditioning is that at inference time an image can be resampled either globally or locally (either the complete image changes or a restricted region in the image).\n\n",
        "category": [
            "Conditional Image-to-Image Translation Models"
        ],
        "proposed_in": {
            "paper_id": "you-only-need-adversarial-supervision-for-1",
            "s2_paper_id": "8f7d4d61292886c111b1fe11f524ecaa8101de27"
        },
        "type": "Method",
        "probable_wikipedia": "OASIS (organization)",
        "score": "6.8034124",
        "probable_description": " The Organization for the Advancement of Structured Information Standards (OASIS) is a global nonprofit consortium that works on the development, convergence, and adoption of open standards for security, Internet of Things, energy, content technologies, emergency management, and other areas. "
    },
    {
        "id": "METHOD_ontology",
        "name": "Ontology",
        "full_name": "Ontology",
        "description": "",
        "category": [
            "Knowledge Distillation"
        ],
        "proposed_in": {
            "paper_id": "ontology-based-production-simulation-with",
            "s2_paper_id": "ccec5459ae25a2242d6779fb161efc97cc8fb522"
        },
        "type": "Method",
        "probable_wikipedia": "Ontology",
        "score": "10.145287",
        "probable_description": " Ontology is the philosophical study of being. More broadly, it studies concepts that directly relate to being, in particular becoming, existence, reality, as well as the basic categories of being and their relations. Traditionally listed as a part of the major branch of philosophy known as metaphysics, ontology often deals with questions concerning what entities exist or may be said to exist and how such entities may be grouped, related within a hierarchy, and subdivided according to similarities and differences. "
    },
    {
        "id": "METHOD_parallax",
        "name": "Parallax",
        "full_name": "Parallax",
        "description": "Parallax is a hybrid parallel method for training large neural networks. Parallax is a framework that optimizes data parallel training by utilizing the sparsity of model parameters. Parallax introduces a hybrid approach that combines Parameter Server and AllReduce architectures to optimize the amount of data transfer according to the sparsity.\nParallax pursues a hybrid approach that uses the Parameter Server architecture for handling sparse variables and the AllReduce architecture for handling dense variables. Moreover, Parallax partitions large sparse variables by a near-optimal number of partitions to maximize parallelism while maintaining low computation and communication overhead. Parallax further optimizes training with local aggregation and smart operation placement to mitigate communication overhead. Graph transformation in Parallax automatically applies all of these optimizations and the data parallel training itself at the framework level to minimize user efforts for writing and optimizing a distributed program by composing low-level primitives.",
        "paper": null,
        "category": [
            "Parameter Server Methods"
        ],
        "type": "Method",
        "probable_wikipedia": "Parallax",
        "score": "6.329577",
        "probable_description": " Parallax () is a displacement or difference in the apparent position of an object viewed along two different lines of sight, and is measured by the angle or semi-angle of inclination between those two lines. Due to foreshortening, nearby objects show a larger parallax than farther objects when observed from different positions, so parallax can be used to determine distances.  To measure large distances, such as the distance of a planet or a star from Earth, astronomers use the principle of parallax. Here, the term \"parallax\" is the semi-angle of inclination between two sight-lines to the star, as observed when Earth is on opposite sides of the Sun in its orbit. These distances form the lowest rung of what is called \"the cosmic distance ladder\", the first in a succession of methods by which astronomers determine the distances to celestial objects, serving as a basis for other distance measurements in astronomy forming the higher rungs of the ladder.  Parallax also affects optical instruments such as rifle scopes, binoculars, microscopes, and twin-lens reflex cameras that view objects from slightly different angles. Many animals, including humans, have two eyes with overlapping visual fields that use parallax to gain depth perception; this process is known as stereopsis. In computer vision the effect is used for computer stereo vision, and there is a device called a parallax rangefinder that uses it to find range, and in some variations also altitude to a target.  A simple everyday example of parallax can be seen in the dashboard of"
    },
    {
        "id": "METHOD_parrot",
        "name": "Parrot",
        "full_name": "Parrot",
        "description": "Parrot is an imitation learning approach to automatically learn cache access patterns by leveraging Belady\u2019s optimal policy. Belady\u2019s optimal policy is an oracle policy that computes the theoretically optimal cache eviction decision based on knowledge of future cache accesses, which Parrot approximates with a policy that only conditions on the past accesses.",
        "category": [
            "Imitation Learning Methods"
        ],
        "proposed_in": {
            "paper_id": "an-imitation-learning-approach-for-cache",
            "s2_paper_id": "ea7ddd0c4f050514ccb288b8fd4c0408805cdf00"
        },
        "type": "Method",
        "probable_wikipedia": "Parrot",
        "score": "4.6304803",
        "probable_description": " Parrots, also known as psittacines , are birds of the roughly 393 species in 92 genera that make up the order Psittaciformes, found in most tropical and subtropical regions. The order is subdivided into three superfamilies: the Psittacoidea (\"true\" parrots), the Cacatuoidea (cockatoos), and the Strigopoidea (New Zealand parrots). Parrots have a generally pantropical distribution with several species inhabiting temperate regions in the Southern Hemisphere, as well. The greatest diversity of parrots is in South America and Australasia.  Characteristic features of parrots include a strong, curved bill, an upright stance, strong legs, and clawed zygodactyl feet. Many parrots are vividly coloured, and some are multi-coloured. Most parrots exhibit little or no sexual dimorphism in the visual spectrum. They form the most variably sized bird order in terms of length. The most important components of most parrots' diets are seeds, nuts, fruit, buds, and other plant material. A few species sometimes eat animals and carrion, while the lories and lorikeets are specialised for feeding on floral nectar and soft fruits. Almost all parrots nest in tree hollows (or nest boxes in captivity), and lay white eggs from which hatch altricial (helpless) young.  Parrots, along with ravens, crows, jays, and magpies, are among the most intelligent birds, and the ability of some species to imitate human voices enhances their popularity as pets. Trapping wild parrots for the pet trade, as well as hunting, habitat loss, and competition from invasive species, has diminished wild populations, with parrots being subjected to more exploitation than any other group"
    },
    {
        "id": "METHOD_pca",
        "name": "PCA",
        "full_name": "Principal Components Analysis",
        "description": "Principle Components Analysis (PCA) is an unsupervised method primary used for dimensionality reduction within machine learning.  PCA is calculated via a singular value decomposition (SVD) of the design matrix, or alternatively, by calculating the covariance matrix of the data and performing eigenvalue decomposition on the covariance matrix. The results of PCA provide a low-dimensional picture of the structure of the data and the leading (uncorrelated) latent factors determining variation in the data.\nImage Source: Wikipedia",
        "paper": null,
        "category": [
            "Dimensionality Reduction"
        ],
        "type": "Method",
        "probable_wikipedia": "Principal component analysis",
        "score": "11.4890995",
        "probable_description": " Principal component analysis (PCA) is a statistical procedure that uses an orthogonal transformation to convert a set of observations of possibly correlated variables (entities each of which takes on various numerical values) into a set of values of linearly uncorrelated variables called principal components. This transformation is defined in such a way that the first principal component has the largest possible variance (that is, accounts for as much of the variability in the data as possible), and each succeeding component in turn has the highest variance possible under the constraint that it is orthogonal to the preceding components. The resulting vectors (each being a linear combination of the variables and containing \"n\" observations) are an uncorrelated orthogonal basis set. PCA is sensitive to the relative scaling of the original variables.  PCA was invented in 1901 by Karl Pearson, as an analogue of the principal axis theorem in mechanics; it was later independently developed and named by Harold Hotelling in the 1930s. Depending on the field of application, it is also named the discrete Karhunen\u2013Loeve transform (KLT) in signal processing, the Hotelling transform in multivariate quality control, proper orthogonal decomposition (POD) in mechanical engineering, singular value decomposition (SVD) of X (Golub and Van Loan, 1983), eigenvalue decomposition (EVD) of XX in linear algebra, factor analysis (for a discussion of the differences between PCA and factor analysis see Ch.\u00a07 of Jolliffe's \"Principal Component Analysis\"), Eckart\u2013Young theorem (Harman, 1960), or empirical orthogonal functions (EOF) in meteorological science, empirical eigenfunction decomposition (Sirovich, 1987), empirical component analysis (Lorenz, 1956),"
    },
    {
        "id": "METHOD_phish",
        "name": "Phish",
        "full_name": "Phish",
        "description": "Deep-learning models estimate values using backpropagation. The activation function within hidden layers is a critical component to minimizing loss in deep neural-networks. Rectified Linear (ReLU) has been the dominant activation function for the past decade. Swish and Mish are newer activation functions that have shown to yield better results than ReLU given specific circumstances. Phish is a novel activation function proposed here. It is a composite function defined as f(x) = xTanH(GELU(x)), where no discontinuities are apparent in the differentiated graph on the domain observed. Generalized networks were constructed using different activation functions. SoftMax was the output function. Using images from MNIST and CIFAR-10 databanks, these networks were trained to minimize sparse categorical crossentropy. A large scale cross-validation was simulated using stochastic Markov chains to account for the law of large numbers for the probability values. Statistical tests support the research hypothesis stating Phish could outperform other activation functions in classification. Future experiments would involve testing Phish in unsupervised learning algorithms and comparing it to more activation functions.",
        "category": [
            "Activation Functions"
        ],
        "proposed_in": {
            "paper_id": "phish-a-novel-hyper-optimizable-activation",
            "s2_paper_id": "a35ba431d6e43f4d433c086f57891a1213767701"
        },
        "type": "Method",
        "probable_wikipedia": "Phish",
        "score": "0.23819077",
        "probable_description": " Phish is an American rock band that was founded at the University of Vermont in Burlington, Vermont in 1983. The band is known for musical improvisation, extended jams, blending of genres, and a dedicated fan base. The band consists of guitarist Trey Anastasio, bassist Mike Gordon, drummer Jon Fishman, and keyboardist Page McConnell, all of whom perform vocals, with Anastasio being the primary lead vocalist.  The band was formed by Anastasio, Gordon, Fishman and guitarist Jeff Holdsworth, who were joined by McConnell in 1985. Holdsworth departed the band in 1986, and the quartet lineup has remained in place since then. Their lineup stable, Phish performed together for 15 years before beginning a two-year hiatus in October 2000. The band regrouped in late 2002, but disbanded in August 2004 after a farewell performance at their Coventry Festival in Vermont. They reunited in March 2009 for a series of three consecutive concerts at Hampton Coliseum in Hampton, Virginia, and have since resumed performing regularly.  Phish's music blends elements of a wide variety of genres, including funk, progressive rock, psychedelic rock, folk, country, jazz, blues, bluegrass, and pop. The band was part of a movement of improvisational rock groups, inspired by the Grateful Dead and colloquially known as \"jam bands\", which gained considerable popularity as touring concert acts in the 1990s.  Phish has developed a large and dedicated following by word of mouth, the exchange of live recordings, and selling over 8 million albums and DVDs in the United States. In 1998, \"Rolling Stone\" described"
    },
    {
        "id": "METHOD_predator",
        "name": "PREDATOR",
        "full_name": "PREDATOR",
        "description": "PREDATOR is a model for pairwise point-cloud registration with deep attention to the overlap region. Its key novelty is an overlap-attention block for early information exchange between the latent encodings of the two point clouds. In this way the subsequent decoding of the latent representations into per-point features is conditioned on the respective other point cloud, and thus can predict which points are not only salient, but also lie in the overlap region between the two point clouds.",
        "category": [
            "Point Cloud Models"
        ],
        "proposed_in": {
            "paper_id": "predator-registration-of-3d-point-clouds-with",
            "s2_paper_id": "06d0515d11386bb1572187f81af5abda655c941e"
        },
        "type": "Method",
        "probable_wikipedia": "Predation",
        "score": "5.7742605",
        "probable_description": " Predation is a biological interaction where one organism, the predator, kills and eats another organism, its prey. It is one of a family of common feeding behaviours that includes parasitism and micropredation (which usually do not kill the host) and parasitoidism (which always does, eventually). It is distinct from scavenging on dead prey, though many predators also scavenge; it overlaps with herbivory, as a seed predator is both a predator and a herbivore.  Predators may actively search for prey or sit and wait for it. When prey is detected, the predator assesses whether to attack it. This may involve ambush or pursuit predation, sometimes after stalking the prey. If the attack is successful, the predator kills the prey, removes any inedible parts like the shell or spines, and eats it.  Predators are adapted and often highly specialized for hunting, with acute senses such as vision, hearing, or smell. Many predatory animals, both vertebrate and invertebrate, have sharp claws or jaws to grip, kill, and cut up their prey. Other adaptations include stealth and aggressive mimicry that improve hunting efficiency.  Predation has a powerful selective effect on prey, and the prey develop antipredator adaptations such as warning coloration, alarm calls and other signals, camouflage, mimicry of well-defended species, and defensive spines and chemicals. Sometimes predator and prey find themselves in an evolutionary arms race, a cycle of adaptations and counter-adaptations. Predation has been a major driver of evolution since at least the Cambrian period. "
    },
    {
        "id": "METHOD_pulse",
        "name": "PULSE",
        "full_name": "PULSE",
        "description": "PULSE is a self-supervised photo upsampling algorithm. Instead of starting with the LR image and slowly adding detail, PULSE traverses the high-resolution natural image manifold, searching for images that downscale to the original LR image. This is formalized through the downscaling loss, which guides exploration through the latent space of a generative model. By leveraging properties of high-dimensional Gaussians, the authors aim to restrict the search space to guarantee realistic outputs.",
        "category": [
            "Image Super-Resolution Models"
        ],
        "proposed_in": {
            "paper_id": "pulse-self-supervised-photo-upsampling-via",
            "s2_paper_id": "5be878987dfccd543c73f31db973a81cc738aefa"
        },
        "type": "Method",
        "probable_wikipedia": "Pulse (signal processing)",
        "score": "4.1579494",
        "probable_description": " A pulse in signal processing is a rapid, transient change in the amplitude of a signal from a baseline value to a higher or lower value, followed by a rapid return to the baseline value. "
    },
    {
        "id": "METHOD_q-learning",
        "name": "Q-Learning",
        "full_name": "Q-Learning",
        "description": "Q-Learning is an off-policy temporal difference control algorithm:\n$$Q\\left(S_{t}, A_{t}\\right) \\leftarrow Q\\left(S_{t}, A_{t}\\right) + \\alpha\\left[R_{t+1} + \\gamma\\max_{a}Q\\left(S_{t+1}, a\\right) - Q\\left(S_{t}, A_{t}\\right)\\right] $$\nThe learned action-value function $Q$ directly approximates $q_{*}$, the optimal action-value function, independent of the policy being followed.\nSource: Sutton and Barto, Reinforcement Learning, 2nd Edition",
        "paper": null,
        "category": [
            "Off-Policy TD Control"
        ],
        "type": "Method",
        "probable_wikipedia": "Q-learning",
        "score": "12.356554",
        "probable_description": " \"Q\"-learning is a model-free reinforcement learning algorithm. The goal of Q-learning is to learn a policy, which tells an agent what action to take under what circumstances. It does not require a model (hence the connotation \"model-free\") of the environment, and it can handle problems with stochastic transitions and rewards, without requiring adaptations.  For any finite Markov decision process (FMDP), \"Q\"-learning finds a policy that is optimal in the sense that it maximizes the expected value of the total reward over any and all successive steps, starting from the current state. \"Q\"-learning can identify an optimal action-selection policy for any given FMDP, given infinite exploration time and a partly-random policy. \"Q\" names the function that returns the reward used to provide the reinforcement and can be said to stand for the \"quality\" of an action taken in a given state. "
    },
    {
        "id": "METHOD_random-gaussian-blur",
        "name": "Random Gaussian Blur",
        "full_name": "Random Gaussian Blur",
        "description": "Random Gaussian Blur is an image data augmentation technique where we randomly blur the image using a Gaussian distribution.\nImage Source: Wikipedia",
        "paper": null,
        "category": [
            "Image Data Augmentation"
        ],
        "type": "Method",
        "probable_wikipedia": "Gaussian blur",
        "score": "0.8154462",
        "probable_description": " In image processing, a Gaussian blur (also known as Gaussian smoothing) is the result of blurring an image by a Gaussian function (named after mathematician and scientist Carl Friedrich Gauss). It is a widely used effect in graphics software, typically to reduce image noise and reduce detail. The visual effect of this blurring technique is a smooth blur resembling that of viewing the image through a translucent screen, distinctly different from the bokeh effect produced by an out-of-focus lens or the shadow of an object under usual illumination. Gaussian smoothing is also used as a pre-processing stage in computer vision algorithms in order to enhance image structures at different scales\u2014see scale space representation and scale space implementation.  Mathematically, applying a Gaussian blur to an image is the same as convolving the image with a Gaussian function. This is also known as a two-dimensional Weierstrass transform. By contrast, convolving by a circle (i.e., a circular box blur) would more accurately reproduce the bokeh effect. Since the Fourier transform of a Gaussian is another Gaussian, applying a Gaussian blur has the effect of reducing the image's high-frequency components; a Gaussian blur is thus a low pass filter. "
    },
    {
        "id": "METHOD_random-search",
        "name": "Random Search",
        "full_name": "Random Search",
        "description": "Random Search replaces the exhaustive enumeration of all combinations by selecting them randomly. This can be simply applied to the discrete setting described above, but also generalizes to continuous and mixed spaces. It can outperform Grid search, especially when only a small number of hyperparameters affects the final performance of the machine learning algorithm. In this case, the optimization problem is said to have a low intrinsic dimensionality. Random Search is also embarrassingly parallel, and additionally allows the inclusion of prior knowledge by specifying the distribution from which to sample.\nExtracted from Wikipedia\nSource Paper\nImage Source: BERGSTRA AND BENGIO",
        "paper": null,
        "category": [
            "Hyperparameter Search",
            "Optimization"
        ],
        "type": "Method",
        "probable_wikipedia": "Random search",
        "score": "8.861136",
        "probable_description": " Random search (RS) is a family of numerical optimization methods that do not require the gradient of the problem to be optimized, and RS can hence be used on functions that are not continuous or differentiable. Such optimization methods are also known as direct-search, derivative-free, or black-box methods.  The name \"random search\" is attributed to Rastrigin who made an early presentation of RS along with basic mathematical analysis. RS works by iteratively moving to better positions in the search-space, which are sampled from a hypersphere surrounding the current position. "
    },
    {
        "id": "METHOD_relic",
        "name": "ReLIC",
        "full_name": "ReLIC",
        "description": "ReLIC, or Representation Learning via Invariant Causal Mechanisms, is a self-supervised learning objective that enforces invariant prediction of proxy targets across augmentations through an invariance regularizer which yields improved generalization guarantees. \nWe can write the objective as:\n$$\n\\underset{X}{\\mathbb{E}} \\underset{\\sim_{l k}, a_{q \\mathcal{A}}}{\\mathbb{E}} \\sum_{b \\in\\left(a_{l k}, a_{q t}\\right)} \\mathcal{L}_{b}\\left(Y^{R}, f(X)\\right) \\text { s.t. } K L\\left(p^{d o\\left(a_{l k}\\right)}\\left(Y^{R} \\mid f(X)\\right), p^{d o\\left(a_{q t}\\right)}\\left(Y^{R} \\mid f(X)\\right)\\right) \\leq \\rho\n$$\nwhere $\\mathcal{L}$ is the proxy task loss and $K L$ is the Kullback-Leibler (KL) divergence. Note that any distance measure on distributions can be used in place of the KL divergence.\nConcretely, as proxy task we associate to every datapoint $x_{i}$ the label $y_{i}^{R}=i$. This corresponds to the instance discrimination task, commonly used in contrastive learning. We take pairs of points $\\left(x_{i}, x_{j}\\right)$ to compute similarity scores and use pairs of augmentations $a_{l k}=\\left(a_{l}, a_{k}\\right) \\in$ $\\mathcal{A} \\times \\mathcal{A}$ to perform a style intervention. Given a batch of samples $\\left(x_{i}\\right)_{i=1}^{N} \\sim \\mathcal{D}$, we use\n$$\np^{d o\\left(a_{l k}\\right)}\\left(Y^{R}=j \\mid f\\left(x_{i}\\right)\\right) \\propto \\exp \\left(\\phi\\left(f\\left(x_{i}^{a_{l}}\\right), h\\left(x_{j}^{a_{k}}\\right)\\right) / \\tau\\right)\n$$\nwith $x^{a}$ data augmented with $a$ and $\\tau$ a softmax temperature parameter. We encode $f$ using a neural network and choose $h$ to be related to $f$, e.g. $h=f$ or as a network with an exponential moving average of the weights of $f$ (e.g. target networks). To compare representations we use the function $\\phi\\left(f\\left(x_{i}\\right), h\\left(x_{j}\\right)\\right)=\\left\\langle g\\left(f\\left(x_{i}\\right)\\right), g\\left(h\\left(x_{j}\\right)\\right)\\right\\rangle$ where $g$ is a fully-connected neural network often called the critic.\nCombining these pieces, we learn representations by minimizing the following objective over the full set of data $x_{i} \\in \\mathcal{D}$ and augmentations $a_{l k} \\in \\mathcal{A} \\times \\mathcal{A}$\n$$\n-\\sum_{i=1}^{N} \\sum_{a_{l k}} \\log \\frac{\\exp \\left(\\phi\\left(f\\left(x_{i}^{a_{l}}\\right), h\\left(x_{i}^{a_{k}}\\right)\\right) / \\tau\\right)}{\\sum_{m=1}^{M} \\exp \\left(\\phi\\left(f\\left(x_{i}^{a_{l}}\\right), h\\left(x_{m}^{a_{k}}\\right)\\right) / \\tau\\right)}+\\alpha \\sum_{a_{l k}, a_{q t}} K L\\left(p^{d o\\left(a_{l k}\\right)}, p^{d o\\left(a_{q t}\\right)}\\right)\n$$\nwith $M$ the number of points we use to construct the contrast set and $\\alpha$ the weighting of the invariance penalty. The shorthand $p^{d o(a)}$ is used for $p^{d o(a)}\\left(Y^{R}=j \\mid f\\left(x_{i}\\right)\\right)$. The Figure shows a schematic of the RELIC objective.",
        "category": [
            "Self-Supervised Learning"
        ],
        "proposed_in": {
            "paper_id": "representation-learning-via-invariant-causal-1",
            "s2_paper_id": "57835c5ad5424f94ee75901c3113730f3900e656"
        },
        "type": "Method",
        "probable_wikipedia": "Relic",
        "score": "1.1752473",
        "probable_description": " In religion, a relic usually consists of the physical remains of a saint or the personal effects of the saint or venerated person preserved for purposes of veneration as a tangible memorial. Relics are an important aspect of some forms of Buddhism, Christianity, Hinduism, Islam, Shamanism, and many other religions. \"Relic\" derives from the Latin \"reliquiae\", meaning \"remains\", and a form of the Latin verb \"relinquere\", to \"leave behind, or abandon\". A reliquary is a shrine that houses one or more religious relics. "
    },
    {
        "id": "METHOD_rescal",
        "name": "RESCAL",
        "full_name": "RESCAL",
        "description": "RESCAL",
        "category": [
            "Graph Embeddings"
        ],
        "proposed_in": {
            "paper_id": "a-three-way-model-for-collective-learning-on",
            "s2_paper_id": "f6764d853a14b0c34df1d2283e76277aead40fde"
        },
        "type": "Method",
        "probable_wikipedia": "Rescale",
        "score": "2.697292",
        "probable_description": " Founded in 2011 in San Francisco, California by Joris Poort (CEO) and Adam McKenzie (CTO), Rescale is a startup that develops a cloud computing simulation platform. Rescale was a Y Combinator company and has received $6.4M in funding from high-profile investors including Sam Altman, Jeff Bezos, Richard Branson, Chris Dixon, Paul Graham, and Peter Thiel.  The founders previously built a software technology stack at Boeing saving the company over $180M through weight improvements on the 787 Dreamliner.  The Rescale platform combines engineering and science software tools with high performance computing (HPC) to create a cloud simulation environment. \"Design Engineering\" magazine describes Rescale as \"...a good fit for manufacturers who need to run complex simulation and optimization jobs, but don't have the HPC hardware required.\"  \"The Platform\" noted in May, 2015 that Rescale claims the largest known globally available commercial HPC network with \"...eight million servers globally with a potential 1,400 petaflops of compute capacity ... across 30 global datacenters.\"  Rescale has partnerships with engineering software companies in the computer aided engineering space including: ANSYS, CD-adapco, Dassault Systemes, MSC Software, and Siemens.    "
    },
    {
        "id": "METHOD_resnet",
        "name": "ResNet",
        "full_name": "Residual Network",
        "description": "Residual Networks, or ResNets, learn residual functions with reference to the layer inputs, instead of learning unreferenced functions. Instead of hoping each few stacked layers directly fit a desired underlying mapping, residual nets let these layers fit a residual mapping. They stack residual blocks ontop of each other to form network: e.g. a ResNet-50 has fifty layers using these blocks. \nFormally, denoting the desired underlying mapping as $\\mathcal{H}(x)$, we let the stacked nonlinear layers fit another mapping of $\\mathcal{F}(x):=\\mathcal{H}(x)-x$. The original mapping is recast into $\\mathcal{F}(x)+x$.\nThere is empirical evidence that these types of network are easier to optimize, and can gain accuracy from considerably increased depth.",
        "category": [
            "Convolutional Neural Networks",
            "Image Models"
        ],
        "proposed_in": {
            "paper_id": "deep-residual-learning-for-image-recognition",
            "s2_paper_id": "2c03df8b48bf3fa39054345bafabfeff15bfd11d"
        },
        "type": "Method",
        "probable_wikipedia": "Residual neural network",
        "score": "5.5081587",
        "probable_description": " A residual neural network (ResNet) is an artificial neural network (ANN) of a kind that builds on constructs known from pyramidal cells in the cerebral cortex. Residual neural networks do this by utilizing \"skip connections\", or \"short-cuts\" to jump over some layers. Typical \"ResNet\" models are implemented with double- or triple- layer skips that contain nonlinearities (ReLu) and batch normalization in between. An additional weight matrix may be used to learn the skip weights; these models are known as \"HighwayNets\". Models with several parallel skips are referred to as \"DenseNets\". In the context of residual neural networks, a non-residual network may be described as a \"plain network\". One motivation for skipping over layers is to avoid the problem of vanishing gradients, by reusing activations from a previous layer until the adjacent layer learns its weights. During training, the weights adapt to mute the upstream layer, and amplify the previously-skipped layer. In the simplest case, only the weights for the adjacent layer's connection are adapted, with no explicit weights for the upstream layer. This works best when a single non-linear layer is stepped over, or when the intermediate layers are all linear. If not, then an explicit weight matrix should be learned for the skipped connection (a \"HighwayNet\" should be used).  Skipping effectively simplifies the network, using fewer layers in the initial training stages. This speeds learning by reducing the impact of vanishing gradients, as there are fewer layers to propagate through. The network then gradually restores the skipped layers as it learns the feature"
    },
    {
        "id": "METHOD_restricted-boltzmann-machine",
        "name": "Restricted Boltzmann Machine",
        "full_name": "Restricted Boltzmann Machine",
        "description": "Restricted Boltzmann Machines, or RBMs, are two-layer generative neural networks that learn a probability distribution over the inputs. They are a special class of Boltzmann Machine in that they have a restricted number of connections between visible and hidden units. Every node in the visible layer is connected to every node in the hidden layer, but no nodes in the same group are connected. RBMs are usually trained using the contrastive divergence learning procedure.\nImage Source: here",
        "paper": null,
        "category": [
            "Generative Models"
        ],
        "type": "Method",
        "probable_wikipedia": "Restricted Boltzmann machine",
        "score": "12.170155",
        "probable_description": " A restricted Boltzmann machine (RBM) is a generative stochastic artificial neural network that can learn a probability distribution over its set of inputs.  RBMs were initially invented under the name Harmonium by Paul Smolensky in 1986, and rose to prominence after Geoffrey Hinton and collaborators invented fast learning algorithms for them in the mid-2000. RBMs have found applications in dimensionality reduction, classification, collaborative filtering, feature learning, topic modelling and even many body quantum mechanics. They can be trained in either supervised or unsupervised ways, depending on the task.  As their name implies, RBMs are a variant of Boltzmann machines, with the restriction that their neurons must form a bipartite graph: a pair of nodes from each of the two groups of units (commonly referred to as the \"visible\" and \"hidden\" units respectively) may have a symmetric connection between them; and there are no connections between nodes within a group. By contrast, \"unrestricted\" Boltzmann machines may have connections between hidden units. This restriction allows for more efficient training algorithms than are available for the general class of Boltzmann machines, in particular the gradient-based contrastive divergence algorithm.  Restricted Boltzmann machines can also be used in deep learning networks. In particular, deep belief networks can be formed by \"stacking\" RBMs and optionally fine-tuning the resulting deep network with gradient descent and backpropagation. "
    },
    {
        "id": "METHOD_rife",
        "name": "RIFE",
        "full_name": "RIFE",
        "description": "RIFE, or Real-time Intermediate Flow Estimation is an intermediate flow estimation algorithm for Video Frame Interpolation (VFI). Many recent flow-based VFI methods first estimate the bi-directional optical flows, then scale and reverse them to approximate intermediate flows, leading to artifacts on motion boundaries. RIFE uses a neural network named IFNet that can directly estimate the intermediate flows from coarse-to-fine with much better speed. It introduces a privileged distillation scheme for training intermediate flow model, which leads to a large performance improvement.\nIn RIFE training, given two input frames $I_{0}, I_{1}$, we directly feed them into the IFNet to approximate intermediate flows $F_{t \\rightarrow 0}, F_{t \\rightarrow 1}$ and the fusion map $M$. During training phase, a privileged teacher refines student's results to get $F_{t \\rightarrow 0}^{T e a}, F_{t \\rightarrow 1}^{T e a}$ and $M^{\\text {Tea }}$ based on ground truth $I_{t}$. The student model and the teacher model are jointly trained from scratch using the reconstruction loss. The teacher's approximations are more accurate so that they can guide the student to learn.",
        "category": [
            "Video Frame Interpolation"
        ],
        "proposed_in": {
            "paper_id": "rife-real-time-intermediate-flow-estimation",
            "s2_paper_id": "3ede5c472bea1d1dfab0fd3dbb5422f402962622"
        },
        "type": "Method",
        "probable_wikipedia": "RIFE",
        "score": "8.6496315",
        "probable_description": " RIFE is a content management framework designed for rapid web application development in Java, without using J2EE.  RIFE's design blends together in a consistent component object model two approaches, request-based and component-based. Through a centralized site structure (web engine), an application can be split up into reusable binary modules that can be integrated into other projects. "
    },
    {
        "id": "METHOD_saga",
        "name": "SAGA",
        "full_name": "SAGA",
        "description": "SAGA is a method in the spirit of SAG, SDCA, MISO and SVRG, a set of recently proposed incremental gradient algorithms with fast linear convergence rates. SAGA improves on the theory behind SAG and SVRG, with better theoretical convergence rates, and has support for composite objectives where a proximal operator is used on the regulariser. Unlike SDCA, SAGA supports non-strongly convex problems directly, and is adaptive to any inherent strong convexity of the problem.",
        "category": [
            "Optimization"
        ],
        "proposed_in": {
            "paper_id": "saga-a-fast-incremental-gradient-method-with",
            "s2_paper_id": "4daec165c1f4aa1206b0d91c0b26f0287d1ef52d"
        },
        "type": "Method",
        "probable_wikipedia": "SAGA GIS",
        "score": "1.1729032",
        "probable_description": " System for Automated Geoscientific Analyses (SAGA GIS) is a geographic information system (GIS) computer program, used to edit spatial data. It is free and open-source software, developed originally by a small team at the Department of Physical Geography, University of Gottingen, Germany, and is now being maintained and extended by an international developer community.  SAGA GIS is intended to give scientists an effective but easily learnable platform for implementing geoscientific methods. This is achieved by the application programming interface (API). SAGA has a fast-growing set of geoscientific methods, bundled in exchangeable module libraries.  The standard modules are:   SAGA GIS is an effective tool with user friendly graphical user interface (GUI) that requires only about 10\u00a0MB disk space. No installation is needed, since SAGA GIS can be run directly from a USB thumb drive if desired.  SAGA GIS is available for Windows, Linux, and FreeBSD.  SAGA GIS can be used together with other GIS software like Kosmo and QGIS in order to obtain enhanced detail in vector datasets as well as higher-resolution map-production capabilities. SAGA GIS modules can be executed from within the statistical data analysis software R, in order to integrate statistical and GIS analyses.    "
    },
    {
        "id": "METHOD_scarf",
        "name": "SCARF",
        "full_name": "SCARF",
        "description": "SCARF is a simple, widely-applicable technique for contrastive learning, where views are formed by corrupting a random subset of features. When applied to pre-train deep neural networks on the 69 real-world, tabular classification datasets from the OpenML-CC18 benchmark, SCARF not only improves classification accuracy in the fully-supervised setting but does so also in the presence of label noise and in the semi-supervised setting where only a fraction of the available training data is labeled.",
        "category": [
            "Deep Tabular Learning"
        ],
        "proposed_in": {
            "paper_id": "scarf-self-supervised-contrastive-learning",
            "s2_paper_id": "7f79fa7994db766a477f35246f57a9ff9c0604e8"
        },
        "type": "Method",
        "probable_wikipedia": "Scarf",
        "score": "8.611633",
        "probable_description": " A scarf, plural \"scarves\", is a piece of fabric worn around the neck for warmth, sun protection, cleanliness, fashion, or religious reasons. They can be made in a variety of different materials such as wool, linen or cotton. It is a common type of neckwear. "
    },
    {
        "id": "METHOD_sgd",
        "name": "SGD",
        "full_name": "Stochastic Gradient Descent",
        "description": "Stochastic Gradient Descent is an iterative optimization technique that uses minibatches of data to form an expectation of the gradient, rather than the full gradient using all available data. That is for weights $w$ and a loss function $L$ we have:\n$$ w_{t+1} = w_{t} - \\eta\\hat{\\nabla}_{w}{L(w_{t})} $$\nWhere $\\eta$ is a learning rate. SGD reduces redundancy compared to batch gradient descent - which recomputes gradients for similar examples before each parameter update - so it is usually much faster.\n(Image Source: here)",
        "paper": null,
        "category": [
            "Stochastic Optimization",
            "Optimization"
        ],
        "type": "Method",
        "probable_wikipedia": "Stochastic gradient descent",
        "score": "12.603767",
        "probable_description": " Stochastic gradient descent (often abbreviated SGD) is an iterative method for optimizing an objective function with suitable smoothness properties (e.g. differentiable or subdifferentiable). It is called stochastic because the method uses randomly selected (or shuffled) samples to evaluate the gradients, hence SGD can be regarded as a stochastic approximation of gradient descent optimization. The ideas can be traced back at least to the 1951 article titled \"A Stochastic Approximation Method\" by Herbert Robbins and Sutton Monro, who proposed with detailed analysis a root-finding method now called the Robbins\u2013Monro algorithm. "
    },
    {
        "id": "METHOD_sniper",
        "name": "SNIPER",
        "full_name": "SNIPER",
        "description": "SNIPER is a multi-scale training approach for instance-level recognition tasks like object detection and instance-level segmentation. Instead of processing all pixels in an image pyramid, SNIPER selectively processes context regions around the ground-truth objects (a.k.a chips). This can help to speed up multi-scale training as it operates on low-resolution chips. Due to its memory-efficient design, SNIPER can benefit from Batch Normalization during training and it makes larger batch-sizes possible for instance-level recognition tasks on a single GPU.",
        "category": [
            "Multi-Scale Training"
        ],
        "proposed_in": {
            "paper_id": "sniper-efficient-multi-scale-training",
            "s2_paper_id": "38d7920f0e8a3a672ea37c8612b2b2947b9ba9d1"
        },
        "type": "Method",
        "probable_wikipedia": "Sniper",
        "score": "11.47047",
        "probable_description": " A sniper is a military/paramilitary marksman who operates to maintain effective visual contact with and engage enemy targets from concealed positions or at distances exceeding the target's detection capabilities. Snipers generally have specialized training and are equipped with high-precision rifles and high-magnification optics, and often feed information back to their units or command headquarters.  In addition to marksmanship and long range shooting, military snipers are trained in a variety of tactical techniques: detection, stalking, and target range estimation methods, camouflage, field craft, infiltration, special reconnaissance and observation, surveillance and target acquisition. "
    },
    {
        "id": "METHOD_softmax",
        "name": "Softmax",
        "full_name": "Softmax",
        "description": "The Softmax output function transforms a previous layer's output into a vector of probabilities. It is commonly used for multiclass classification.  Given an input vector $x$ and a weighting vector $w$ we have:\n$$ P(y=j \\mid{x}) = \\frac{e^{x^{T}w_{j}}}{\\sum^{K}_{k=1}e^{x^{T}wk}} $$",
        "paper": null,
        "category": [
            "Output Functions"
        ],
        "type": "Method",
        "probable_wikipedia": "Softmax function",
        "score": "2.5123932",
        "probable_description": " In mathematics, the softmax function, also known as softargmax or normalized exponential function, is a function that takes as input a vector of \"K\" real numbers, and normalizes it into a probability distribution consisting of \"K\" probabilities proportional to the exponentials of the input numbers. That is, prior to applying softmax, some vector components could be negative, or greater than one; and might not sum to 1; but after applying softmax, each component will be in the interval formula_1, and the components will add up to 1, so that they can be interpreted as probabilities. Furthermore, the larger input components will correspond to larger probabilities. Softmax is often used in neural networks, to map the non-normalized output of a network to a probability distribution over predicted output classes.  The standard (unit) softmax function formula_2is defined by the formula  In words: we apply the standard exponential function to each element formula_4 of the input vector formula_5 and normalize these values by dividing by the sum of all these exponentials; this normalization ensures that the sum of the components of the output vector formula_6 is 1.  Instead of , a different base \u00a0>\u00a00 can be used; choosing a larger value of will create a probability distribution that is more concentrated around the positions of the largest input values. Writing formula_7 or formula_8 (for real ) yields the expressions:  In some fields, the base is fixed, corresponding to a fixed scale, while in others the parameter is varied. "
    },
    {
        "id": "METHOD_som",
        "name": "SOM",
        "full_name": "Self-Organizing Map",
        "description": "The Self-Organizing Map (SOM), commonly also known as Kohonen network (Kohonen 1982, Kohonen 2001) is a computational method for the visualization and analysis of high-dimensional data, especially experimentally acquired information.\nExtracted from scholarpedia\nSources:\nImage: scholarpedia\nPaper: Kohonen, T. Self-organized formation of topologically correct feature maps. Biol. Cybern. 43, 59\u201369 (1982)\nBook: Self-Organizing Maps",
        "paper": null,
        "category": [
            "Clustering"
        ],
        "type": "Method",
        "probable_wikipedia": "Self-organizing map",
        "score": "12.110935",
        "probable_description": " A self-organizing map (SOM) or self-organizing feature map (SOFM) is a type of artificial neural network (ANN) that is trained using unsupervised learning to produce a low-dimensional (typically two-dimensional), discretized representation of the input space of the training samples, called a map, and is therefore a method to do dimensionality reduction. Self-organizing maps differ from other artificial neural networks as they apply competitive learning as opposed to error-correction learning (such as backpropagation with gradient descent), and in the sense that they use a neighborhood function to preserve the topological properties of the input space. This makes SOMs useful for visualization by creating low-dimensional views of high-dimensional data, akin to multidimensional scaling. The artificial neural network introduced by the Finnish professor Teuvo Kohonen in the 1980s is sometimes called a Kohonen map or network. The Kohonen net is a computationally convenient abstraction building on biological models of neural systems from the 1970s and morphogenesis models dating back to Alan Turing in the 1950s.  While it is typical to consider this type of network structure as related to feedforward networks where the nodes are visualized as being attached, this type of architecture is fundamentally different in arrangement and motivation.  Useful extensions include using toroidal grids where opposite edges are connected and using large numbers of nodes.  It has been shown that while self-organizing maps with a small number of nodes behave in a way that is similar to K-means, larger self-organizing maps rearrange data in a way that is fundamentally topological in character. "
    },
    {
        "id": "METHOD_spectral-clustering",
        "name": "Spectral Clustering",
        "full_name": "Spectral Clustering",
        "description": "Spectral clustering has attracted increasing attention due to\nthe promising ability in dealing with nonlinearly separable datasets [15], [16]. In spectral clustering, the spectrum of the graph Laplacian is used to reveal the cluster structure. The spectral clustering algorithm mainly consists of two steps: 1) constructs the low dimensional embedded representation of the data based on the eigenvectors of the graph Laplacian, 2) applies k-means on the constructed low dimensional data to obtain the clustering result. Thus,",
        "category": [
            "Clustering"
        ],
        "proposed_in": {
            "paper_id": "a-tutorial-on-spectral-clustering",
            "s2_paper_id": "eda90bd43f4256986688e525b45b833a3addab97"
        },
        "type": "Method",
        "probable_wikipedia": "Spectral clustering",
        "score": "11.233858",
        "probable_description": " In multivariate statistics and the clustering of data, spectral clustering techniques make use of the spectrum (eigenvalues) of the similarity matrix of the data to perform dimensionality reduction before clustering in fewer dimensions. The similarity matrix is provided as an input and consists of a quantitative assessment of the relative similarity of each pair of points in the dataset.  In application to image segmentation, spectral clustering is known as segmentation-based object categorization. "
    },
    {
        "id": "METHOD_squeezenet",
        "name": "SqueezeNet",
        "full_name": "SqueezeNet",
        "description": "SqueezeNet is a convolutional neural network that employs design strategies to reduce the number of parameters, notably with the use of fire modules that \"squeeze\" parameters using 1x1 convolutions.",
        "category": [
            "Convolutional Neural Networks",
            "Image Models"
        ],
        "proposed_in": {
            "paper_id": "squeezenet-alexnet-level-accuracy-with-50x",
            "s2_paper_id": "969fbdcd0717bec06228053788c2ff78bbb4daac"
        },
        "type": "Method",
        "probable_wikipedia": "SqueezeNet",
        "score": "11.466662",
        "probable_description": " SqueezeNet is the name of a deep neural network for computer vision that was released in 2016. SqueezeNet was developed by researchers at DeepScale, University of California, Berkeley, and Stanford University. In designing SqueezeNet, the authors' goal was to create a smaller neural network with fewer parameters that can more easily fit into computer memory and can more easily be transmitted over a computer network. "
    },
    {
        "id": "METHOD_stylegan",
        "name": "StyleGAN",
        "full_name": "StyleGAN",
        "description": "StyleGAN is a type of generative adversarial network. It uses an alternative generator architecture for generative adversarial networks, borrowing from style transfer literature; in particular, the use of adaptive instance normalization. Otherwise it follows Progressive GAN in using a progressively growing training regime. Other quirks include the fact it generates from a fixed value tensor not stochastically generated latent variables as in regular GANs. The stochastically generated latent variables are used as style vectors in the adaptive instance normalization at each resolution after being transformed by an 8-layer feedforward network. Lastly, it employs a form of regularization called mixing regularization, which mixes two style latent variables during training.",
        "category": [
            "Generative Adversarial Networks",
            "Generative Models"
        ],
        "proposed_in": {
            "paper_id": "a-style-based-generator-architecture-for",
            "s2_paper_id": "ceb2ebef0b41e31c1a21b28c2734123900c005e2"
        },
        "type": "Method",
        "probable_wikipedia": "StyleGAN",
        "score": "10.319779",
        "probable_description": " This Person Does Not Exist (ThisPersonDoesNotExist.com) is a website showcasing fully automated human image synthesis by endlessly generating images that look like facial portraits of human faces using StyleGAN, a novel generative adversarial network (GAN) created by Nvidia researchers. The website was published in February 2019 by Phillip Wang. The technology has drawn comparison with Deep fakes and the tells of poker, and its potential usage for sinister purposes has been bruited.   "
    },
    {
        "id": "METHOD_svm",
        "name": "SVM",
        "full_name": "Support Vector Machine",
        "description": "A Support Vector Machine, or SVM, is a non-parametric supervised learning model. For non-linear classification and regression, they utilise the kernel trick to map inputs to high-dimensional feature spaces. SVMs construct a hyper-plane or set of hyper-planes in a high or infinite dimensional space, which can be used for classification, regression or other tasks. Intuitively, a good separation is achieved by the hyper-plane that has the largest distance to the nearest training data points of any class (so-called functional margin), since in general the larger the margin the lower the generalization error of the classifier. The figure to the right shows the decision function for a linearly separable problem, with three samples on the margin boundaries, called \u201csupport vectors\u201d. \nSource: scikit-learn",
        "paper": null,
        "category": [
            "Non-Parametric Regression"
        ],
        "type": "Method",
        "probable_wikipedia": "Support-vector machine",
        "score": "8.4864235",
        "probable_description": " In machine learning, support-vector machines (SVMs, also support-vector networks) are supervised learning models with associated learning algorithms that analyze data used for classification and regression analysis. Given a set of training examples, each marked as belonging to one or the other of two categories, an SVM training algorithm builds a model that assigns new examples to one category or the other, making it a non-probabilistic binary linear classifier (although methods such as Platt scaling exist to use SVM in a probabilistic classification setting). An SVM model is a representation of the examples as points in space, mapped so that the examples of the separate categories are divided by a clear gap that is as wide as possible. New examples are then mapped into that same space and predicted to belong to a category based on the side of the gap on which they fall.  In addition to performing linear classification, SVMs can efficiently perform a non-linear classification using what is called the , implicitly mapping their inputs into high-dimensional feature spaces.  When data is unlabelled, supervised learning is not possible, and an unsupervised learning approach is required, which attempts to find natural clustering of the data to groups, and then map new data to these formed groups. The support-vector clustering algorithm, created by Hava Siegelmann and Vladimir Vapnik, applies the statistics of support vectors, developed in the support vector machines algorithm, to categorize unlabeled data, and is one of the most widely used clustering algorithms in industrial applications. "
    },
    {
        "id": "METHOD_synthesizer",
        "name": "Synthesizer",
        "full_name": "Synthesizer",
        "description": "The  Synthesizer is a model that learns synthetic attention weights without token-token interactions. Unlike Transformers, the model eschews dot product self-attention but also content-based self-attention altogether. Synthesizer learns to synthesize the self-alignment matrix instead of manually computing pairwise dot products. It is transformation-based, only relies on simple feed-forward layers, and completely dispenses with dot products and explicit token-token interactions. \nThis new module employed by the Synthesizer is called \"Synthetic Attention\": a new way of learning to attend without explicitly attending (i.e., without dot product attention or content-based attention). Instead, Synthesizer generate the alignment matrix independent of token-token dependencies.",
        "category": [
            "Language Models"
        ],
        "proposed_in": {
            "paper_id": "synthesizer-rethinking-self-attention-in",
            "s2_paper_id": "7c5c149699a0ba54b52cd5b9e291077f4a1f9d13"
        },
        "type": "Method",
        "probable_wikipedia": "Synthesizer",
        "score": "5.7685156",
        "probable_description": " A synthesizer or synthesiser (often abbreviated to synth) is an electronic musical instrument that generates audio signals that may be converted to sound. Synthesizers may imitate traditional musical instruments such as piano, flute, vocals, or natural sounds such as ocean waves; or generate novel electronic timbres. They are often played with a musical keyboard, but they can be controlled via a variety of other devices, including music sequencers, instrument controllers, fingerboards, guitar synthesizers, wind controllers, and electronic drums. Synthesizers without built-in controllers are often called \"sound modules\", and are controlled via USB, MIDI or CV/gate using a controller device, often a MIDI keyboard or other controller.  Synthesizers use various methods to generate electronic signals (sounds). Among the most popular waveform synthesis techniques are subtractive synthesis, additive synthesis, wavetable synthesis, frequency modulation synthesis, phase distortion synthesis, physical modeling synthesis and sample-based synthesis.  Synthesizers were first used in pop music in the 1960s. In the late 1970s, synths were used in progressive rock, pop and disco. In the 1980s, the invention of the relatively inexpensive Yamaha DX7 synth made digital synthesizers widely available. 1980s pop and dance music often made heavy use of synthesizers. Synthesizers are used in genres such as pop, hip hop, metal, rock, dance, and contemporary classical music. "
    },
    {
        "id": "METHOD_td-gammon",
        "name": "TD-Gammon",
        "full_name": "TD-Gammon",
        "description": "TD-Gammon is a game-learning architecture for playing backgammon. It involves the use of a $TD\\left(\\lambda\\right)$ learning algorithm and a feedforward neural network.\nCredit: Temporal Difference Learning and\nTD-Gammon",
        "paper": null,
        "category": [
            "Board Game Models"
        ],
        "type": "Method",
        "probable_wikipedia": "TD-Gammon",
        "score": "12.606966",
        "probable_description": " TD-Gammon is a computer backgammon program developed in 1992 by Gerald Tesauro at IBM's Thomas J. Watson Research Center. Its name comes from the fact that it is an artificial neural net trained by a form of temporal-difference learning, specifically TD-lambda.  TD-Gammon achieved a level of play just slightly below that of the top human backgammon players of the time. It explored strategies that humans had not pursued and led to advances in the theory of correct backgammon play. "
    },
    {
        "id": "METHOD_tnt",
        "name": "TNT",
        "full_name": "Transformer in Transformer",
        "description": "Transformer is a type of self-attention-based neural networks originally applied for NLP tasks. Recently, pure transformer-based models are proposed to solve computer vision problems. These visual transformers usually view an image as a sequence of patches while they ignore the intrinsic structure information inside each patch. In this paper, we propose a novel Transformer-iN-Transformer (TNT) model for modeling both patch-level and pixel-level representation. In each TNT block, an outer transformer block is utilized to process patch embeddings, and an inner transformer block extracts local features from pixel embeddings. The pixel-level feature is projected to the space of patch embedding by a linear transformation layer and then added into the patch. By stacking the TNT blocks, we build the TNT model for image recognition.\nImage source: Han et al.",
        "category": [
            "Vision Transformers",
            "Image Models"
        ],
        "proposed_in": {
            "paper_id": "transformer-in-transformer",
            "s2_paper_id": "0ae67202f0584afccefa770865d14a46655d2975"
        },
        "type": "Method",
        "probable_wikipedia": "Transformer",
        "score": "0.649595",
        "probable_description": " A transformer is a passive electrical device that transfers electrical energy between two or more circuits. A varying current in one coil of the transformer produces a varying magnetic flux, which, in turn, induces a varying electromotive force across a second coil wound around the same core. Electrical energy can be transferred between the two coils, without a metallic connection between the two circuits. Faraday's law of induction discovered in 1831 described the induced voltage effect in any coil due to changing magnetic flux encircled by the coil.  Transformers are used for increasing or decreasing the alternating voltages in electric power applications, and for coupling the stages of signal processing circuits.  Since the invention of the first constant-potential transformer in 1885, transformers have become essential for the transmission, distribution, and utilization of alternating current electric power. A wide range of transformer designs is encountered in electronic and electric power applications. Transformers range in size from RF transformers less than a cubic centimeter in volume, to units weighing hundreds of tons used to interconnect the power grid."
    },
    {
        "id": "METHOD_trades",
        "name": "TraDeS",
        "full_name": "TraDeS",
        "description": "TradeS is an online joint detection and tracking model, coined as TRACK to DEtect and Segment, exploiting tracking clues to assist detection end-to-end. TraDeS infers object tracking offset by a cost volume, which is used to propagate previous object features for improving current object detection and segmentation.",
        "category": [
            "Multi-Object Tracking Models"
        ],
        "proposed_in": {
            "paper_id": "track-to-detect-and-segment-an-online-multi",
            "s2_paper_id": "827455225f598f5444b8025073cefeb9bb3d4a85"
        },
        "type": "Method",
        "probable_wikipedia": "Trade",
        "score": "7.4911942",
        "probable_description": " Trade involves the transfer of goods or services from one person or entity to another, often in exchange for money. A system or network that allows trade is called a market.  An early form of trade, barter, saw the direct exchange of goods and services for other goods and services. Barter involves trading things without the use of money. Later, one bartering party started to involve precious metals, which gained symbolic as well as practical importance. Modern traders generally negotiate through a medium of exchange, such as money. As a result, buying can be separated from selling, or earning. The invention of money (and later credit, paper money and non-physical money) greatly simplified and promoted trade. Trade between two traders is called bilateral trade, while trade involving more than two traders is called multilateral trade.  Trade exists due to specialization and the division of labor, a predominant form of economic activity in which individuals and groups concentrate on a small aspect of production, but use their output in trades for other products and needs. Trade exists between regions because different regions may have a comparative advantage (perceived or real) in the production of some trade-able commodity\u2014including production of natural resources scarce or limited elsewhere, or because different regions' sizes may encourage mass production. In such circumstances, trade at market prices between locations can benefit both locations.  Retail trade consists of the sale of goods or merchandise from a very fixed location (such as a department store, boutique or kiosk), online or by"
    },
    {
        "id": "METHOD_transformer",
        "name": "Transformer",
        "full_name": "Transformer",
        "description": "A Transformer is a model architecture that eschews recurrence and instead relies entirely on an attention mechanism to draw global dependencies between input and output. Before Transformers, the dominant sequence transduction models were based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The Transformer also employs an encoder and decoder, but removing recurrence in favor of attention mechanisms allows for significantly more parallelization than methods like RNNs and CNNs.",
        "category": [
            "Autoregressive Transformers",
            "Transformers",
            "Language Models"
        ],
        "proposed_in": {
            "paper_id": "attention-is-all-you-need",
            "s2_paper_id": "204e3073870fae3d05bcbc2f6a8e263d9b72e776"
        },
        "type": "Method",
        "probable_wikipedia": "Transformer",
        "score": "9.977637",
        "probable_description": " A transformer is a passive electrical device that transfers electrical energy between two or more circuits. A varying current in one coil of the transformer produces a varying magnetic flux, which, in turn, induces a varying electromotive force across a second coil wound around the same core. Electrical energy can be transferred between the two coils, without a metallic connection between the two circuits. Faraday's law of induction discovered in 1831 described the induced voltage effect in any coil due to changing magnetic flux encircled by the coil.  Transformers are used for increasing or decreasing the alternating voltages in electric power applications, and for coupling the stages of signal processing circuits.  Since the invention of the first constant-potential transformer in 1885, transformers have become essential for the transmission, distribution, and utilization of alternating current electric power. A wide range of transformer designs is encountered in electronic and electric power applications. Transformers range in size from RF transformers less than a cubic centimeter in volume, to units weighing hundreds of tons used to interconnect the power grid."
    },
    {
        "id": "METHOD_triplet-loss",
        "name": "Triplet Loss",
        "full_name": "Triplet Loss",
        "description": "The goal of Triplet loss, in the context of Siamese Networks, is to maximize the joint probability among all score-pairs i.e. the product of all probabilities. By using its negative logarithm, we can get the loss formulation as follows:\n$$\nL_{t}\\left(\\mathcal{V}_{p}, \\mathcal{V}_{n}\\right)=-\\frac{1}{M N} \\sum_{i}^{M} \\sum_{j}^{N} \\log \\operatorname{prob}\\left(v p_{i}, v n_{j}\\right)\n$$\nwhere the balance weight $1/MN$ is used to keep the loss with the same scale for different number of instance sets.",
        "category": [
            "Loss Functions"
        ],
        "proposed_in": {
            "paper_id": "triplet-loss-in-siamese-network-for-object",
            "s2_paper_id": "fdb98f5a7015de0956ef8d4e468257dc3079b5e5"
        },
        "type": "Method",
        "probable_wikipedia": "Triplet loss",
        "score": "11.3090925",
        "probable_description": " Triplet loss is a loss function for artificial neural networks where a baseline (anchor) input is compared to a positive (truthy) input and a negative (falsy) input. The distance from the baseline (anchor) input to the positive (truthy) input is minimized, and the distance from the baseline (anchor) input to the negative (falsy) input is maximized.  It is often used for learning similarity of for the purpose of learning embeddings, like word embeddings and even thought vectors, and metric learning.  The loss function can be described using a Euclidean distance function  This can then be used in a cost function, that is the sum of all losses, which can then be used for minimization of the posed optimization problem  "
    },
    {
        "id": "METHOD_tucker",
        "name": "TuckER",
        "full_name": "TuckER",
        "description": "TuckER",
        "category": [
            "Graph Embeddings"
        ],
        "proposed_in": {
            "paper_id": "tucker-tensor-factorization-for-knowledge",
            "s2_paper_id": "05dc5fb3a3bdefdf181aafcc42cd80ff6b7704e7"
        },
        "type": "Method",
        "probable_wikipedia": "Tucker, Georgia",
        "score": "6.0917244",
        "probable_description": " Tucker is a city located in DeKalb County, Georgia. There are some unincorporated areas in Gwinnett County that have a Tucker postal address but are not part of the city. It is located near Atlanta and was originally settled in the 1820s, and later developed as a railroad community in 1892. According to the 2016 United States Census Bureau annual estimate of resident population, it has a population of 35,322. In a November 2015 referendum, by a 3:1 margin (73.94%), voters approved incorporating Tucker into a city. In March 2016, Tucker residents elected the city\u2019s first mayor and city council. "
    },
    {
        "id": "METHOD_u-net",
        "name": "U-Net",
        "full_name": "U-Net",
        "description": "U-Net is an architecture for semantic segmentation. It consists of a contracting path and an expansive path. The contracting path follows the typical architecture of a convolutional network. It consists of the repeated application of two 3x3 convolutions (unpadded convolutions), each followed by a rectified linear unit (ReLU) and a 2x2 max pooling operation with stride 2 for downsampling. At each downsampling step we double the number of feature channels. Every step in the expansive path consists of an upsampling of the feature map followed by a 2x2 convolution (\u201cup-convolution\u201d) that halves the number of feature channels, a concatenation with the correspondingly cropped feature map from the contracting path, and two 3x3 convolutions, each followed by a ReLU. The cropping is necessary due to the loss of border pixels in every convolution. At the final layer a 1x1 convolution is used to map each 64-component feature vector to the desired number of classes. In total the network has 23 convolutional layers.\nOriginal MATLAB Code",
        "category": [
            "Semantic Segmentation Models"
        ],
        "proposed_in": {
            "paper_id": "u-net-convolutional-networks-for-biomedical",
            "s2_paper_id": "6364fdaa0a0eccd823a779fcdd489173f938e91a"
        },
        "type": "Method",
        "probable_wikipedia": "U-Net",
        "score": "10.959713",
        "probable_description": " U-Net is a convolutional neural network that was developed for biomedical image segmentation at the Computer Science Department of the University of Freiburg, Germany. The network is based on the fully convolutional network and its architecture was modified and extended to work with fewer training images and to yield more precise segmentations. Segmentation of a 512*512 image takes less than a second on a recent GPU. "
    },
    {
        "id": "METHOD_vos",
        "name": "VOS",
        "full_name": "VOS",
        "description": "VOS is a type of video object segmentation model consisting of two network components. The target appearance model consists of a light-weight module, which is learned during the inference stage using fast optimization techniques to predict a coarse but robust target segmentation. The segmentation model is exclusively trained offline, designed to process the coarse scores into high quality segmentation masks.",
        "category": [
            "Video Object Segmentation Models"
        ],
        "proposed_in": {
            "paper_id": "learning-fast-and-robust-target-models-for",
            "s2_paper_id": "b37b2f3129d55a8619fe0a319b88e083c79bb285"
        },
        "type": "Method",
        "probable_wikipedia": "Virtual Object System",
        "score": "4.611147",
        "probable_description": " The Virtual Object System (VOS) is a computer software technology for creating distributed object systems. The sites hosting Vobjects are typically linked by a computer network, such as a local area network or the Internet. Vobjects may send messages to other Vobjects over these network links (remotely) or within the same host site (locally) to perform actions and synchronize state. In this way, VOS may also be called an object-oriented remote procedure call system. In addition, Vobjects may have a number of directed relations to other Vobjects, which allows them to form directed graph data structures.  VOS is patent free, and its implementation is Free Software. The primary application focus of VOS is general purpose, multiuser, collaborative 3D virtual environments or virtual reality. The primary designer and author of VOS is Peter Amstutz.   "
    },
    {
        "id": "METHOD_wavenet",
        "name": "WaveNet",
        "full_name": "WaveNet",
        "description": "WaveNet is an audio generative model based on the PixelCNN architecture. In order to deal with long-range temporal dependencies needed for raw audio generation, architectures are developed based on dilated causal convolutions, which exhibit very large receptive fields.\nThe joint probability of a waveform $\\vec{x} = { x_1, \\dots, x_T }$ is factorised as a product of conditional probabilities as follows:\n$$p\\left(\\vec{x}\\right) = \\prod_{t=1}^{T} p\\left(x_t \\mid x_1, \\dots ,x_{t-1}\\right)$$\nEach audio sample $x_t$ is therefore conditioned on the samples at all previous timesteps.",
        "category": [
            "Generative Audio Models"
        ],
        "proposed_in": {
            "paper_id": "wavenet-a-generative-model-for-raw-audio",
            "s2_paper_id": "df0402517a7338ae28bc54acaac400de6b456a46"
        },
        "type": "Method",
        "probable_wikipedia": "WaveNet",
        "score": "11.602994",
        "probable_description": " WaveNet is a deep neural network for generating raw audio. It was created by researchers at London-based artificial intelligence firm DeepMind. The technique, outlined in a paper in September 2016, is able to generate relatively realistic-sounding human-like voices by directly modelling waveforms using a neural network method trained with recordings of real speech. Tests with US English and Mandarin reportedly showed that the system outperforms Google's best existing text-to-speech (TTS) systems, although as of 2016 its text-to-speech synthesis still was less convincing than actual human speech. WaveNet's ability to generate raw waveforms means that it can model any kind of audio, including music.  Its ability to clone voices has raised ethical concerns about WaveNet's ability to mimic the voices of living and dead persons. According to a 2016 BBC article, companies working on similar voice-cloning technologies (such as Adobe Voco) intend to insert watermarking inaudible to humans to prevent counterfeiting, while maintaining that voice cloning satisfying, for instance, the needs of entertainment-industry purposes would be of a far lower complexity and use different methods than required to fool forensic evidencing methods and electronic ID devices, so that natural voices and voices cloned for entertainment-industry purposes could still be easily told apart by technological analysis. "
    },
    {
        "id": "METHOD_weight-decay",
        "name": "Weight Decay",
        "full_name": "Weight Decay",
        "description": "Weight Decay, or $L_{2}$ Regularization, is a regularization technique applied to the weights of a neural network. We minimize a loss function compromising both the primary loss function and a penalty on the $L_{2}$ Norm of the weights:\n$$L_{new}\\left(w\\right) = L_{original}\\left(w\\right) + \\lambda{w^{T}w}$$\nwhere $\\lambda$ is a value determining the strength of the penalty (encouraging smaller weights). \nWeight decay can be incorporated directly into the weight update rule, rather than just implicitly by defining it through to objective function. Often weight decay refers to the implementation where we specify it directly in the weight update rule (whereas L2 regularization is usually the implementation which is specified in the objective function).\nImage Source: Deep Learning, Goodfellow et al",
        "paper": null,
        "category": [
            "Parameter Norm Penalties",
            "Regularization"
        ],
        "type": "Method",
        "probable_wikipedia": "Tikhonov regularization",
        "score": "0.4231091",
        "probable_description": " Tikhonov regularization, named for Andrey Tikhonov, is the most commonly used method of regularization of ill-posed problems. In statistics, the method is known as ridge regression, in machine learning it is known as weight decay, and with multiple independent discoveries, it is also variously known as the Tikhonov\u2013Miller method, the Phillips\u2013Twomey method, the constrained linear inversion method, and the method of linear regularization. It is related to the Levenberg\u2013Marquardt algorithm for non-linear least-squares problems.  Suppose that for a known matrix formula_1 and vector formula_2, we wish to find a vector formula_3 such that The standard approach is ordinary least squares linear regression. However, if no formula_3 satisfies the equation or more than one formula_3 does\u2014that is, the solution is not unique\u2014the problem is said to be ill posed. In such cases, ordinary least squares estimation leads to an overdetermined, or more often an underdetermined system of equations. Most real-world phenomena have the effect of low-pass filters in the forward direction where formula_1 maps formula_3 to formula_2. Therefore, in solving the inverse-problem, the inverse mapping operates as a high-pass filter that has the undesirable tendency of amplifying noise (eigenvalues / singular values are largest in the reverse mapping where they were smallest in the forward mapping). In addition, ordinary least squares implicitly nullifies every element of the reconstructed version of formula_3 that is in the null-space of formula_1, rather than allowing for a model to be used as a prior for formula_3. Ordinary least squares seeks to minimize the sum of squared residuals, which can"
    },
    {
        "id": "METRIC_accuracy",
        "name": "Accuracy",
        "full_name": "Accuracy",
        "also_known_as": [
            "Acc",
            "Hamming Score"
        ],
        "description": "Accuracy is how close or far off a given set of measurements (observations or readings) or are to their true value",
        "wikipedia": "Accuracy_and_precision",
        "type": "Metric",
        "probable_wikipedia": "Accuracy and precision",
        "score": "4.100286",
        "probable_description": " Precision is a description of \"random errors\", a measure of statistical variability.  Accuracy has two definitions:  In simplest terms, given a set of data points from repeated measurements of the same quantity, the set can be said to be \"precise\" if the values are close to each other, while the set can be said to be \"accurate\" if their average is close to the \"true value\" of the quantity being measured. In the first, more common definition above, the two concepts are independent of each other, so a particular set of data can be said to be either accurate, or precise, or both, or neither. "
    },
    {
        "id": "METRIC_Wasserstein_metric",
        "name": "Wasserstein metric",
        "also_known_as": [
            "Wasserstein distance",
            "Kantorovich Rubinstein metric"
        ],
        "description": "In mathematics, the Wasserstein distance or Kantorovich Rubinstein metric is a distance function defined between probability distributions on a given metric space",
        "type": "Metric",
        "probable_wikipedia": "Wasserstein metric",
        "score": "11.700071",
        "probable_description": " In mathematics, the Wasserstein or Kantorovich\u2013Rubinstein metric or distance is a distance function defined between probability distributions on a given metric space formula_1.  Intuitively, if each distribution is viewed as a unit amount of \"dirt\" piled on \"formula_1\", the metric is the minimum \"cost\" of turning one pile into the other, which is assumed to be the amount of dirt that needs to be moved times the mean distance it has to be moved. Because of this analogy, the metric is known in computer science as the earth mover's distance.  The name \"Wasserstein distance\" was coined by R. L. Dobrushin in 1970, after the Russian mathematician Leonid Vaserstein who introduced the concept in 1969. Most English-language publications use the German spelling \"Wasserstein\" (attributed to the name \"Vaserstein\" being of German origin). "
    },
    {
        "id": "METRIC_Effectiveness",
        "name": "Effectiveness",
        "also_known_as": [
            "efficiency",
            "efficacy",
            "effectivity"
        ],
        "description": "Effectiveness is the capability of producing a desired result or the ability to produce desired output. When something is deemed effective, it means it has an intended or expected outcome, or produces a deep, vivid impression.",
        "wikipedia": "Effectiveness",
        "type": "Metric",
        "probable_wikipedia": "Effectiveness",
        "score": "11.702668",
        "probable_description": " Effectiveness is the capability of producing a desired result or the ability to produce desired output. When something is deemed effective, it means it has an intended or expected outcome, or produces a deep, vivid impression. "
    },
    {
        "id": "METRIC_robustness",
        "name": "Robustness",
        "description": "Robustness is the property of being strong and healthy in constitution. When it is transposed into a system, it refers to the ability of tolerating perturbations that might affect the system\u2019s functional body.",
        "wikipedia": "Robustness",
        "type": "Metric",
        "probable_wikipedia": "Robustness",
        "score": "11.487033",
        "probable_description": " Robustness is the property of being strong and healthy in constitution. When it is transposed into a system, it refers to the ability of tolerating perturbations that might affect the system\u2019s functional body. In the same line \"robustness\" can be defined as \"the ability of a system to resist change without adapting its initial stable configuration\". \"Robustness in the small\" refers to situations wherein perturbations are small in magnitude, which considers that the \"small\" magnitude hypothesis can be difficult to verify because \"small\" or \"large\" depends on the specific problem. Conversely, \"Robustness in the large problem\" refers to situations wherein no assumptions can be made about the magnitude of perturbations, which can either be small or large. It has been discussed that robustness has two dimensions: resistance and avoidance. "
    },
    {
        "id": "METRIC_latency",
        "name": "Latency",
        "also_known_as": [
            "lag",
            "delay",
            "response time"
        ],
        "description": "Latency, from a general point of view, is a time delay between the cause and the effect of some physical change in the system being observed. Lag, as it is known in gaming circles, refers to the latency between the input to a simulation and the visual or auditory response, often occurring because of network delay in online games",
        "wikipedia": "Latency_(engineering)",
        "type": "Metric",
        "probable_wikipedia": "Latency (engineering)",
        "score": "6.93165",
        "probable_description": " Latency is a time interval between the stimulation and response, or, from a more general point of view, a time delay between the cause and the effect of some physical change in the system being observed. Latency is physically a consequence of the limited velocity with which any physical interaction can propagate. The magnitude of this velocity is always less than or equal to the speed of light. Therefore, every physical system with any physical separation (distance) between cause and effect will experience some sort of latency, regardless of the nature of stimulation that it has been exposed to.  The precise definition of latency depends on the system being observed and the nature of stimulation. In communications, the lower limit of latency is determined by the medium being used for communications. In reliable two-way communication systems, latency limits the maximum rate that information can be transmitted, as there is often a limit on the amount of information that is \"in-flight\" at any one moment. In the field of human\u2013machine interaction, perceptible latency has a strong effect on user satisfaction and usability. "
    },
    {
        "id": "METRIC_speed",
        "name": "Speed",
        "also_known_as": [
            "time"
        ],
        "description": "Speed is the rate of motion, change, or activity.",
        "wikipedia": "Speed",
        "type": "Metric",
        "probable_wikipedia": "Speed",
        "score": "8.930088",
        "probable_description": " In everyday use and in kinematics, the speed of an object is the magnitude of its velocity (the rate of change of its position); it is thus a scalar quantity. The average speed of an object in an interval of time is the distance travelled by the object divided by the duration of the interval; the instantaneous speed is the limit of the average speed as the duration of the time interval approaches zero.  Speed has the dimensions of distance divided by time. The SI unit of speed is the metre per second, but the most common unit of speed in everyday usage is the kilometre per hour or, in the US and the UK, miles per hour. For air and marine travel the knot is commonly used.  The fastest possible speed at which energy or information can travel, according to special relativity, is the speed of light in a vacuum \"c\" = metres per second (approximately or ). Matter cannot quite reach the speed of light, as this would require an infinite amount of energy. In relativity physics, the concept of rapidity replaces the classical idea of speed. "
    },
    {
        "id": "METRIC_runtime",
        "name": "runtime",
        "full_name": "runtime",
        "description": "In computer science, runtime, run time, or execution time is the final phase of a computer program's life cycle, in which the code is being executed on the computer's central processing unit (CPU) as machine code. In other words, \"runtime\" is the running phase of a program.",
        "related_terms": [
            "execution time",
            "run time"
        ],
        "type": "Metric",
        "probable_wikipedia": "Run time (program lifecycle phase)",
        "score": "11.430301",
        "probable_description": " In computer science, run time, runtime or execution time is the time during which a program is running (executing), in contrast to other program lifecycle phases such as compile time, link time and load time.  A run-time error is detected after or during the execution (running state) of a program, whereas a compile-time error is detected by the compiler before the program is ever executed. Type checking, register allocation, code generation, and code optimization are typically done at compile time, but may be done at run time depending on the particular language and compiler. Many other runtime errors exist and are handled differently by different languages, such as Division by zero errors, domain errors, array subscript out of bounds errors, Arithmetic underflow errors, several types of underflow and overflow errors, and many other runtime errors generally considered as software bugs which may or may not be caught and handled by any particular computer language. "
    },
    {
        "id": "METRIC_Responsiveness",
        "name": "Responsiveness",
        "wikipedia": "Responsiveness",
        "description": "Responsiveness as a concept of computer science refers to the specific ability of a system or functional unit to complete assigned tasks within a given time.",
        "type": "Metric",
        "probable_wikipedia": "Responsiveness",
        "score": "12.164816",
        "probable_description": " Responsiveness as a concept of computer science refers to the specific ability of a system or functional unit to complete assigned tasks within a given time. For example, it would refer to the ability of an artificial intelligence system to understand and carry out its tasks in a timely fashion. It is one of the criteria under the principle of robustness (from a v principle). The other three are observability, recoverability, and task conformance. "
    },
    {
        "id": "METRIC_observability",
        "name": "observability",
        "wikipedia": "Observability",
        "description": "Observability is a measure of how well internal states of a system can be inferred from knowledge of its external outputs.",
        "type": "Metric",
        "probable_wikipedia": "Observability",
        "score": "11.997427",
        "probable_description": " In control theory, observability is a measure of how well internal states of a system can be inferred from knowledge of its external outputs. The observability and controllability of a system are mathematical duals. The concept of observability was introduced by Hungarian-American engineer Rudolf E. Kalman for linear dynamic systems. "
    },
    {
        "id": "METRIC_Reliability",
        "name": "Reliability",
        "wikipedia": "Reliability_engineering",
        "description": "Reliability engineering is a sub-discipline of systems engineering that emphasizes the ability of equipment to function without failure. Reliability describes the ability of a system or component to function under stated conditions for a specified period of time.",
        "type": "Metric",
        "probable_wikipedia": "Reliability engineering",
        "score": "5.4536734",
        "probable_description": " Reliability engineering is a sub-discipline of systems engineering that emphasizes dependability in the lifecycle management of a product. Dependability, or reliability, describes the ability of a system or component to function under stated conditions for a specified period of time. Reliability is closely related to availability, which is typically described as the ability of a component or system to function at a specified moment or interval of time.  Reliability is theoretically defined as the probability of success formula_1 as the frequency of failures; or in terms of availability, as a probability derived from reliability, testability and maintainability. Testability, maintainability and maintenance are often defined as a part of \"reliability engineering\" in reliability programs. Reliability plays a key role in the cost-effectiveness of systems.  Reliability engineering deals with the estimation, prevention and management of high levels of \"lifetime\" engineering uncertainty and risks of failure. Although stochastic parameters define and affect reliability, reliability is not (solely) achieved by mathematics and statistics. One cannot really find a root cause (needed to effectively prevent failures) by only looking at statistics. \"Nearly all teaching and literature on the subject emphasize these aspects, and ignore the reality that the ranges of uncertainty involved largely invalidate quantitative methods for prediction and measurement.\" For example, it is easy to represent \"probability of failure\" as a symbol or value in an equation, but it is almost impossible to predict its true magnitude in practice, which is massively multivariate, so having the equation for reliability does not begin to equal having an accurate"
    },
    {
        "id": "METRIC_Jaccard_index",
        "name": "IoU",
        "full_name": "Intersection over union",
        "also_known_as": [
            "Jaccard index",
            "Jaccard distance",
            "Jaccard similarity coefficient"
        ],
        "description": "IoU is used to estimate how well a predicted mask or bounding box matches the ground truth data.",
        "wikipedia": "Jaccard_index",
        "type": "Metric",
        "probable_wikipedia": "Jaccard index",
        "score": "5.352369",
        "probable_description": " The Jaccard index, also known as Intersection over Union and the Jaccard similarity coefficient (originally given the French name \"coefficient de communaute\" by Paul Jaccard), is a statistic used for gauging the similarity and diversity of sample sets. The Jaccard coefficient measures similarity between finite sample sets, and is defined as the size of the intersection divided by the size of the union of the sample sets:  (If \"A\" and \"B\" are both empty, we define \"J\"(\"A\",\"B\")\u00a0=\u00a01.)  The Jaccard distance, which measures \"dis\"similarity between sample sets, is complementary to the Jaccard coefficient and is obtained by subtracting the Jaccard coefficient from 1, or, equivalently, by dividing the difference of the sizes of the union and the intersection of two sets by the size of the union:  An alternate interpretation of the Jaccard distance is as the ratio of the size of the symmetric difference formula_4 to the union.  This distance is a metric on the collection of all finite sets.  There is also a version of the Jaccard distance for measures, including probability measures. If formula_5 is a measure on a measurable space formula_6, then we define the Jaccard coefficient by formula_7, and the Jaccard distance by formula_8. Care must be taken if formula_9 or formula_10, since these formulas are not well defined in these cases.  The MinHash min-wise independent permutations locality sensitive hashing scheme may be used to efficiently compute an accurate estimate of the Jaccard similarity coefficient of pairs of sets, where each set is represented by"
    },
    {
        "id": "METRIC_recall",
        "name": "Recall",
        "full_name": "Recall",
        "also_known_as": [
            "sensivity",
            "hit rate",
            "true positive rate",
            "TPR",
            "Hits"
        ],
        "description": "Recall is the fraction of the relevant documents that are successfully retrieved",
        "wikipedia": "Precision_and_recall",
        "type": "Metric",
        "probable_wikipedia": "Recall election",
        "score": "0.09851801",
        "probable_description": " A recall election (also called a recall referendum or representative recall) is a procedure by which, in certain polities, voters can remove an elected official from office through a direct vote before that official's term has ended. Recalls, which are initiated when sufficient voters sign a petition, have a history dating back to ancient Athenian democracy and feature in several current constitutions. In indirect or representative democracy, people's representatives are elected and these representatives rule for a specific period of time. However, where the facility to recall exists, should any representative come to be perceived as not properly discharging their responsibilities, then they can be called back with the written request of specific number or proportion of voters. "
    },
    {
        "id": "METRIC_specificity",
        "name": "Specificity",
        "full_name": "Specificity",
        "also_known_as": [
            "selectivity",
            "true negative rate",
            "TNR"
        ],
        "description": "Specificity is a measure of how well a test can identify true negatives",
        "wikipedia": "Sensitivity_and_specificity",
        "type": "Metric",
        "probable_wikipedia": "Sensitivity and specificity",
        "score": "2.1375191",
        "probable_description": " Sensitivity and specificity are statistical measures of the performance of a binary classification test, also known in statistics as a classification function, that are widely used in medicine: \"Sensitivity' (also called the true positive rate, the recall, or probability of detection in some fields) measures the proportion of actual positives that are correctly identified as such (e.g., the percentage of sick people who are correctly identified as having the condition).  Note that the terms \"positive\" and \"negative\" don't refer to the value of the condition of interest, but to its presence or absence. The condition itself could be a disease, so that \"positive\" might mean \"diseased\", while \"negative\" might mean \"healthy\".  In medical tests, sensitivity is the extent to which actual positives are not overlooked (so false negatives are few), and specificity is the extent to which actual negatives are classified as such (so false positives are few). Thus, a highly sensitive test rarely overlooks an actual positive (for example, showing \"nothing bad\" despite something bad existing); a highly specific test rarely registers a positive classification for anything that is not the target of testing (for example, finding one bacterial species and mistaking it for another closely related one that is the true target); and a test that is highly sensitive \"and\" highly specific does both, so it \"rarely overlooks a thing that it is looking for\" \"and\" it \"rarely mistakes anything else for that thing.\" Because most medical tests do not have sensitivity and specificity values above 99%, \"rarely\" does \"not\" equate"
    },
    {
        "id": "METRIC_precision",
        "name": "Precision",
        "full_name": "Precision",
        "also_known_as": [
            "positive predictive value",
            "PPV"
        ],
        "description": "Precision is the fraction of relevant instances among the retrieved instances",
        "wikipedia": "Precision_and_recall",
        "type": "Metric",
        "probable_wikipedia": "Precision (statistics)",
        "score": "4.6281824",
        "probable_description": " In statistics, precision is the reciprocal of the variance, and the precision matrix (also known as concentration matrix) is the matrix inverse of the covariance matrix. Thus, if we are considering a single random variable in isolation, its precision is the inverse of its variance: \"p=1/\u03c3\u00b2\". Some particular statistical models define the term \"precision\" differently.  One particular use of the precision matrix is in the context of Bayesian analysis of the multivariate normal distribution: for example, Bernardo & Smith prefer to parameterise the multivariate normal distribution in terms of the precision matrix, rather than the covariance matrix, because of certain simplifications that then arise. For instance, if both the prior and the likelihood have Gaussian form, and the precision matrix of both of these exist (because their covariance matrix is full rank and thus invertible), then the precision matrix of the posterior will simply be the sum of the precision matrices of the prior and the likelihood.  As the inverse of a Hermitian matrix, the precision matrix of real-valued random variables, if it exists, is positive definite and symmetrical.  Another reason the precision matrix may be useful is that if two dimensions \"i\" and \"j\" of a multivariate normal are conditionally independent, then the \"ij\" and \"ji\" elements of the precision matrix are 0. This means that precision matrices tend to be sparse when many of the dimensions are conditionally independent, which can lead to computational efficiencies when working with them. It also means that precision matrices are closely related to the"
    },
    {
        "id": "METRIC_negative-predictive-rate",
        "name": "NPV",
        "full_name": "negative predictive value",
        "wikipedia": "Positive_and_negative_predictive_values",
        "type": "Metric",
        "probable_wikipedia": "Positive and negative predictive values",
        "score": "1.9835906",
        "probable_description": " The positive and negative predictive values (PPV and NPV respectively) are the proportions of positive and negative results in statistics and diagnostic tests that are true positive and true negative results, respectively. The PPV and NPV describe the performance of a diagnostic test or other statistical measure. A high result can be interpreted as indicating the accuracy of such a statistic. The PPV and NPV are not intrinsic to the test; they depend also on the prevalence. The PPV can be derived using Bayes' theorem.  Although sometimes used synonymously, a \"positive predictive value\" generally refers to what is established by control groups, while a post-test probability refers to a probability for an individual. Still, if the individual's pre-test probability of the target condition is the same as the prevalence in the control group used to establish the positive predictive value, the two are numerically equal.  In information retrieval, the PPV statistic is often called the precision. "
    },
    {
        "id": "METRIC_false-negative-rate",
        "name": "FNR",
        "full_name": "false negative rate",
        "also_known_as": [
            "miss rate",
            "error rate"
        ],
        "wikipedia": "False_positives_and_false_negatives",
        "type": "Metric",
        "probable_wikipedia": "False positives and false negatives",
        "score": "2.1216135",
        "probable_description": " In medical testing, and more generally in binary classification, a false positive is an error in data reporting in which a test result improperly indicates presence of a condition, such as a disease (the result is \"positive\"), when in reality it is not present, while a false negative is an error in which a test result improperly indicates no presence of a condition (the result is \"negative\"), when in reality it is present. These are the two kinds of errors in a binary test (and are contrasted with a correct result, either a or a .) They are also known in medicine as a false positive (respectively negative) diagnosis, and in statistical classification as a false positive (respectively negative) error. A false positive is distinct from overdiagnosis, and is also different from overtesting.  In statistical hypothesis testing the analogous concepts are known as type I and type II errors, where a positive result corresponds to rejecting the null hypothesis, and a negative result corresponds to not rejecting the null hypothesis. The terms are often used interchangeably, but there are differences in detail and interpretation due to the differences between medical testing and statistical hypothesis testing. "
    },
    {
        "id": "METRIC_False_positive_rate",
        "name": "FPR",
        "full_name": "false positive rate",
        "also_known_as": [
            "fall out",
            "false alarm ratio",
            "false alarm rate"
        ],
        "description": "false positive ratio is the probability of falsely rejecting the null hypothesis for a particular test. The false positive rate is calculated as the ratio between the number of negative events wrongly categorized as positive (false positives) and the total number of actual negative events (regardless of classification).",
        "wikipedia": "False_positives_and_false_negatives",
        "type": "Metric",
        "probable_wikipedia": "False positive rate",
        "score": "12.088445",
        "probable_description": " In statistics, when performing multiple comparisons, a false positive ratio (or false alarm ratio) is the probability of falsely rejecting the null hypothesis for a particular test. The false positive rate is calculated as the ratio between the number of negative events wrongly categorized as positive (false positives) and the total number of actual negative events (regardless of classification).  The false positive rate (or \"false alarm rate\") usually refers to the expectancy of the false positive ratio. "
    },
    {
        "id": "METRIC_False_discovery_rate",
        "name": "FDR",
        "full_name": "false discovery rate",
        "description": "false discovery rate (FDR) is a method of conceptualizing the rate of type I errors in null hypothesis testing when conducting multiple comparisons",
        "wikipedia": "False_discovery_rate",
        "type": "Metric",
        "probable_wikipedia": "False discovery rate",
        "score": "12.02701",
        "probable_description": " The false discovery rate (FDR) is a method of conceptualizing the rate of type I errors in null hypothesis testing when conducting multiple comparisons. FDR-controlling procedures are designed to control the expected proportion of \"discoveries\" (rejected null hypotheses) that are false (incorrect rejections). FDR-controlling procedures provide less stringent control of Type I errors compared to familywise error rate (FWER) controlling procedures (such as the Bonferroni correction), which control the probability of \"at least one\" Type I error. Thus, FDR-controlling procedures have greater power, at the cost of increased numbers of Type I errors. "
    },
    {
        "id": "METRIC_F-score",
        "name": "F1",
        "full_name": "F1 Score",
        "also_known_as": [
            "F score",
            "F Measure",
            "F1 Measure"
        ],
        "description": "The F1 score is the harmonic mean of the precision and recall.",
        "wikipedia": "F-score",
        "type": "Metric",
        "probable_wikipedia": "F1 score",
        "score": "12.265096",
        "probable_description": " In statistical analysis of binary classification, the F score (also F-score or F-measure) is a measure of a test's accuracy. It considers both the precision \"p\" and the recall \"r\" of the test to compute the score: \"p\" is the number of correct positive results divided by the number of all positive results returned by the classifier, and \"r\" is the number of correct positive results divided by the number of all relevant samples (all samples that should have been identified as positive). The F score is the harmonic average of the precision and recall, where an F score reaches its best value at 1 (perfect precision and recall) and worst at 0."
    },
    {
        "id": "METRIC_Mean_squared_error",
        "name": "MSE",
        "full_name": "mean squared error",
        "also_known_as": [
            "mean squared deviation",
            "MSD"
        ],
        "description": "In statistics, the mean squared error (MSE) or mean squared deviation (MSD) of an estimator (of a procedure for estimating an unobserved quantity) measures the average of the squares of the errors\u2014that is, the average squared difference between the estimated values and the actual value. MSE is a risk function, corresponding to the expected value of the squared error loss.",
        "wikipedia": "Mean_squared_error",
        "type": "Metric",
        "probable_wikipedia": "Mean squared error",
        "score": "12.168555",
        "probable_description": " In statistics, the mean squared error (MSE) or mean squared deviation (MSD) of an estimator (of a procedure for estimating an unobserved quantity) measures the average of the squares of the errors\u2014that is, the average squared difference between the estimated values and what is estimated. MSE is a risk function, corresponding to the expected value of the squared error loss. The fact that MSE is almost always strictly positive (and not zero) is because of randomness or because the estimator does not account for information that could produce a more accurate estimate.  The MSE is a measure of the quality of an estimator\u2014it is always non-negative, and values closer to zero are better.  The MSE is the second moment (about the origin) of the error, and thus incorporates both the variance of the estimator (how widely spread the estimates are from one data sample to another) and its bias (how far off the average estimated value is from the truth). For an unbiased estimator, the MSE is the variance of the estimator. Like the variance, MSE has the same units of measurement as the square of the quantity being estimated. In an analogy to standard deviation, taking the square root of MSE yields the root-mean-square error or root-mean-square deviation (RMSE or RMSD), which has the same units as the quantity being estimated; for an unbiased estimator, the RMSE is the square root of the variance, known as the standard error. "
    },
    {
        "id": "METRIC_Euclidean_distance",
        "name": "Euclidean distance",
        "full_name": "Euclidean distance",
        "description": "In mathematics, the Euclidean distance between two points in Euclidean space is the length of a line segment between the two points. It can be calculated from the Cartesian coordinates of the points using the Pythagorean theorem, therefore occasionally being called the Pythagorean distance.",
        "also_known_as": [
            "l2 distance",
            "l2 norm"
        ],
        "wikipedia": "Euclidean_distance",
        "type": "Metric",
        "probable_wikipedia": "Euclidean distance",
        "score": "11.608979",
        "probable_description": " In mathematics, the Euclidean distance or Euclidean metric is the \"ordinary\" straight-line distance between two points in Euclidean space. With this distance, Euclidean space becomes a metric space. The associated norm is called the Euclidean norm. Older literature refers to the metric as the Pythagorean metric. A generalized term for the Euclidean norm is the L norm or L distance. "
    },
    {
        "id": "METRIC_Mean_absolute_error",
        "name": "MAE",
        "also_known_as": [
            "L1"
        ],
        "full_name": "Mean absolute error",
        "description": "In statistics, mean absolute error (MAE) is a measure of errors between paired observations expressing the same phenomenon. Examples of Y versus X include comparisons of predicted versus observed, subsequent time versus initial time, and one technique of measurement versus an alternative technique of measurement.",
        "wikipedia": "Mean_absolute_error",
        "type": "Metric",
        "probable_wikipedia": "Mean absolute error",
        "score": "12.439873",
        "probable_description": " In statistics, mean absolute error (MAE) is a measure of difference between two continuous variables. Assume \"X\" and \"Y\" are variables of paired observations that express the same phenomenon. Examples of \"Y\" versus \"X\" include comparisons of predicted versus observed, subsequent time versus initial time, and one technique of measurement versus an alternative technique of measurement. Consider a scatter plot of \"n\" points, where point \"i\" has coordinates (\"x\", \"y\")... Mean Absolute Error (MAE) is the average vertical distance between each point and the identity line. MAE is also the average horizontal distance between each point and the identity line.  The Mean Absolute Error is given by:  formula_1  MAE has a clear interpretation as the average absolute difference between \"y\" and \"x\". Many researchers want to know this average difference because its interpretation is clear, but researchers frequently compute and misinterpret the Root Mean Squared Error (RMSE), which is not the average absolute error.  As the name suggests, the mean absolute error is an average of the absolute errors formula_2, where formula_3 is the prediction and formula_4 the true value. Note that alternative formulations may include relative frequencies as weight factors. The mean absolute error uses the same scale as the data being measured. This is known as a scale-dependent accuracy measure and therefore cannot be used to make comparisons between series using different scales. The mean absolute error is a common measure of forecast error in time series analysis, sometimes used in confusion with the more standard definition of mean"
    },
    {
        "id": "METRIC_Root-mean-square_deviation",
        "name": "RMSE",
        "full_name": "root mean square error",
        "also_known_as": [
            "root mean square deviation",
            "RMSD"
        ],
        "description": "The root-mean-square deviation (RMSD) or root-mean-square error (RMSE) is a frequently used measure of the differences between values (sample or population values) predicted by a model or an estimator and the values observed.",
        "wikipedia": "Root-mean-square_deviation",
        "type": "Metric",
        "probable_wikipedia": "Root-mean-square deviation",
        "score": "8.711577",
        "probable_description": " The root-mean-square deviation (RMSD) or root-mean-square error (RMSE) (or sometimes root-mean-square\"d\" error) is a frequently used measure of the differences between values (sample or population values) predicted by a model or an estimator and the values observed. The RMSD represents the square root of the second sample moment of the differences between predicted values and observed values or the quadratic mean of these differences. These deviations are called \"residuals\" when the calculations are performed over the data sample that was used for estimation and are called \"errors\" (or prediction errors) when computed out-of-sample. The RMSD serves to aggregate the magnitudes of the errors in predictions for various times into a single measure of predictive power. RMSD is a measure of accuracy, to compare forecasting errors of different models for a particular dataset and not between datasets, as it is scale-dependent.  RMSD is always non-negative, and a value of 0 (almost never achieved in practice) would indicate a perfect fit to the data. In general, a lower RMSD is better than a higher one. However, comparisons across different types of data would be invalid because the measure is dependent on the scale of the numbers used.  RMSD is the square root of the average of squared errors. The effect of each error on RMSD is proportional to the size of the squared error; thus larger errors have a disproportionately large effect on RMSD. Consequently, RMSD is sensitive to outliers. "
    },
    {
        "id": "METRIC_Root_mean_square",
        "name": "RMS",
        "full_name": "Root mean square",
        "also_known_as": [
            "quadratic mean"
        ],
        "description": "In mathematics and its applications, the root mean square (RMS) is defined as the square root of the mean square (the arithmetic mean of the squares of a set of numbers). The RMS is also known as the quadratic mean[2][3] and is a particular case of the generalized mean with exponent 2.",
        "wikipedia": "Root_mean_square",
        "type": "Metric",
        "probable_wikipedia": "Root mean square",
        "score": "10.533476",
        "probable_description": " In mathematics and its applications, the root mean square (RMS or rms) is defined as the square root of the mean square (the arithmetic mean of the squares of a set of numbers). The RMS is also known as the quadratic mean and is a particular case of the generalized mean with exponent\u00a02. RMS can also be defined for a continuously varying function in terms of an integral of the squares of the instantaneous values during a cycle.  For alternating electric current, RMS is equal to the value of the direct current that would produce the same average power dissipation in a resistive load.  In estimation theory, the root mean square error of an estimator is a measure of the imperfection of the fit of the estimator to the data. "
    },
    {
        "id": "METRIC_Root_mean_square",
        "name": "RMS",
        "full_name": "Root mean square",
        "also_known_as": [
            "quadratic mean"
        ],
        "description": "In mathematics and its applications, the root mean square (RMS) is defined as the square root of the mean square (the arithmetic mean of the squares of a set of numbers). The RMS is also known as the quadratic mean[2][3] and is a particular case of the generalized mean with exponent 2.",
        "wikipedia": "Root_mean_square",
        "type": "Metric",
        "probable_wikipedia": "Root mean square",
        "score": "10.533476",
        "probable_description": " In mathematics and its applications, the root mean square (RMS or rms) is defined as the square root of the mean square (the arithmetic mean of the squares of a set of numbers). The RMS is also known as the quadratic mean and is a particular case of the generalized mean with exponent\u00a02. RMS can also be defined for a continuously varying function in terms of an integral of the squares of the instantaneous values during a cycle.  For alternating electric current, RMS is equal to the value of the direct current that would produce the same average power dissipation in a resistive load.  In estimation theory, the root mean square error of an estimator is a measure of the imperfection of the fit of the estimator to the data. "
    },
    {
        "id": "METRIC_Average_absolute_deviation",
        "name": "AAD",
        "full_name": "Average absolute deviation",
        "also_known_as": [
            "mean absolute deviation",
            "median absolute deviation",
            "MAD"
        ],
        "description": "The average absolute deviation (AAD) of a data set is the average of the absolute deviations from a central point. It is a summary statistic of statistical dispersion or variability. In the general form, the central point can be a mean, median, mode, or the result of any other measure of central tendency or any reference value related to the given data set. AAD includes the mean absolute deviation and the median absolute deviation (both abbreviated as MAD).",
        "wikipedia": "Average_absolute_deviation",
        "type": "Metric",
        "probable_wikipedia": "Average absolute deviation",
        "score": "12.023769",
        "probable_description": " The average absolute deviation (or mean absolute deviation (MAD)) about any certain point (or 'avg. absolute deviation' only) of a data set is the average of the absolute deviations or the \"positive difference\" of the given data and that certain value (generally central values). It is a summary statistic of statistical dispersion or variability. In the general form, the central point can be the mean, median, mode, or the result of any other measure of central tendency or any random data point related to the given data set. The absolute values of the difference, between the data points and their central tendency, are totaled and divided by the number of data points. "
    },
    {
        "id": "METRIC_Reproducibility",
        "description": "Reproducibility, also known as replicability and repeatability, is a major principle underpinning the scientific method. For the findings of a study to be reproducible means that results obtained by an experiment or an observational study or in a statistical analysis of a data set should be achieved again with a high degree of reliability when the study is replicated.",
        "full_name": "Reproducibility",
        "wikipedia": "Reproducibility",
        "name": "Reproducibility",
        "also_known_as": [
            "replicability",
            "repeatability"
        ],
        "type": "Metric",
        "probable_wikipedia": "Reproducibility",
        "score": "11.247125",
        "probable_description": " Reproducibility is the closeness of the agreement between the results of measurements of the same measurand carried out with same methodology described in the corresponding scientific evidence (e.g. a publication in a peer-reviewed journal). Reproducibility can also be applied under changed conditions of measurement for the same measurand - to check that the results are not an artefact of the measurement procedures. A related concept is replication, which is the ability to independently achieve non-identical conclusions that are at least similar, when differences in sampling, research procedures and data analysis methods may exist. Reproducibility and replicability together are among the main tools of \"the scientific method\" \u2014 with the concrete expressions of the ideal of such a method varying considerably across research disciplines and fields of study. The study of reproducibility is an important topic in metascience. "
    },
    {
        "id": "METRIC_Peak_signal-to-noise_ratio",
        "name": "PSNR",
        "full_name": "Peak signal-to-noise ratio",
        "description": "Peak signal-to-noise ratio (PSNR) is an engineering term for the ratio between the maximum possible power of a signal and the power of corrupting noise that affects the fidelity of its representation. Because many signals have a very wide dynamic range, PSNR is usually expressed as a logarithmic quantity using the decibel scale.",
        "wikipedia": "Peak_signal-to-noise_ratio",
        "type": "Metric",
        "probable_wikipedia": "Peak signal-to-noise ratio",
        "score": "11.626891",
        "probable_description": " Peak signal-to-noise ratio, often abbreviated PSNR, is an engineering term for the ratio between the maximum possible power of a signal and the power of corrupting noise that affects the fidelity of its representation. Because many signals have a very wide dynamic range, PSNR is usually expressed in terms of the logarithmic decibel scale. "
    },
    {
        "id": "METRIC_Signal-to-noise_ratio",
        "name": "SNR",
        "also_known_as": [
            "S/N"
        ],
        "full_name": "Signal to noise ratio",
        "description": "Signal-to-noise ratio (SNR or S/N) is a measure used in science and engineering that compares the level of a desired signal to the level of background noise. SNR is defined as the ratio of signal power to the noise power, often expressed in decibels.",
        "wikipedia": "Signal-to-noise_ratio",
        "type": "Metric",
        "probable_wikipedia": "Signal-to-noise ratio",
        "score": "12.239565",
        "probable_description": " Signal-to-noise ratio (abbreviated SNR or S/N) is a measure used in science and engineering that compares the level of a desired signal to the level of background noise. SNR is defined as the ratio of signal power to the noise power, often expressed in decibels. A ratio higher than 1:1 (greater than 0\u00a0dB) indicates more signal than noise.  While SNR is commonly quoted for electrical signals, it can be applied to any form of signal, for example isotope levels in an ice core, biochemical signaling between cells, or financial trading signals. Signal-to-noise ratio is sometimes used metaphorically to refer to the ratio of useful information to false or irrelevant data in a conversation or exchange. For example, in online discussion forums and other online communities, off-topic posts and spam are regarded as \"noise\" that interferes with the \"signal\" of appropriate discussion.  The signal-to-noise ratio, the bandwidth, and the channel capacity of a communication channel are connected by the Shannon\u2013Hartley theorem. "
    },
    {
        "id": "METRIC_Mean_reciprocal_rank",
        "name": "MRR",
        "full_name": "Mean reciprocal rank",
        "description": "The mean reciprocal rank is a statistic measure for evaluating any process that produces a list of possible responses to a sample of queries, ordered by probability of correctness. The reciprocal rank of a query response is the multiplicative inverse of the rank of the first correct answer: 1 for first place, 1\u20442 for second place, 1\u20443 for third place and so on. The mean reciprocal rank is the average of the reciprocal ranks of results for a sample of queries Q",
        "wikipedia": "Mean_reciprocal_rank",
        "type": "Metric",
        "probable_wikipedia": "Mean reciprocal rank",
        "score": "12.44641",
        "probable_description": " The mean reciprocal rank is a statistic measure for evaluating any process that produces a list of possible responses to a sample of queries, ordered by probability of correctness. The reciprocal rank of a query response is the multiplicative inverse of the rank of the first correct answer: 1 for first place, for second place, for third place and so on. The mean reciprocal rank is the average of the reciprocal ranks of results for a sample of queries Q:  where formula_2 refers to the rank position of the \"first\" relevant document for the \"i\"-th query.  The reciprocal value of the mean reciprocal rank corresponds to the harmonic mean of the ranks. "
    },
    {
        "id": "METRIC_Huber_loss",
        "name": "Huber loss",
        "full_name": "Huber loss",
        "description": "In statistics, the Huber loss is a loss function used in robust regression, that is less sensitive to outliers in data than the squared error loss. A variant for classification is also sometimes used.",
        "wikipedia": "Huber_loss",
        "type": "Metric",
        "probable_wikipedia": "Huber loss",
        "score": "12.2764845",
        "probable_description": " In statistics, the Huber loss is a loss function used in robust regression, that is less sensitive to outliers in data than the squared error loss. A variant for classification is also sometimes used. "
    },
    {
        "id": "METRIC_Cross_entropy",
        "name": "Cross Entropy",
        "full_name": "Cross Entropy",
        "description": "Cross entropy indicates the distance between what the model believes the output distribution should be, and what the original distribution",
        "wikipedia": "Cross_entropy",
        "type": "Metric",
        "probable_wikipedia": "Cross entropy",
        "score": "9.84145",
        "probable_description": " In information theory, the cross entropy between two probability distributions formula_1 and formula_2 over the same underlying set of events measures the average number of bits needed to identify an event drawn from the set if a coding scheme used for the set is optimized for an estimated probability distribution formula_2, rather than the true distribution formula_1. "
    },
    {
        "id": "METRIC_Kullback-Leibler_divergence",
        "name": "KL divergence",
        "full_name": "Kullback Leibler divergence",
        "also_known_as": [
            "relative entropy",
            "KL Divergece",
            "DKL"
        ],
        "description": "In mathematical statistics, the Kullback-Leibler divergence, DKL (also called relative entropy), is a statistical distance: a measure of how one probability distribution Q is different from a second, reference probability distribution P",
        "type": "Metric",
        "probable_wikipedia": "Kullback\u2013Leibler divergence",
        "score": "12.209587",
        "probable_description": " In mathematical statistics, the Kullback\u2013Leibler divergence (also called relative entropy) is a measure of how one probability distribution is different from a second, reference probability distribution. Applications include characterizing the relative (Shannon) entropy in information systems, randomness in continuous time-series, and information gain when comparing statistical models of inference. In contrast to variation of information, it is a distribution-wise \"asymmetric\" measure and thus does not qualify as a statistical \"metric\" of spread (it also does not satisfy the triangle inequality). In the simple case, a Kullback\u2013Leibler divergence of 0 indicates that the two distributions in question are identical. In simplified terms, it is a measure of surprise, with diverse applications such as applied statistics, fluid mechanics, neuroscience and machine learning. "
    },
    {
        "id": "METRIC_Rate_of_convergence",
        "name": "Rate of convergence",
        "full_name": "Rate of convergence",
        "also_known_as": [
            "order of convergence"
        ],
        "description": "In numerical analysis, the order of convergence and the rate of convergence of a convergent sequence are quantities that represent how quickly the sequence approaches its limit. ",
        "type": "Metric",
        "probable_wikipedia": "Rate of convergence",
        "score": "12.464111",
        "probable_description": " In numerical analysis, the speed at which a convergent sequence approaches its limit is called the rate of convergence. Although strictly speaking, a limit does not give information about any finite first part of the sequence, the concept of rate of convergence is of practical importance when working with a sequence of successive approximations for an iterative method, as then typically fewer iterations are needed to yield a useful approximation if the rate of convergence is higher. This may even make the difference between needing ten or a million iterations.  Similar concepts are used for discretization methods. The solution of the discretized problem converges to the solution of the continuous problem as the grid size goes to zero, and the speed of convergence is one of the factors of the efficiency of the method. However, the terminology in this case is different from the terminology for iterative methods.  Series acceleration is a collection of techniques for improving the rate of convergence of a series discretization. Such acceleration is commonly accomplished with sequence transformations. "
    },
    {
        "id": "TASK_3d-face-reconstruction",
        "name": "3D Face Reconstruction",
        "description": "3D face reconstruction is the task of reconstructing a face from an image into a 3D form (or mesh).\r\n\r\n<span style=\"color:grey; opacity: 0.6\">( Image credit: [3DDFA_V2](https://github.com/cleardusk/3DDFA_V2) )</span>",
        "type": "Task",
        "probable_wikipedia": "3D reconstruction",
        "score": "2.4114194",
        "probable_description": " In computer vision and computer graphics, 3D reconstruction is the process of capturing the shape and appearance of real objects. This process can be accomplished either by active or passive methods. If the model is allowed to change its shape in time, this is referred to as non-rigid or spatio-temporal reconstruction. "
    },
    {
        "id": "TASK_3d-object-reconstruction",
        "name": "3D Object Reconstruction",
        "description": "Image: [Choy et al](https://arxiv.org/pdf/1604.00449v1.pdf)",
        "type": "Task",
        "probable_wikipedia": "3D reconstruction",
        "score": "5.2270875",
        "probable_description": " In computer vision and computer graphics, 3D reconstruction is the process of capturing the shape and appearance of real objects. This process can be accomplished either by active or passive methods. If the model is allowed to change its shape in time, this is referred to as non-rigid or spatio-temporal reconstruction. "
    },
    {
        "id": "TASK_3d-pose-estimation",
        "name": "3D Pose Estimation",
        "description": "Image credit: [GSNet: Joint Vehicle Pose and Shape Reconstruction with Geometrical and Scene-aware Supervision\r\n, ECCV'20](https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123600511.pdf)",
        "type": "Task",
        "probable_wikipedia": "3D pose estimation",
        "score": "10.502289",
        "probable_description": " 3D pose estimation is the problem of determining the transformation of an object in a 2D image which gives the 3D object. One of the requirements of 3D pose estimation arises from the limitations of feature-based pose estimation. There exist environments where it is difficult to extract corners or edges from an image. To circumvent these issues, the object is dealt with as a whole in noted techniques through the use of free-form contours. "
    },
    {
        "id": "TASK_3d-reconstruction",
        "name": "3D Reconstruction",
        "description": "Image: [Gwak et al](https://arxiv.org/pdf/1705.10904v2.pdf)",
        "type": "Task",
        "probable_wikipedia": "3D reconstruction",
        "score": "9.879914",
        "probable_description": " In computer vision and computer graphics, 3D reconstruction is the process of capturing the shape and appearance of real objects. This process can be accomplished either by active or passive methods. If the model is allowed to change its shape in time, this is referred to as non-rigid or spatio-temporal reconstruction. "
    },
    {
        "id": "TASK_3d-scene-reconstruction",
        "name": "3D Scene Reconstruction",
        "description": "Creating 3D scene either using conventional SFM pipelines or latest deep learning approaches.",
        "type": "Task",
        "probable_wikipedia": "3D reconstruction",
        "score": "1.699249",
        "probable_description": " In computer vision and computer graphics, 3D reconstruction is the process of capturing the shape and appearance of real objects. This process can be accomplished either by active or passive methods. If the model is allowed to change its shape in time, this is referred to as non-rigid or spatio-temporal reconstruction. "
    },
    {
        "id": "TASK_3d-shape-reconstruction",
        "name": "3D Shape Reconstruction",
        "description": "Image credit: [GSNet: Joint Vehicle Pose and Shape Reconstruction with Geometrical and Scene-aware Supervision\r\n, ECCV'20](https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123600511.pdf)",
        "type": "Task",
        "probable_wikipedia": "3D reconstruction",
        "score": "1.6604888",
        "probable_description": " In computer vision and computer graphics, 3D reconstruction is the process of capturing the shape and appearance of real objects. This process can be accomplished either by active or passive methods. If the model is allowed to change its shape in time, this is referred to as non-rigid or spatio-temporal reconstruction. "
    },
    {
        "id": "TASK_abusive-language",
        "name": "Abusive Language",
        "description": "",
        "type": "Task",
        "probable_wikipedia": "Abusive language (law)",
        "score": "2.3662624",
        "probable_description": " The use of abusive language to another person is illegal in a small number of U.S. states. Offenders are typically charged with this offense in conjunction with other crimes, such as aggressive driving or assault. However, in 1989 the New York State Court of Appeals ruled that abusive language was protected under the First Amendment to the United States Constitution.  "
    },
    {
        "id": "TASK_acoustic-echo-cancellation",
        "name": "Acoustic echo cancellation",
        "description": "",
        "type": "Task",
        "probable_wikipedia": "Echo suppression and cancellation",
        "score": "7.007931",
        "probable_description": " Echo suppression and echo cancellation are methods used in telephony to improve voice quality by preventing echo from being created or removing it after it is already present. In addition to improving subjective audio quality, echo suppression increases the capacity achieved through silence suppression by preventing echo from traveling across a network. Echo suppressors were developed in the 1950s in response to the first use of satellites for telecommunications, but they have since been largely supplanted by better performing echo cancellers.  Echo suppression and cancellation methods are commonly called acoustic echo suppression (AES) and acoustic echo cancellation (AEC), and more rarely line echo cancellation (LEC). In some cases, these terms are more precise, as there are various types and causes of echo with unique characteristics, including acoustic echo (sounds from a loudspeaker being reflected and recorded by a microphone, which can vary substantially over time) and line echo (electrical impulses caused by, e.g., coupling between the sending and receiving wires, impedance mismatches, electrical reflections, etc., which varies much less than acoustic echo). In practice, however, the same techniques are used to treat all types of echo, so an acoustic echo canceller can cancel line echo as well as acoustic echo. \"AEC\" in particular is commonly used to refer to echo cancelers in general, regardless of whether they were intended for acoustic echo, line echo, or both.  Although echo suppressors and echo cancellers have similar goals\u2014preventing a speaking individual from hearing an echo of their own voice\u2014the methods they use are different:  ITU"
    },
    {
        "id": "TASK_active-learning",
        "name": "Active Learning",
        "description": "**Active Learning** is a paradigm in supervised machine learning which uses fewer training examples to achieve better optimization by iteratively training a predictor, and using the predictor in each iteration to choose the training examples which will increase its chances of finding better configurations and at the same time improving the accuracy of the prediction model\r\n\r\n\r\n<span class=\"description-source\">Source: [Polystore++: Accelerated Polystore System for Heterogeneous Workloads ](https://arxiv.org/abs/1905.10336)</span>",
        "type": "Task",
        "probable_wikipedia": "Active learning (machine learning)",
        "score": "10.57282",
        "probable_description": " Active learning is a special case of machine learning in which a learning algorithm is able to interactively query the user (or some other information source) to obtain the desired outputs at new data points. In statistics literature it is sometimes also called optimal experimental design.  There are situations in which unlabeled data is abundant but manually labeling is expensive. In such a scenario, learning algorithms can actively query the user/teacher for labels. This type of iterative supervised learning is called active learning. Since the learner chooses the examples, the number of examples to learn a concept can often be much lower than the number required in normal supervised learning. With this approach, there is a risk that the algorithm is overwhelmed by uninformative examples. Recent developments are dedicated to multi-label active learning, hybrid active learning and active learning in a single-pass (on-line) context, combining concepts from the field of machine learning (e.g. conflict and ignorance) with adaptive, incremental learning policies in the field of online machine learning. "
    },
    {
        "id": "TASK_activity-recognition",
        "name": "Activity Recognition",
        "description": "Human **Activity Recognition** is the problem of identifying events performed by humans given a video input. It is formulated as a binary (or multiclass) classification problem of outputting activity class labels. Activity Recognition is an important problem with many societal applications including smart surveillance, video search/retrieval, intelligent robots, and other monitoring systems.\n\n\n<span class=\"description-source\">Source: [Learning Latent Sub-events in Activity Videos Using Temporal Attention Filters ](https://arxiv.org/abs/1605.08140)</span>",
        "type": "Task",
        "probable_wikipedia": "Activity recognition",
        "score": "8.272366",
        "probable_description": " Activity recognition aims to recognize the actions and goals of one or more agents from a series of observations on the agents' actions and the environmental conditions. Since the 1980s, this research field has captured the attention of several computer science communities due to its strength in providing personalized support for many different applications and its connection to many different fields of study such as medicine, human-computer interaction, or sociology.  Due to its many-faceted nature, different fields may refer to activity recognition as plan recognition, goal recognition, intent recognition, behavior recognition, location estimation and location-based services. "
    },
    {
        "id": "TASK_additive-models",
        "name": "Additive models",
        "description": "",
        "type": "Task",
        "probable_wikipedia": "Additive model",
        "score": "8.085113",
        "probable_description": " In statistics, an additive model (AM) is a nonparametric regression method. It was suggested by Jerome H. Friedman and Werner Stuetzle (1981) and is an essential part of the ACE algorithm. The \"AM\" uses a one-dimensional smoother to build a restricted class of nonparametric regression models. Because of this, it is less affected by the curse of dimensionality than e.g. a \"p\"-dimensional smoother. Furthermore, the \"AM\" is more flexible than a standard linear model, while being more interpretable than a general regression surface at the cost of approximation errors. Problems with \"AM\" include model selection, overfitting, and multicollinearity. "
    },
    {
        "id": "TASK_affine-transformation",
        "name": "Affine Transformation",
        "description": "",
        "type": "Task",
        "probable_wikipedia": "Affine transformation",
        "score": "9.073915",
        "probable_description": " In geometry, an affine transformation, affine map or an affinity (from the Latin, \"affinis\", \"connected with\") is a function between affine spaces which preserves points, straight lines and planes. Also, sets of parallel lines remain parallel after an affine transformation. An affine transformation does not necessarily preserve angles between lines or distances between points, though it does preserve ratios of distances between points lying on a straight line.  Examples of affine transformations include translation, scaling, homothety, similarity transformation, reflection, rotation, shear mapping, and compositions of them in any combination and sequence.  If formula_1 and formula_2 are affine spaces, then every affine transformation formula_3 is of the form formula_4, where formula_5 is a linear transformation on the space formula_1, formula_7 is a vector in formula_1, and formula_9 is a vector in formula_2. Unlike a purely linear transformation, an affine map need not preserve the zero point in a linear space. Thus, every linear transformation is affine, but not every affine transformation is linear.  All Euclidean spaces are affine, but there are affine spaces that are non-Euclidean. In affine coordinates, which include Cartesian coordinates in Euclidean spaces, each output coordinate of an affine map is a linear function (in the sense of calculus) of all input coordinates. Another way to deal with affine transformations systematically is to select a point as the origin; then, any affine transformation is equivalent to a linear transformation (of position vectors) followed by a translation. "
    },
    {
        "id": "TASK_air-pollution-prediction",
        "name": "Air Pollution Prediction",
        "description": "",
        "type": "Task",
        "probable_wikipedia": "Air pollution forecasting",
        "score": "1.3915808",
        "probable_description": " Air pollution forecasting is the application of science and technology to predict the composition of the Air pollution in the atmosphere for a given location and time.  The forecast may give the pollutants concentration or the air quality index.  Countries and cities are given forecasts by state and local government organizations, as well as private companies like Airly, AirVisual, Aerostate, BreezoMeter, PlumeLabs, and DRAXIS that give air pollution forecast.  "
    },
    {
        "id": "TASK_algorithmic-trading",
        "name": "Algorithmic Trading",
        "description": "An algorithmic trading system is a software that is used for trading in the stock market.",
        "type": "Task",
        "probable_wikipedia": "Algorithmic trading",
        "score": "12.226731",
        "probable_description": " Algorithmic trading is a method of executing orders using automated pre-programmed trading instructions accounting for variables such as time, price, and volume to send small slices of the order (child orders) out to the market over time. They were developed so that traders do not need to constantly watch a stock and repeatedly send those slices out manually. Popular \"algos\" include Percentage of Volume, Pegged, VWAP, TWAP, Implementation shortfall, Target close. In the twenty-first century, algorithmic trading has been gaining traction with both retail and institutional traders.  The term is also used to mean automated trading system. These do indeed have the goal of making a profit. Also known as \"black box trading\", \"Quant or Quantitative trading\", these encompass trading strategies that are heavily reliant on complex mathematical formulas and high-speed computer programs.  Such systems run strategies including market making, inter-market spreading, arbitrage, or pure speculation such as trend following. Many fall into the category of high-frequency trading (HFT), which are characterized by high turnover and high order-to-trade ratios. As a result, in February 2012, the Commodity Futures Trading Commission (CFTC) formed a special working group that included academics and industry experts to advise the CFTC on how best to define HFT. HFT strategies utilize computers that make elaborate decisions to initiate orders based on information that is received electronically, before human traders are capable of processing the information they observe. Algorithmic trading and HFT have resulted in a dramatic change of the market microstructure, particularly in the way liquidity is provided. "
    },
    {
        "id": "TASK_anomaly-detection",
        "name": "Anomaly Detection",
        "description": "Anomaly Detection, Anomaly Segmentation, Novelty Detection, Out-of-Distribution Detection",
        "type": "Task",
        "probable_wikipedia": "Anomaly detection",
        "score": "11.218549",
        "probable_description": " In data mining, anomaly detection (also outlier detection) is the identification of rare items, events or observations which raise suspicions by differing significantly from the majority of the data. Typically the anomalous items will translate to some kind of problem such as bank fraud, a structural defect, medical problems or errors in a text. Anomalies are also referred to as outliers, novelties, noise, deviations and exceptions.  In particular, in the context of abuse and network intrusion detection, the interesting objects are often not \"rare\" objects, but unexpected \"bursts\" in activity. This pattern does not adhere to the common statistical definition of an outlier as a rare object, and many outlier detection methods (in particular unsupervised methods) will fail on such data, unless it has been aggregated appropriately. Instead, a cluster analysis algorithm may be able to detect the micro clusters formed by these patterns.  Three broad categories of anomaly detection techniques exist. Unsupervised anomaly detection techniques detect anomalies in an unlabeled test data set under the assumption that the majority of the instances in the data set are normal by looking for instances that seem to fit least to the remainder of the data set. Supervised anomaly detection techniques require a data set that has been labeled as \"normal\" and \"abnormal\" and involves training a classifier (the key difference to many other statistical classification problems is the inherent unbalanced nature of outlier detection). Semi-supervised anomaly detection techniques construct a model representing normal behavior from a given \"normal\" training data set, and then test"
    },
    {
        "id": "TASK_argument-mining",
        "name": "Argument Mining",
        "description": "**Argument Mining** is a field of corpus-based discourse analysis that involves the automatic identification of argumentative structures in text.\n\n\n<span class=\"description-source\">Source: [AMPERSAND: Argument Mining for PERSuAsive oNline Discussions ](https://arxiv.org/abs/2004.14677)</span>",
        "type": "Task",
        "probable_wikipedia": "Argument mining",
        "score": "12.702195",
        "probable_description": " Argument mining, or argumentation mining, is a research area within the natural-language processing field. The goal of argument mining is the automatic extraction and identification of argumentative structures from natural language text with the aid of computer programs. Such argumentative structures include the premise, conclusions, the argument scheme and the relationship between the main and subsidiary argument, or the main and counter-argument within discourse. The Argument Mining workshop series is the main research forum for argument mining related research. "
    },
    {
        "id": "TASK_artificial-life",
        "name": "Artificial Life",
        "description": "",
        "type": "Task",
        "probable_wikipedia": "Artificial life",
        "score": "11.559078",
        "probable_description": " Artificial life (often abbreviated ALife or A-Life) is a field of study wherein researchers examine systems related to natural life, its processes, and its evolution, through the use of simulations with computer models, robotics, and biochemistry. The discipline was named by Christopher Langton, an American theoretical biologist, in 1986. There are three main kinds of alife, named for their approaches: \"soft\", from software; \"hard\", from hardware; and \"wet\", from biochemistry. Artificial life researchers study traditional biology by trying to recreate aspects of biological phenomena. "
    },
    {
        "id": "DATASET_atari-games",
        "name": "Atari games",
        "full_name": null,
        "url": "https://www.atariage.com/system_items.html?SystemID=2600&ItemTypeID=ROM",
        "type": "Dataset",
        "probable_wikipedia": "Atari Games",
        "score": "9.69854",
        "probable_description": " Atari Games Corporation (later renamed as Midway Games West) was an American producer of arcade games. It was originally the coin-operated arcade game division of Atari, Inc. and was split off into its own company in 1984. "
    },
    {
        "id": "TASK_audio-signal-processing",
        "name": "Audio Signal Processing",
        "description": "This is a general task that covers transforming audio inputs into audio outputs, not limited to existing PaperWithCode categories of Source Separation, Denoising, Classification, Recognition, etc.",
        "type": "Task",
        "probable_wikipedia": "Audio signal processing",
        "score": "11.977086",
        "probable_description": " Audio signal processing is a subfield of signal processing that is concerned with the electronic manipulation of audio signals. Audio signals are electronic representations of sound waves\u2014longitudinal waves which travel through air, consisting of compressions and rarefactions. The energy contained in audio signals is typically measured in decibels. As audio signals may be represented in either digital or analog format, processing may occur in either domain. Analog processors operate directly on the electrical signal, while digital processors operate mathematically on its digital representation. "
    },
    {
        "id": "TASK_audio-source-separation",
        "name": "Audio Source Separation",
        "description": "**Audio Source Separation** is the process of separating a mixture (e.g. a pop band recording) into isolated sounds from individual sources (e.g. just the lead vocals).\r\n\r\n\r\n<span class=\"description-source\">Source: [Model selection for deep audio source separation via clustering analysis ](https://arxiv.org/abs/1910.12626)</span>",
        "type": "Task",
        "probable_wikipedia": "Signal separation",
        "score": "0.8854372",
        "probable_description": " Source separation, blind signal separation (BSS) or blind source separation, is the separation of a set of source signals from a set of mixed signals, without the aid of information (or with very little information) about the source signals or the mixing process. It is most commonly applied in digital signal processing and involves the analysis of mixtures of signals; the objective is to recover the original component signals from a mixture signal. The classical example of a source separation problem is the cocktail party problem, where a number of people are talking simultaneously in a room (for example, at a cocktail party), and a listener is trying to follow one of the discussions. The human brain can handle this sort of auditory source separation problem, but it is a difficult problem in digital signal processing.  This problem is in general highly underdetermined, but useful solutions can be derived under a surprising variety of conditions. Much of the early literature in this field focuses on the separation of temporal signals such as audio. However, blind signal separation is now routinely performed on multidimensional data, such as images and tensors, which may involve no time dimension whatsoever.  Several approaches have been proposed for the solution of this problem but development is currently still very much in progress. Some of the more successful approaches are principal components analysis and independent component analysis, which work well when there are no delays or echoes present; that is, the problem is simplified a great deal. The field of"
    },
    {
        "id": "TASK_audio-visual-speech-recognition",
        "name": "Audio-Visual Speech Recognition",
        "description": "Audio-visual speech recognition is the task of transcribing a paired audio and visual stream into text.",
        "type": "Task",
        "probable_wikipedia": "Audio-visual speech recognition",
        "score": "6.9987907",
        "probable_description": " Audio visual speech recognition (AVSR) is a technique that uses image processing capabilities in lip reading to aid speech recognition systems in recognizing undeterministic phones or giving preponderance among near probability decisions. Each system of lip reading and speech recognition works separately, then their results are mixed at the stage of feature fusion. As the name suggests, it has two parts. first one is the audio part and second one is the visual part. In audio part we use features like log mel spectogram, mfcc etc. from the raw audio samples and we build a model to get feature vector out of it . For visual part generally we use some variant of convolutional neural network to compress the image to a feature vector after that we concatenate these two vectors (audio and visual ) and try to predict the target object.   "
    },
    {
        "id": "TASK_automated-essay-scoring",
        "name": "Automated Essay Scoring",
        "description": "Essay scoring: **Automated Essay Scoring** is the task of assigning a score to an essay, usually in the context of assessing the language ability of a language learner. The quality of an essay is affected by the following four primary dimensions: topic relevance, organization and coherence, word usage and sentence complexity, and grammar and mechanics.\n\n\n<span class=\"description-source\">Source: [A Joint Model for Multimodal Document Quality Assessment ](https://arxiv.org/abs/1901.01010)</span>",
        "type": "Task",
        "probable_wikipedia": "Automated essay scoring",
        "score": "12.1683655",
        "probable_description": " Automated essay scoring (AES) is the use of specialized computer programs to assign grades to essays written in an educational setting. It is a method of educational assessment and an application of natural language processing. Its objective is to classify a large set of textual entities into a small number of discrete categories, corresponding to the possible grades\u2014for example, the numbers 1 to 6. Therefore, it can be considered a problem of statistical classification.  Several factors have contributed to a growing interest in AES. Among them are cost, accountability, standards, and technology. Rising education costs have led to pressure to hold the educational system accountable for results by imposing standards. The advance of information technology promises to measure educational achievement at reduced cost.  The use of AES for high-stakes testing in education has generated significant backlash, with opponents pointing to research that computers cannot yet grade writing accurately and arguing that their use for such purposes promotes teaching writing in reductive ways (i.e. teaching to the test). "
    },
    {
        "id": "TASK_automated-theorem-proving",
        "name": "Automated Theorem Proving",
        "description": "The goal of **Automated Theorem Proving** is to automatically generate a proof, given a conjecture (the target theorem) and a knowledge base of known facts, all expressed in a formal language. Automated Theorem Proving is useful in a wide range of applications, including the verification and synthesis of software and hardware systems.\n\n\n<span class=\"description-source\">Source: [Learning to Prove Theorems by Learning to Generate Theorems ](https://arxiv.org/abs/2002.07019)</span>",
        "type": "Task",
        "probable_wikipedia": "Automated theorem proving",
        "score": "11.318149",
        "probable_description": " Automated theorem proving (also known as ATP or automated deduction) is a subfield of automated reasoning and mathematical logic dealing with proving mathematical theorems by computer programs. Automated reasoning over mathematical proof was a major impetus for the development of computer science. "
    },
    {
        "id": "TASK_automatic-speech-recognition",
        "name": "Automatic Speech Recognition",
        "description": "",
        "type": "Task",
        "probable_wikipedia": "Speech recognition",
        "score": "4.7230773",
        "probable_description": " Speech recognition is a interdisciplinary subfield of computational linguistics that develops methodologies and technologies that enables the recognition and translation of spoken language into text by computers. It is also known as automatic speech recognition (ASR), computer speech recognition or speech to text (STT). It incorporates knowledge and research in the linguistics, computer science, and electrical engineering fields.  Some speech recognition systems require \"training\" (also called \"enrollment\") where an individual speaker reads text or isolated vocabulary into the system. The system analyzes the person's specific voice and uses it to fine-tune the recognition of that person's speech, resulting in increased accuracy. Systems that do not use training are called \"speaker independent\" systems. Systems that use training are called \"speaker dependent\".  Speech recognition applications include voice user interfaces such as voice dialing (e.g. \"call home\"), call routing (e.g. \"I would like to make a collect call\"), domotic appliance control, search (e.g. find a podcast where particular words were spoken), simple data entry (e.g., entering a credit card number), preparation of structured documents (e.g. a radiology report), determining speaker characteristics, speech-to-text processing (e.g., word processors or emails), and aircraft (usually termed direct voice input).  The term \"voice recognition\" or \"speaker identification\" refers to identifying the speaker, rather than what they are saying. Recognizing the speaker can simplify the task of translating speech in systems that have been trained on a specific person's voice or it can be used to authenticate or verify the identity of a speaker as part of a security process. "
    },
    {
        "id": "TASK_automatic-writing",
        "name": "Automatic Writing",
        "description": "Generating text based on internal machine representations.",
        "type": "Task",
        "probable_wikipedia": "Automatic writing",
        "score": "8.381663",
        "probable_description": " Automatic writing or psychography is a claimed psychic ability allowing a person to produce written words without consciously writing. The words purportedly arise from a subconscious, spiritual or supernatural source. Scientists and skeptics consider automatic writing to be the result of the ideomotor effect and even proponents of automatic writing admit it has been the source of innumerable cases of self-delusion. Automatic writing is not the same thing as free writing. "
    },
    {
        "id": "TASK_automl",
        "name": "AutoML",
        "description": "Automated Machine Learning (**AutoML**) is a general concept which covers diverse techniques for automated model learning including automatic data preprocessing, architecture search, and model selection.\nSource: Evaluating recommender systems for AI-driven data science (1905.09205)\n\n\n<span class=\"description-source\">Source: [CHOPT : Automated Hyperparameter Optimization Framework for Cloud-Based Machine Learning Platforms ](https://arxiv.org/abs/1810.03527)</span>",
        "type": "Task",
        "probable_wikipedia": "Automated machine learning",
        "score": "7.04811",
        "probable_description": " Automated machine learning (AutoML) is the process of automating end-to-end the process of applying machine learning to real-world problems. In a typical machine learning application, practitioners have a dataset consisting of input data points to train on. The raw data itself may not be in a form that all algorithms may be applicable to it out of the box. An expert may have to apply the appropriate data pre-processing, feature engineering, feature extraction, and feature selection methods that make the dataset amenable for machine learning. Following those preprocessing steps, practitioners must then perform algorithm selection and hyperparameter optimization to maximize the predictive performance of their final machine learning model. As many of these steps are often beyond the abilities of non-experts, AutoML was proposed as an artificial intelligence-based solution to the ever-growing challenge of applying machine learning. Automating the process of applying machine learning end-to-end offers the advantages of producing simpler solutions, faster creation of those solutions, and models that often outperform models that were designed by hand. However, AutoML is not a silver bullet and can introduce additional parameters of its own, called hyperhyperparameters, which may need some expertise to be set themselves. But it does make application of Machine Learning easier for non-experts. "
    },
    {
        "id": "TASK_band-gap",
        "name": "Band Gap",
        "description": "",
        "type": "Task",
        "probable_wikipedia": "Band gap",
        "score": "11.067381",
        "probable_description": " In solid-state physics, a band gap, also called an energy gap or bandgap, is an energy range in a solid where no electron states can exist. In graphs of the electronic band structure of solids, the band gap generally refers to the energy difference (in electron volts) between the top of the valence band and the bottom of the conduction band in insulators and semiconductors. It is the energy required to promote a valence electron bound to an atom to become a conduction electron, which is free to move within the crystal lattice and serve as a charge carrier to conduct electric current. It is closely related to the HOMO/LUMO gap in chemistry. If the valence band is completely full and the conduction band is completely empty, then electrons cannot move in the solid; however, if some electrons transfer from the valence to the conduction band, then current \"can\" flow (see carrier generation and recombination). Therefore, the band gap is a major factor determining the electrical conductivity of a solid. Substances with large band gaps are generally insulators, those with smaller band gaps are semiconductors, while conductors either have very small band gaps or none, because the valence and conduction bands overlap. "
    },
    {
        "id": "TASK_bandwidth-extension",
        "name": "Bandwidth Extension",
        "description": "",
        "type": "Task",
        "probable_wikipedia": "Bandwidth extension",
        "score": "9.850398",
        "probable_description": " Bandwidth extension of signal is defined as the deliberate process of expanding the frequency range (bandwidth) of a signal in which it contains an appreciable and useful content, and/or the frequency range in which its effects are such. Its significant advancement in recent years has led to the technology being adopted commercially in several areas including psychacoustic bass enhancement of small loudspeakers and the high frequency enhancement of coded speech and audio.  Bandwidth extension has been used in both speech and audio compression applications. The algorithms used in G.729.1 and Spectral Band Replication (SBR) are two of many examples of bandwidth extension algorithms currently in use. In these methods, the low band of the spectrum is encoded using an existing codec, whereas the high band is coarsely parameterized using fewer parameters. Many of these bandwidth extension algorithms make use of the correlation between the low band and the high band in order to predict the wider band signal from extracted lower-band features. Others encode the high band using very few bits. This is often sufficient since the ear is less sensitive to distortions in the high band compared to the low band. "
    },
    {
        "id": "TASK_bayesian-inference",
        "name": "Bayesian Inference",
        "description": "Bayesian Inference is a methodology that employs Bayes Rule to estimate parameters (and their full posterior).",
        "type": "Task",
        "probable_wikipedia": "Bayesian inference",
        "score": "12.386669",
        "probable_description": " Bayesian inference is a method of statistical inference in which Bayes' theorem is used to update the probability for a hypothesis as more evidence or information becomes available. Bayesian inference is an important technique in statistics, and especially in mathematical statistics. Bayesian updating is particularly important in the dynamic analysis of a sequence of data. Bayesian inference has found application in a wide range of activities, including science, engineering, philosophy, medicine, sport, and law. In the philosophy of decision theory, Bayesian inference is closely related to subjective probability, often called \"Bayesian probability\". "
    },
    {
        "id": "TASK_bilevel-optimization",
        "name": "Bilevel Optimization",
        "description": "**Bilevel Optimization** is a branch of optimization, which contains a nested optimization problem within the constraints of the outer optimization problem. The outer optimization task is usually referred as the upper level task, and the nested inner optimization task is referred as the lower level task. The lower level problem appears as a constraint, such that only an optimal solution to the lower level optimization problem is a possible feasible candidate to the upper level optimization problem.\r\n\r\n\r\n<span class=\"description-source\">Source: [Efficient Evolutionary Algorithm for Single-Objective Bilevel Optimization ](https://arxiv.org/abs/1303.3901)</span>",
        "type": "Task",
        "probable_wikipedia": "Bilevel optimization",
        "score": "12.508203",
        "probable_description": " Bilevel optimization is a special kind of optimization where one problem is embedded (nested) within another. The outer optimization task is commonly referred to as the upper-level optimization task, and the inner optimization task is commonly referred to as the lower-level optimization task. These problems involve two kinds of variables, referred to as the upper-level variables and the lower-level variables. "
    },
    {
        "id": "TASK_blood-cell-count",
        "name": "Blood Cell Count",
        "description": "",
        "type": "Task",
        "probable_wikipedia": "Cell counting",
        "score": "0.7828181",
        "probable_description": " Cell counting is any of various methods for the counting or similar quantification of cells in the life sciences, including medical diagnosis and treatment. It is an important subset of cytometry, with applications in research and clinical practice. For example, the complete blood count can help a physician to determine why a patient feels unwell and what to do to help. Cell counts within liquid media (such as blood, plasma, lymph, or laboratory rinsate) are usually expressed as a number of cells per unit of volume, thus expressing a concentration (for example, 5,000 cells per milliliter). "
    },
    {
        "id": "TASK_board-games",
        "name": "Board Games",
        "description": "",
        "type": "Task",
        "probable_wikipedia": "Board game",
        "score": "11.489493",
        "probable_description": " A board game is a tabletop game that involves counters or moved or placed on a pre-marked surface or \"board\", according to a set of rules. Some games are based on pure strategy, but many contain an element of chance; and some are purely chance, with no element of skill.  Games usually have a goal that a player aims to achieve. Early board games represented a battle between two armies, and most modern board games are still based on defeating opponents in terms of counters, winning position, or accrual of points.  There are many varieties of board games. Their representation of real-life situations can range from having no inherent theme, like checkers, to having a specific theme and narrative, like \"Cluedo\". Rules can range from the very simple, like Tic-tac-toe, to those describing a game universe in great detail, like \"Dungeons & Dragons\"\u00a0\u2013 although most of the latter are role-playing games where the board is secondary to the game, serving to help visualize the game scenario.  The time required to learn to play or master a game varies greatly from game to game, but is not necessarily correlated with the number or complexity of rules; games like chess or Go possess relatively simple , but have great strategic depth. "
    },
    {
        "id": "TASK_breast-tumour-classification",
        "name": "Breast Tumour Classification",
        "description": "",
        "type": "Task",
        "probable_wikipedia": "Breast cancer classification",
        "score": "1.6388571",
        "probable_description": " Breast cancer classification divides breast cancer into categories according to different schemes criteria and serving a different purpose. The major categories are the histopathological type, the grade of the tumor, the stage of the tumor, and the expression of proteins and genes. As knowledge of cancer cell biology develops these classifications are updated.  The purpose of classification is to select the best treatment. The effectiveness of a specific treatment is demonstrated for a specific breast cancer (usually by randomized, controlled trials). That treatment may not be effective in a different breast cancer. Some breast cancers are aggressive and life-threatening, and must be treated with aggressive treatments that have major adverse effects. Other breast cancers are less aggressive and can be treated with less aggressive treatments, such as lumpectomy.  Treatment algorithms rely on breast cancer classification to define specific subgroups that are each treated according to the best evidence available. Classification aspects must be carefully tested and validated, such that confounding effects are minimized, making them either true prognostic factors, which estimate disease outcomes such as disease-free or overall survival in the absence of therapy, or true predictive factors, which estimate the likelihood of response or lack of response to a specific treatment.  Classification of breast cancer is usually, but not always, primarily based on the histological appearance of tissue in the tumor. A variant from this approach, defined on the basis of physical exam findings, is that inflammatory breast cancer (IBC), a form of ductal carcinoma or malignant cancer in the ducts,"
    },
    {
        "id": "TASK_camera-auto-calibration",
        "name": "Camera Auto-Calibration",
        "description": "",
        "type": "Task",
        "probable_wikipedia": "Camera auto-calibration",
        "score": "12.4553795",
        "probable_description": " Camera auto-calibration is the process of determining internal camera parameters directly from multiple uncalibrated images of unstructured scenes. In contrast to classic camera calibration, auto-calibration does not require any special calibration objects in the scene. In the visual effects industry, camera auto-calibration is often part of the \"Match Moving\" process where a synthetic camera trajectory and intrinsic projection model are solved to reproject synthetic content into video.  Camera auto-calibration is a form of sensor ego-structure discovery; the subjective effects of the sensor are separated from the objective effects of the environment leading to a reconstruction of the perceived world without the bias applied by the measurement device. This is achieved via the fundamental assumption that images are projected from a Euclidean space through a linear, 5 degree of freedom (in the simplest case), pinhole camera model with non-linear optical distortion. The linear pinhole parameters are the focal length, the aspect ratio, the skew, and the 2D principal point. With only a set of uncalibrated (or calibrated) images, a scene may be reconstructed up to a six degree of freedom euclidean transform and an isotropic scaling.  A mathematical theory for general multi-view camera self-calibration was originally demonstrated in 1992 by Olivier Faugeras, QT Luong, and Stephen J. Maybank. In 3D scenes and general motions, each pair of views provides two constraints on the 5 degree-of-freedom calibration. Therefore, three views are the minimum needed for full calibration with fixed intrinsic parameters between views. Quality modern imaging sensors and optics may also provide further prior constraints"
    },
    {
        "id": "TASK_camera-calibration",
        "name": "Camera Calibration",
        "description": "",
        "type": "Task",
        "probable_wikipedia": "Camera resectioning",
        "score": "1.644636",
        "probable_description": " Camera resectioning is the process of estimating the parameters of a pinhole camera model approximating the camera that produced a given photograph or video. Usually, the pinhole camera parameters are represented in a 3\u00a0\u00d7\u00a04 matrix called the camera matrix.  This process is often called camera calibration, although that term can also refer to photometric camera calibration. "
    },
    {
        "id": "TASK_cantilever-beam",
        "name": "Cantilever Beam",
        "description": "",
        "type": "Task",
        "probable_wikipedia": "Cantilever",
        "score": "0.124595486",
        "probable_description": " A cantilever is a rigid structural element, such as a beam or a plate, anchored at one end to a (usually vertical) support from which it protrudes; this connection could also be perpendicular to a flat, vertical surface such as a wall. Cantilevers can also be constructed with trusses or slabs. When subjected to a structural load, the cantilever carries the load to the support where it is forced against by a moment and shear stress.  Cantilever construction allows overhanging structures without external bracing, in contrast to constructions supported at both ends with loads applied between the supports, such as a simply supported beam found in a post and lintel system. "
    },
    {
        "id": "TASK_card-games",
        "name": "Card Games",
        "description": "Card games involve playing cards: the task is to train an agent to play the game with specified rules and beat other players.",
        "type": "Task",
        "probable_wikipedia": "Card game",
        "score": "11.619886",
        "probable_description": " A card game is any game using playing cards as the primary device with which the game is played, be they traditional or game-specific.  Countless card games exist, including families of related games (such as poker). A small number of card games played with traditional decks have formally standardized rules, but most are folk games whose rules vary by region, culture, and person.  A card game is played with a \"deck\" or \"pack\" of playing cards which are identical in size and shape. Each card has two sides, the \"face\" and the \"back\". Normally the backs of the cards are indistinguishable. The faces of the cards may all be unique, or there can be duplicates. The composition of a deck is known to each player. In some cases several decks are shuffled together to form a single \"pack\" or \"shoe\".  Games using playing cards exploit the fact that cards are individually identifiable from one side only, so that each player knows only the cards he holds and not those held by anyone else. For this reason card games are often characterized as games of chance or \u201cimperfect information\u201d\u2014as distinct from games of strategy or \u201cperfect information,\u201d where the current position is fully visible to all players throughout the game. Many games that are not generally placed in the family of card games do in fact use cards for some aspect of their gameplay.  Some games that are placed in the card game genre involve a board. The distinction is that the"
    },
    {
        "id": "TASK_caricature",
        "name": "Caricature",
        "description": "**Caricature** is a pictorial representation or description that deliberately exaggerates a person\u00e2\u20ac\u2122s distinctive features or peculiarities to create an easily identifiable visual likeness with a comic effect. This vivid art form contains the concepts of abstraction, simplification and exaggeration.\n\n\n<span class=\"description-source\">Source: [Alive Caricature from 2D to 3D ](https://arxiv.org/abs/1803.06802)</span>",
        "type": "Task",
        "probable_wikipedia": "Caricature",
        "score": "11.680912",
        "probable_description": " A caricature is a rendered image showing the features of its subject in a simplified or exaggerated way through sketching, pencil strokes, or through other artistic drawings.  In literature, a caricature is a description of a person using exaggeration of some characteristics and oversimplification of others.  Caricatures can be insulting or complimentary and can serve a political purpose or be drawn solely for entertainment. Caricatures of politicians are commonly used in editorial cartoons, while caricatures of movie stars are often found in entertainment magazines. "
    },
    {
        "id": "TASK_carracing-v0",
        "name": "Car Racing",
        "description": "https://gym.openai.com/envs/CarRacing-v0/",
        "type": "Task",
        "probable_wikipedia": "Auto racing",
        "score": "6.7695684",
        "probable_description": " Auto racing (also known as car racing, motor racing, or automobile racing) is a motorsport involving the racing of automobiles for competition.  Auto racing has existed since the invention of the automobile. Races of various sorts were organised, with the first recorded as early as 1867. Many of the earliest events were effectively reliability trials, aimed at proving these new machines were a practical mode of transport, but soon became an important way for competing makers to demonstrate their machines. By the 1930s, specialist racing cars had developed. There are now numerous different categories, each with different rules and regulations. "
    },
    {
        "id": "METHOD_causal-inference",
        "name": "Causal Inference",
        "full_name": "Causal Inference",
        "description": "Causal inference is the process of drawing a conclusion about a causal connection based on the conditions of the occurrence of an effect. The main difference between causal inference and inference of association is that the former analyzes the response of the effect variable when the cause is changed.",
        "paper": null,
        "type": "Method",
        "probable_wikipedia": "Causal inference",
        "score": "12.58503",
        "probable_description": " Causal inference is the process of drawing a conclusion about a causal connection based on the conditions of the occurrence of an effect. The main difference between causal inference and inference of association is that the former analyzes the response of the effect variable when the cause is changed. The science of why things occur is called etiology. Causal inference is an example of causal reasoning. "
    },
    {
        "id": "TASK_ccg-supertagging",
        "name": "CCG Supertagging",
        "description": "Combinatory Categorical Grammar (CCG; [Steedman, 2000](http://www.citeulike.org/group/14833/article/8971002)) is a\r\nhighly lexicalized formalism. The standard parsing model of [Clark and Curran (2007)](https://www.mitpressjournals.org/doi/abs/10.1162/coli.2007.33.4.493)\r\nuses over 400 lexical categories (or _supertags_), compared to about 50 part-of-speech tags for typical parsers.\r\n\r\nExample:\r\n\r\n| Vinken | , | 61 | years | old |\r\n| --- | ---| --- | --- | --- |\r\n| N| , | N/N | N | (S[adj]\\ NP)\\ NP |",
        "type": "Task",
        "probable_wikipedia": "Combinatory categorial grammar",
        "score": "2.9667711",
        "probable_description": " Combinatory categorial grammar (CCG) is an efficiently parsable, yet linguistically expressive grammar formalism. It has a transparent interface between surface syntax and underlying semantic representation, including predicate-argument structure, quantification and information structure. The formalism generates constituency-based structures (as opposed to dependency-based ones) and is therefore a type of phrase structure grammar (as opposed to a dependency grammar).  CCG relies on combinatory logic, which has the same expressive power as the lambda calculus, but builds its expressions differently. The first linguistic and psycholinguistic arguments for basing the grammar on combinators were put forth by Steedman and Szabolcsi. More recent prominent proponents of the approach are Pauline Jacobson and Jason Baldridge.  For example, the combinator B (the compositor) is useful in creating long-distance dependencies, as in \"Who do you think Mary is talking about?\" and the combinator W (the duplicator) is useful as the lexical interpretation of reflexive pronouns, as in \"Mary talks about herself\". Together with I (the identity mapping) and C (the permutator) these form a set of primitive, non-interdefinable combinators. Jacobson interprets personal pronouns as the combinator I, and their binding is aided by a complex combinator Z, as in \"Mary lost her way\". Z is definable using W and B. "
    },
    {
        "id": "TASK_change-detection",
        "name": "Change Detection",
        "description": "Image credit: [\"A TRANSFORMER-BASED SIAMESE NETWORK FOR CHANGE DETECTION\"](https://arxiv.org/pdf/2201.01293v1.pdf)",
        "type": "Task",
        "probable_wikipedia": "Change detection",
        "score": "9.370454",
        "probable_description": " In statistical analysis, change detection or change point detection tries to identify times when the probability distribution of a stochastic process or time series changes. In general the problem concerns both detecting whether or not a change has occurred, or whether several changes might have occurred, and identifying the times of any such changes.  Specific applications, like step detection and edge detection, may be concerned with changes in the mean, variance, correlation, or spectral density of the process. More generally change detection also includes the detection of anomalous behavior: anomaly detection. "
    },
    {
        "id": "TASK_change-point-detection",
        "name": "Change Point Detection",
        "description": "Change point detection is concerned with the accurate detection of abrupt and significant changes in the behavior of a time series.",
        "type": "Task",
        "probable_wikipedia": "Change detection",
        "score": "9.886384",
        "probable_description": " In statistical analysis, change detection or change point detection tries to identify times when the probability distribution of a stochastic process or time series changes. In general the problem concerns both detecting whether or not a change has occurred, or whether several changes might have occurred, and identifying the times of any such changes.  Specific applications, like step detection and edge detection, may be concerned with changes in the mean, variance, correlation, or spectral density of the process. More generally change detection also includes the detection of anomalous behavior: anomaly detection. "
    },
    {
        "id": "TASK_chatbot",
        "name": "Chatbot",
        "description": "**Chatbot** or conversational AI is a language model designed and implemented to have conversations with humans. \r\n\r\n\r\n<span class=\"description-source\">Source: [Open Data Chatbot ](https://arxiv.org/abs/1909.03653)</span>\r\n\r\n[Image source](https://arxiv.org/pdf/2006.16779v3.pdf)",
        "type": "Task",
        "probable_wikipedia": "Chatbot",
        "score": "12.548416",
        "probable_description": " A chatbot is a computer program which conducts a conversation via auditory or textual methods. Such programs are often designed to convincingly simulate how a human would behave as a conversational partner, although as of 2019, they are far short of being able to pass the Turing test. Chatbots are typically used in dialog systems for various practical purposes including customer service or information acquisition. Some chatbots use sophisticated natural language processing systems, but many simpler ones scan for keywords within the input, then pull a reply with the most matching keywords, or the most similar wording pattern, from a database.  The term \"ChatterBot\" was originally coined by Michael Mauldin (creator of the first Verbot, Julia) in 1994 to describe these conversational programs. Today, most chatbots are accessed via virtual assistants such as Google Assistant and Amazon Alexa, via messaging apps such as Facebook Messenger or WeChat, or via individual organizations' apps and websites. Chatbots can be classified into usage categories such as conversational commerce (e-commerce via chat), analytics, communication, customer support, design, developer tools, education, entertainment, finance, food, games, health, HR, marketing, news, personal, productivity, shopping, social, sports, travel and utilities.  Beyond chatbots, Conversational AI refers to the use of messaging apps, speech-based assistants and chatbots to automate communication and create personalized customer experiences at scale. "
    },
    {
        "id": "TASK_chinese-part-of-speech-tagging",
        "name": "Chinese Part-of-Speech Tagging",
        "description": "",
        "type": "Task",
        "probable_wikipedia": "Part-of-speech tagging",
        "score": "0.9077344",
        "probable_description": " In corpus linguistics, part-of-speech tagging (POS tagging or PoS tagging or POST), also called grammatical tagging or word-category disambiguation, is the process of marking up a word in a text (corpus) as corresponding to a particular part of speech, based on both its definition and its context\u2014i.e., its relationship with adjacent and related words in a phrase, sentence, or paragraph. A simplified form of this is commonly taught to school-age children, in the identification of words as nouns, verbs, adjectives, adverbs, etc.  Once performed by hand, POS tagging is now done in the context of computational linguistics, using algorithms which associate discrete terms, as well as hidden parts of speech, in accordance with a set of descriptive tags. POS-tagging algorithms fall into two distinctive groups: rule-based and stochastic. E. Brill's tagger, one of the first and most widely used English POS-taggers, employs rule-based algorithms. "
    },
    {
        "id": "TASK_chunking",
        "name": "Chunking",
        "description": "Chunking, also known as shallow parsing, identifies continuous spans of tokens that form syntactic units such as noun phrases or verb phrases.\r\n\r\nExample:\r\n\r\n| Vinken | , | 61 | years | old |\r\n| --- | ---| --- | --- | --- |\r\n| B-NLP| I-NP | I-NP | I-NP | I-NP |",
        "type": "Task",
        "probable_wikipedia": "Chunking (computing)",
        "score": "5.3881707",
        "probable_description": " In computer programming, chunking has multiple meanings. "
    },
    {
        "id": "TASK_clickbait-detection",
        "name": "Clickbait Detection",
        "description": "Clickbait detection is the task of identifying clickbait, a form of false advertisement, that uses hyperlink text or a thumbnail link that is designed to attract attention and to entice users to follow that link and read, view, or listen to the linked piece of online content, with a defining characteristic of being deceptive, typically sensationalized or misleading (Source: Adapted from Wikipedia)",
        "type": "Task",
        "probable_wikipedia": "Clickbait",
        "score": "3.124396",
        "probable_description": " Clickbait is a form of false advertisement which uses hyperlink text or a thumbnail link that is designed to attract attention and entice users to follow that link and read, view, or listen to the linked piece of online content, with a defining characteristic of being deceptive, typically sensationalized or misleading. A \"teaser\" aims to exploit the \"curiosity gap\", providing just enough information to make readers of news websites curious, but not enough to satisfy their curiosity without clicking through to the linked content. Click-bait headlines add an element of dishonesty, using enticements that do not accurately reflect the content being delivered. The \"-bait\" part of the term is used in analogy to fishing, where a hook is disguised by an enticement (bait), presenting the impression to the fish that it is a desirable thing to swallow.  Long before the internet, there was the unscrupulous marketing practice known as bait-and-switch, using similar dishonest methods to hook customers. Like bait-and-switch, clickbait is a form of fraud. (\"Click fraud\", however, is a separate form of online misrepresentation which uses a more extreme disconnect between what is being presented in the frontside of the link versus what is on the click-through side of the link, also encompassing malicious code.) The term 'clickbait' does not encompass all cases where the user arrives at a destination that is not anticipated from the link that is clicked. When the manipulation is done for the purpose of humor, as with rickrolling, and there is no element of exploitation, then that deception"
    },
    {
        "id": "TASK_click-through-rate-prediction",
        "name": "Click-Through Rate Prediction",
        "description": "Click-through rate prediction is the task of predicting the likelihood that something on a website (such as an advertisement) will be clicked.\r\n\r\n<span style=\"color:grey; opacity: 0.6\">( Image credit: [Deep Spatio-Temporal Neural Networks for Click-Through Rate Prediction](https://arxiv.org/pdf/1906.03776v2.pdf) )</span>",
        "type": "Task",
        "probable_wikipedia": "Click-through rate",
        "score": "3.7325616",
        "probable_description": " Click-through rate (CTR) is the ratio of users who click on a specific link to the number of total users who view a page, email, or advertisement. It is commonly used to measure the success of an online advertising campaign for a particular website as well as the effectiveness of email campaigns.  Click-through rates for ad campaigns vary tremendously. The very first online display ad shown for AT&T on the website HotWired in 1994, had a 44% click-through rate. With time, the overall rate of user's clicks on webpage banner ads has decreased. "
    },
    {
        "id": "TASK_cloze-test",
        "name": "Cloze Test",
        "description": "",
        "type": "Task",
        "probable_wikipedia": "Cloze test",
        "score": "11.866337",
        "probable_description": " A cloze test (also cloze deletion test) is an exercise, test, or assessment consisting of a portion of language with certain items, words, or signs removed (cloze text), where the participant is asked to replace the missing language item. Cloze tests require the ability to understand context and vocabulary in order to identify the correct language or part of speech that belongs in the deleted passages. This exercise is commonly administered for the assessment of native and second language learning and instruction.  The word \"cloze\" is derived from \"closure\" in Gestalt theory. The exercise was first described by W.L. Taylor in 1953. "
    },
    {
        "id": "TASK_code-generation",
        "name": "Code Generation",
        "description": "**Code Generation** is an important field to predict explicit code or program structure from multimodal data sources such as incomplete code, programs in another programming language, natural language descriptions or execution examples. Code Generation tools can assist the development of automatic programming tools to improve programming productivity.\r\n\r\n\r\n<span class=\"description-source\">Source: [Deep Learning for Source Code Modeling and Generation ](https://arxiv.org/abs/2002.05442)</span>\r\n\r\nImage source: [Measuring Coding Challenge Competence With APPS](https://paperswithcode.com/paper/measuring-coding-challenge-competence-with)",
        "type": "Task",
        "probable_wikipedia": "Code generation (compiler)",
        "score": "4.2616553",
        "probable_description": " In computing, code generation is the process by which a compiler's code generator converts some intermediate representation of source code into a form (e.g., machine code) that can be readily executed by a machine.  Sophisticated compilers typically perform multiple passes over various intermediate forms. This multi-stage process is used because many algorithms for code optimization are easier to apply one at a time, or because the input to one optimization relies on the completed processing performed by another optimization. This organization also facilitates the creation of a single compiler that can target multiple architectures, as only the last of the code generation stages (the \"backend\") needs to change from target to target. (For more information on compiler design, see Compiler.)  The input to the code generator typically consists of a parse tree or an abstract syntax tree. The tree is converted into a linear sequence of instructions, usually in an intermediate language such as three-address code. Further stages of compilation may or may not be referred to as \"code generation\", depending on whether they involve a significant change in the representation of the program. (For example, a peephole optimization pass would not likely be called \"code generation\", although a code generator might incorporate a peephole optimization pass.) "
    },
    {
        "id": "TASK_collaborative-filtering",
        "name": "Collaborative Filtering",
        "description": "",
        "type": "Task",
        "probable_wikipedia": "Collaborative filtering",
        "score": "11.965314",
        "probable_description": " Collaborative filtering (CF) is a technique used by recommender systems. Collaborative filtering has two senses, a narrow one and a more general one.  In the newer, narrower sense, collaborative filtering is a method of making automatic predictions (filtering) about the interests of a user by collecting preferences or taste information from many users (collaborating). The underlying assumption of the collaborative filtering approach is that if a person \"A\" has the same opinion as a person \"B\" on an issue, A is more likely to have B's opinion on a different issue than that of a randomly chosen person. For example, a collaborative filtering recommendation system for television tastes could make predictions about which television show a user should like given a partial list of that user's tastes (likes or dislikes). Note that these predictions are specific to the user, but use information gleaned from many users. This differs from the simpler approach of giving an average (non-specific) score for each item of interest, for example based on its number of votes.  In the more general sense, collaborative filtering is the process of filtering for information or patterns using techniques involving collaboration among multiple agents, viewpoints, data sources, etc. Applications of collaborative filtering typically involve very large data sets. Collaborative filtering methods have been applied to many different kinds of data including: sensing and monitoring data, such as in mineral exploration, environmental sensing over large areas or multiple sensors; financial data, such as financial service institutions that integrate many financial sources; or in electronic"
    },
    {
        "id": "TASK_color-constancy",
        "name": "Color Constancy",
        "description": "**Color Constancy** is the ability of the human vision system to perceive the colors of the objects in the scene largely invariant to the color of the light source. The task of computational Color Constancy is to estimate the scene illumination and then perform the chromatic adaptation in order to remove the influence of the illumination color on the colors of the objects in the scene.\n\n\n<span class=\"description-source\">Source: [CroP: Color Constancy Benchmark Dataset Generator ](https://arxiv.org/abs/1903.12581)</span>",
        "type": "Task",
        "probable_wikipedia": "Color constancy",
        "score": "12.189561",
        "probable_description": " Color constancy is an example of subjective constancy and a feature of the human color perception system which ensures that the perceived color of objects remains relatively constant under varying illumination conditions. A green apple for instance looks green to us at midday, when the main illumination is white sunlight, and also at sunset, when the main illumination is red. This helps us identify objects. "
    },
    {
        "id": "METHOD_colorization",
        "name": "Colorization",
        "full_name": "Colorization",
        "description": "Colorization is a self-supervision approach that relies on colorization as the pretext task in order to learn image representations.",
        "category": [
            "Self-Supervised Learning"
        ],
        "proposed_in": {
            "paper_id": "colorful-image-colorization",
            "s2_paper_id": "8201e6e687f2de477258e9be53ba7b73ee30d7de"
        },
        "type": "Method",
        "probable_wikipedia": "Film colorization",
        "score": "3.7656262",
        "probable_description": " Film colorization (American English; or colourisation (British English), or colourization (Canadian English)) is any process that adds color to black-and-white, sepia, or other monochrome moving-picture images. It may be done as a special effect, to \"modernize\" black-and-white films, or to restore color films. The first examples date from the early 20th century, but colorization has become common with the advent of digital image processing. "
    },
    {
        "id": "TASK_combinatorial-optimization",
        "name": "Combinatorial Optimization",
        "description": "**Combinatorial Optimization** is a category of problems which requires optimizing a function over a combination of discrete objects and the solutions are constrained. Examples include finding shortest paths in a graph, maximizing value in the Knapsack problem and finding boolean settings that satisfy a set of constraints. Many of these problems are NP-Hard, which means that no polynomial time solution can be developed for them. Instead, we can only produce approximations in polynomial time that are guaranteed to be some factor worse than the true optimal solution.\n\n\n<span class=\"description-source\">Source: [Recent Advances in Neural Program Synthesis ](https://arxiv.org/abs/1802.02353)</span>",
        "type": "Task",
        "probable_wikipedia": "Combinatorial optimization",
        "score": "11.326779",
        "probable_description": " In Operations Research, applied mathematics and theoretical computer science, combinatorial optimization is a topic that consists of finding an optimal object from a finite set of objects. In many such problems, exhaustive search is not tractable. It operates on the domain of those optimization problems in which the set of feasible solutions is discrete or can be reduced to discrete, and in which the goal is to find the best solution. Some common problems involving combinatorial optimization are the travelling salesman problem (\"TSP\"), the minimum spanning tree problem (\"MST\"), and the knapsack problem.  Combinatorial optimization is a subset of mathematical optimization that is related to operations research, algorithm theory, and computational complexity theory. It has important applications in several fields, including artificial intelligence, machine learning, auction theory, and software engineering.  Some research literature considers discrete optimization to consist of integer programming together with combinatorial optimization (which in turn is composed of optimization problems dealing with graph structures) although all of these topics have closely intertwined research literature. It often involves determining the way to efficiently allocate resources used to find solutions to mathematical problems. "
    },
    {
        "id": "TASK_commonsense-rl",
        "name": "Commonsense Reasoning for RL",
        "description": "Commonsense reasoning for Reinforcement Learning agents",
        "type": "Task",
        "probable_wikipedia": "Commonsense reasoning",
        "score": "3.6302688",
        "probable_description": " Commonsense reasoning is one of the branches of artificial intelligence (AI) that is concerned with simulating the human ability to make presumptions about the type and essence of ordinary situations they encounter every day. These assumptions include judgments about the physical properties, purpose, intentions and behavior of people and objects, as well as possible outcomes of their actions and interactions. A device that exhibits commonsense reasoning will be capable of predicting results and drawing conclusions that are similar to humans' folk psychology (humans' innate ability to reason about people's behavior and intentions) and naive physics (humans' natural understanding of the physical world). "
    },
    {
        "id": "TASK_compressive-sensing",
        "name": "Compressive Sensing",
        "description": "**Compressive Sensing** is a new signal processing framework for efficiently acquiring and reconstructing a signal that have a sparse representation in a fixed linear basis.\n\n\n<span class=\"description-source\">Source: [Sparse Estimation with Generalized Beta Mixture and the Horseshoe Prior ](https://arxiv.org/abs/1411.2405)</span>",
        "type": "Task",
        "probable_wikipedia": "Compressed sensing",
        "score": "6.1358037",
        "probable_description": " Compressed sensing (also known as compressive sensing, compressive sampling, or sparse sampling) is a signal processing technique for efficiently acquiring and reconstructing a signal, by finding solutions to underdetermined linear systems. This is based on the principle that, through optimization, the sparsity of a signal can be exploited to recover it from far fewer samples than required by the Nyquist\u2013Shannon sampling theorem. There are two conditions under which recovery is possible. The first one is sparsity, which requires the signal to be sparse in some domain. The second one is incoherence, which is applied through the isometric property, which is sufficient for sparse signals. "
    },
    {
        "id": "TASK_content-based-image-retrieval",
        "name": "Content-Based Image Retrieval",
        "description": "**Content-Based Image Retrieval** is a well studied problem in computer vision, with retrieval problems generally divided into two groups: category-level retrieval and instance-level retrieval. Given a query image of the Sydney Harbour bridge, for instance, category-level retrieval aims to find any bridge in a given dataset of images, whilst instance-level retrieval must find the Sydney Harbour bridge to be considered a match.\n\n\n<span class=\"description-source\">Source: [Camera Obscurer: Generative Art for Design Inspiration ](https://arxiv.org/abs/1903.02165)</span>",
        "type": "Task",
        "probable_wikipedia": "Content-based image retrieval",
        "score": "12.47347",
        "probable_description": " Content-based image retrieval, also known as query by image content (QBIC) and content-based visual information retrieval (CBVIR), is the application of computer vision techniques to the image retrieval problem, that is, the problem of searching for digital images in large databases (see this survey for a recent scientific overview of the CBIR field). Content-based image retrieval is opposed to traditional concept-based approaches (see Concept-based image indexing).  \"Content-based\" means that the search analyzes the contents of the image rather than the metadata such as keywords, tags, or descriptions associated with the image. The term \"content\" in this context might refer to colors, shapes, textures, or any other information that can be derived from the image itself. CBIR is desirable because searches that rely purely on metadata are dependent on annotation quality and completeness.  Having humans manually annotate images by entering keywords or metadata in a large database can be time consuming and may not capture the keywords desired to describe the image. The evaluation of the effectiveness of keyword image search is subjective and has not been well-defined. In the same regard, CBIR systems have similar challenges in defining success. \"Keywords also limit the scope of queries to the set of predetermined criteria.\" and, \"having been set up\" are less reliable than using the content itself. "
    },
    {
        "id": "TASK_cross-lingual-pos-tagging",
        "name": "Cross-Lingual POS Tagging",
        "description": "",
        "type": "Task",
        "probable_wikipedia": "Part-of-speech tagging",
        "score": "0.90523356",
        "probable_description": " In corpus linguistics, part-of-speech tagging (POS tagging or PoS tagging or POST), also called grammatical tagging or word-category disambiguation, is the process of marking up a word in a text (corpus) as corresponding to a particular part of speech, based on both its definition and its context\u2014i.e., its relationship with adjacent and related words in a phrase, sentence, or paragraph. A simplified form of this is commonly taught to school-age children, in the identification of words as nouns, verbs, adjectives, adverbs, etc.  Once performed by hand, POS tagging is now done in the context of computational linguistics, using algorithms which associate discrete terms, as well as hidden parts of speech, in accordance with a set of descriptive tags. POS-tagging algorithms fall into two distinctive groups: rule-based and stochastic. E. Brill's tagger, one of the first and most widely used English POS-taggers, employs rule-based algorithms. "
    },
    {
        "id": "TASK_crowd-counting",
        "name": "Crowd Counting",
        "description": "**Crowd Counting** is a task to count people in image. It is mainly used in real-life for automated public monitoring such as surveillance and traffic control. Different from object detection, Crowd Counting aims at recognizing arbitrarily sized targets in various situations including sparse and cluttering scenes at the same time.\n\n\n<span class=\"description-source\">Source: [Deep Density-aware Count Regressor ](https://arxiv.org/abs/1908.03314)</span>",
        "type": "Task",
        "probable_wikipedia": "Crowd counting",
        "score": "12.315275",
        "probable_description": " Crowd counting or crowd estimating is a technique used to count or estimate the number of people in a crowd.  The most direct method is to actually count each person in the crowd. For example, turnstiles are often used to precisely count the number of people entering an event.  At events in streets or a park rather than an enclosed venue, crowd counting is more difficult and less precise. For many events, especially political rallies or protests, the number of people in a crowd carries political significance and count results are controversial. For example, the global protests against the Iraq war had many protests with widely differing counts offered by organizers on one side and the police on the other side. Another memorable incident occurred when Louis Farrakhan threatened to sue the Washington, D.C. Park Police for announcing that only 400,000 people attended the 1995 Million Man March he organized. The National Park Police still estimates crowd size for its own planning purposes, but does not publicly reveal the figures. "
    },
    {
        "id": "TASK_cryptanalysis",
        "name": "Cryptanalysis",
        "description": "",
        "type": "Task",
        "probable_wikipedia": "Cryptanalysis",
        "score": "12.128314",
        "probable_description": " Cryptanalysis (from the Greek \"kryptos\", \"hidden\", and \"analyein\", \"to loosen\" or \"to untie\") is the study of analyzing information systems in order to study the hidden aspects of the systems. Cryptanalysis is used to breach cryptographic security systems and gain access to the contents of encrypted messages, even if the cryptographic key is unknown.  In addition to mathematical analysis of cryptographic algorithms, cryptanalysis includes the study of side-channel attacks that do not target weaknesses in the cryptographic algorithms themselves, but instead exploit weaknesses in their implementation.  Even though the goal has been the same, the methods and techniques of cryptanalysis have changed drastically through the history of cryptography, adapting to increasing cryptographic complexity, ranging from the pen-and-paper methods of the past, through machines like the British Bombes and Colossus computers at Bletchley Park in World War II, to the mathematically advanced computerized schemes of the present. Methods for breaking modern cryptosystems often involve solving carefully constructed problems in pure mathematics, the best-known being integer factorization. "
    },
    {
        "id": "TASK_data-compression",
        "name": "Data Compression",
        "description": "",
        "type": "Task",
        "probable_wikipedia": "Data compression",
        "score": "11.239837",
        "probable_description": " In signal processing, data compression, source coding, or bit-rate reduction involves encoding information using fewer bits than the original representation. Compression can be either lossy or lossless. Lossless compression reduces bits by identifying and eliminating statistical redundancy. No information is lost in lossless compression. Lossy compression reduces bits by removing unnecessary or less important information.  The process of reducing the size of a data file is often referred to as data compression. In the context of data transmission, it is called source coding; encoding done at the source of the data before it is stored or transmitted. Source coding should not be confused with channel coding, for error detection and correction or line coding, the means for mapping data onto a signal.  Compression is useful because it reduces resources required to store and transmit data. Computational resources are consumed in the compression process and, usually, in the reversal of the process (decompression). Data compression is subject to a space\u2013time complexity trade-off. For instance, a compression scheme for video may require expensive hardware for the video to be decompressed fast enough to be viewed as it is being decompressed, and the option to decompress the video in full before watching it may be inconvenient or require additional storage. The design of data compression schemes involves trade-offs among various factors, including the degree of compression, the amount of distortion introduced (when using lossy data compression), and the computational resources required to compress and decompress the data. "
    },
    {
        "id": "TASK_data-visualization",
        "name": "Data Visualization",
        "description": "",
        "type": "Task",
        "probable_wikipedia": "Data visualization",
        "score": "11.759775",
        "probable_description": " Data visualization is viewed by many disciplines as a modern equivalent of visual communication. It involves the creation and study of the visual representation of data.  To communicate information clearly and efficiently, data visualization uses statistical graphics, plots, information graphics and other tools. Numerical data may be encoded using dots, lines, or bars, to visually communicate a quantitative message. Effective visualization helps users analyze and reason about data and evidence. It makes complex data more accessible, understandable and usable. Users may have particular analytical tasks, such as making comparisons or understanding causality, and the design principle of the graphic (i.e., showing comparisons or showing causality) follows the task. Tables are generally used where users will look up a specific measurement, while charts of various types are used to show patterns or relationships in the data for one or more variables.  Data visualization is both an art and a science. It is viewed as a branch of descriptive statistics by some, but also as a grounded theory development tool by others. Increased amounts of data created by Internet activity and an expanding number of sensors in the environment are referred to as \"big data\" or Internet of things. Processing, analyzing and communicating this data present ethical and analytical challenges for data visualization. The field of data science and practitioners called data scientists help address this challenge. "
    },
    {
        "id": "TASK_deblurring",
        "name": "Deblurring",
        "description": "<span style=\"color:grey; opacity: 0.6\">( Image credit: [Deblurring Face Images using Uncertainty Guided Multi-Stream Semantic Networks](https://arxiv.org/pdf/1907.13106v1.pdf) )</span>",
        "type": "Task",
        "probable_wikipedia": "Deblurring",
        "score": "11.731735",
        "probable_description": " Deblurring is the process of removing blurring artifacts from images, such as blur caused by defocus aberration or motion blur. The blur is typically modeled as the convolution of a (sometimes space- or time-varying) point spread function with a hypothetical sharp input image, where both the sharp input image (which is to be recovered) and the point spread function are unknown. This is an example of an inverse problem. In almost all cases, there is insufficient information in the blurred image to uniquely determine a plausible original image, making it an ill-posed problem. In addition the blurred image contains additional noise which complicates the task of determining the original image. This is generally solved by the use of a regularization term to attempt to eliminate implausible solutions. This problem is analogous to echo removal in the signal processing domain. Nevertheless, when coherent beam is used for imaging, the point spread function can be modeled mathematically. By proper deconvolution of the point spread function and the image, the resolution can be enhanced several times.   "
    },
    {
        "id": "TASK_decipherment",
        "name": "Decipherment",
        "description": "",
        "type": "Task",
        "probable_wikipedia": "Decipherment",
        "score": "8.878925",
        "probable_description": " In philology, decipherment is the discovery of the meaning of texts written in ancient or obscure languages or scripts. Decipherment in cryptography refers to decryption. The term is used sardonically in everyday language to describe attempts to read poor handwriting. In genetics, decipherment is the successful attempt to understand DNA, which is viewed metaphorically as a text containing word-like units. Throughout science the term decipherment is synonymous with the understanding of biological and chemical phenomena. "
    },
    {
        "id": "TASK_decision-making",
        "name": "Decision Making",
        "description": "**Decision Making** is a complex task that involves analyzing data (of different level of abstraction) from disparate sources and with different levels of certainty, merging the information by weighing in on some data source more than other, and arriving at a conclusion by exploring all possible alternatives.\n\n\n<span class=\"description-source\">Source: [Complex Events Recognition under Uncertainty in a Sensor Network ](https://arxiv.org/abs/1411.0085)</span>",
        "type": "Task",
        "probable_wikipedia": "Decision-making",
        "score": "9.196123",
        "probable_description": " In psychology, decision-making (also spelled decision making and decisionmaking) is regarded as the cognitive process resulting in the selection of a belief or a course of action among several alternative possibilities. Every decision-making process produces a final choice, which may or may not prompt action.  Decision-making is the process of identifying and choosing alternatives based on the values, preferences and beliefs of the decision-maker. "
    },
    {
        "id": "TASK_deepfake-detection",
        "name": "DeepFake Detection",
        "description": "DeepFakes involves videos, often obscene, in which a face can be swapped with someone else\u2019s using neural networks. DeepFakes are a general public concern, thus it's important to develop methods to detect them. \r\n\r\nDescription source: [DeepFakes: a New Threat to Face Recognition? Assessment and Detection](https://arxiv.org/pdf/1812.08685.pdf)\r\n\r\nImage source: [DeepFakes: a New Threat to Face Recognition? Assessment and Detection](https://paperswithcode.com/paper/deepfakes-a-new-threat-to-face-recognition)",
        "type": "Task",
        "probable_wikipedia": "Deepfake",
        "score": "1.8182676",
        "probable_description": " Deepfake (a portmanteau of \"deep learning\" and \"fake\") is a technique for human image synthesis based on artificial intelligence. It is used to combine and superimpose existing images and videos onto source images or videos using a machine learning technique known as generative adversarial network. The phrase \"deepfake\" was coined in 2017.  Because of these capabilities, deepfakes have been used to create fake celebrity pornographic videos or revenge porn. Deepfakes can also be used to create fake news and malicious hoaxes. "
    },
    {
        "id": "TASK_de-identification",
        "name": "De-identification",
        "description": "De-identification is the task of detecting privacy-related entities in text, such as person names, emails and contact data.",
        "type": "Task",
        "probable_wikipedia": "De-identification",
        "score": "12.056101",
        "probable_description": " De-identification is the process used to prevent someone's personal identity from being revealed. For example, data produced during human subject research might be de-identified to preserve privacy for research participants.  When applied to metadata or general data about identification, the process is also known as data anonymization. Common strategies include deleting or masking personal identifiers, such as personal name, and suppressing or generalizing quasi-identifiers, such as date of birth. The reverse process of using de-identified data to identify individuals is known as data re-identification. Successful re-identifications cast doubt on de-identification's effectiveness. A systematic review of fourteen distinct re-identification attacks found \"a high re-identification rate [\u2026] dominated by small-scale studies on data that was not de-identified according to existing standards.\"  De-identification is adopted as one of the main approaches of data privacy protection. It is commonly used in fields of communications, multimedia, biometrics, big data, cloud computing, data mining, internet, social networks and audio\u2013video surveillance. "
    },
    {
        "id": "TASK_demosaicking",
        "name": "Demosaicking",
        "description": "Most modern digital cameras acquire color images by measuring only one color channel per pixel, red, green, or blue, according to a specific pattern called the Bayer pattern. **Demosaicking** is the processing step that reconstruct a full color image given these incomplete measurements.\r\n\r\n\r\n<span class=\"description-source\">Source: [Revisiting Non Local Sparse Models for Image Restoration ](https://arxiv.org/abs/1912.02456)</span>",
        "type": "Task",
        "probable_wikipedia": "Demosaicing",
        "score": "11.682666",
        "probable_description": " A demosaicing (also de-mosaicing, demosaicking or debayering) algorithm is a digital image process used to reconstruct a full color image from the incomplete color samples output from an image sensor overlaid with a color filter array (CFA). It is also known as \"CFA interpolation\" or \"color reconstruction\".  Most modern digital cameras acquire images using a single image sensor overlaid with a CFA, so demosaicing is part of the processing pipeline required to render these images into a viewable format.  Many modern digital cameras can save images in a raw format allowing the user to demosaic them using software, rather than using the camera's built-in firmware. "
    },
    {
        "id": "TASK_density-estimation",
        "name": "Density Estimation",
        "description": "The goal of **Density Estimation** is to give an accurate description of the underlying probabilistic density distribution of an observable data set with unknown density.\n\n\n<span class=\"description-source\">Source: [Contrastive Predictive Coding Based Feature for Automatic Speaker Verification ](https://arxiv.org/abs/1904.01575)</span>",
        "type": "Task",
        "probable_wikipedia": "Density estimation",
        "score": "12.031157",
        "probable_description": " In probability and statistics, density estimation is the construction of an estimate, based on observed data, of an unobservable underlying probability density function. The unobservable density function is thought of as the density according to which a large population is distributed; the data are usually thought of as a random sample from that population.  A variety of approaches to density estimation are used, including Parzen windows and a range of data clustering techniques, including vector quantization. The most basic form of density estimation is a rescaled histogram. "
    },
    {
        "id": "TASK_diabetic-retinopathy-grading",
        "name": "Diabetic Retinopathy Grading",
        "description": "Grading the severity of diabetic retinopathy from (ophthalmic) fundus images",
        "type": "Task",
        "probable_wikipedia": "Diabetic retinopathy",
        "score": "3.4058905",
        "probable_description": " Diabetic retinopathy, also known as diabetic eye disease, is a medical condition in which damage occurs to the retina due to diabetes mellitus. It is a leading cause of blindness.  Diabetic retinopathy affects up to 80 percent of those who have had diabetes for 20 years or more. At least 90% of new cases could be reduced with proper treatment and monitoring of the eyes. The longer a person has diabetes, the higher his or her chances of developing diabetic retinopathy. Each year in the United States, diabetic retinopathy accounts for 12% of all new cases of blindness. It is also the leading cause of blindness in people aged 20 to 64. "
    },
    {
        "id": "TASK_dialog-act-classification",
        "name": "Dialog Act Classification",
        "description": "",
        "type": "Task",
        "probable_wikipedia": "Dialog act",
        "score": "3.2860186",
        "probable_description": " In linguistics and in particular in natural language understanding, a dialog act is an utterance, in the context of a conversational dialog, that serves a function in the dialog. Types of dialog acts include a question, a statement, or a request for action. Dialog acts are a type of speech act.  Dialog act recognition, also known as spoken utterance classification, is an important part of spoken language understanding. AI inference models or statistical models are used to recognize and classify dialog acts.  A dialog system typically includes a taxonomy of dialog types or \"tags\" that classify the different functions dialog acts can play. One study had 42 types of dialog act in their taxonomy. Examples of types in this study include \"STATEMENT\", \"OPINION\", \"AGREEMENT/ACCEPT\", and \"YES-NO-QUESTION\".  The research on dialog acts have increased since 1999, after spoken dialog systems became commercial reality.  "
    },
    {
        "id": "TASK_dialogue-interpretation",
        "name": "Dialogue Interpretation",
        "description": "Interpreting the meaning of a dialog.",
        "type": "Task",
        "probable_wikipedia": "Dialogue",
        "score": "0.6884785",
        "probable_description": " Dialogue (sometimes spelled dialog in American English) is a written or spoken conversational exchange between two or more people, and a literary and theatrical form that depicts such an exchange. As a narrative, philosophical or didactic device, it is chiefly associated in the West with the Socratic dialogue as developed by Plato, but antecedents are also found in other traditions including Indian literature.  In the 20th century, philosophical treatments of dialogue emerged from thinkers including Mikhail Bakhtin, Paulo Freire, Martin Buber, and David Bohm. Although diverging in many details, these thinkers have articulated a holistic concept of dialogue as a multi-dimensional, dynamic and context-dependent process of creating meaning. Educators such as Freire and Ramon Flecha have also developed a body of theory and techniques for using egalitarian dialogue as a pedagogical tool. "
    },
    {
        "id": "TASK_dimensionality-reduction",
        "name": "Dimensionality Reduction",
        "description": "Dimensionality reduction is the task of reducing the dimensionality of a dataset.\r\n\r\n<span style=\"color:grey; opacity: 0.6\">( Image credit: [openTSNE](https://github.com/pavlin-policar/openTSNE) )</span>",
        "type": "Task",
        "probable_wikipedia": "Dimensionality reduction",
        "score": "11.701829",
        "probable_description": " In statistics, machine learning, and information theory, dimensionality reduction or dimension reduction is the process of reducing the number of random variables under consideration by obtaining a set of principal variables. It can be divided into feature selection and feature extraction. "
    },
    {
        "id": "TASK_direction-of-arrival-estimation",
        "name": "Direction of Arrival Estimation",
        "description": "Estimating the direction-of-arrival (DOA) of a sound source from multi-channel recordings.",
        "type": "Task",
        "probable_wikipedia": "Direction of arrival",
        "score": "2.3504782",
        "probable_description": " In signal processing, direction of arrival (DOA) denotes the direction from which usually a propagating wave arrives at a point, where usually a set of sensors are located. These set of sensors forms what is called a sensor array. Often there is the associated technique of beamforming which is estimating the signal from a given direction. Various engineering problems addressed in the associated literature are:   "
    },
    {
        "id": "TASK_disaster-response",
        "name": "Disaster Response",
        "description": "",
        "type": "Task",
        "probable_wikipedia": "Disaster response",
        "score": "11.216811",
        "probable_description": " Disaster response is the second phase of the disaster management cycle. It consists of a number of elements, for example; warning/evacuation, search and rescue, providing immediate assistance, assessing damage, continuing assistance and the immediate restoration or construction of infrastructure (i.e. provisional storm drains or diversion dams).The aim of emergency response is to provide immediate assistance to maintain life, improve health and support the morale of the affected population. Such assistance may range from providing specific but limited aid, such as assisting refugees with transport, temporary shelter, and food, to establishing semi-permanent settlement in camps and other locations. It also may involve initial repairs to damaged or diversion to infrastructure.  The focus in the response phase is on putting people safe, prevent need disasters and meeting the basic needs of the people until more permanent and sustainable solutions can be found. The main responsibility to address these needs and respond to a disaster lies with the government or governments in whose territory the disaster has occurred. In addition, Humanitarian organizations are often strongly present in this phase of the disaster management cycle, particularly in countries where the government lacks the resources to respond adequately to the needs. "
    },
    {
        "id": "TASK_distributed-computing",
        "name": "Distributed Computing",
        "description": "",
        "type": "Task",
        "probable_wikipedia": "Distributed computing",
        "score": "11.358127",
        "probable_description": " Distributed computing is a field of computer science that studies distributed systems. A \"distributed system\" is a system whose components are located on different networked computers, which communicate and coordinate their actions by passing messages to one another. The components interact with one another in order to achieve a common goal. Three significant characteristics of distributed systems are: concurrency of components, lack of a global clock, and independent failure of components. Examples of distributed systems vary from SOA-based systems to massively multiplayer online games to peer-to-peer applications.  A computer program that runs within a distributed system is called a distributed program (and distributed programming is the process of writing such programs). There are many different types of implementations for the message passing mechanism, including pure HTTP, RPC-like connectors and message queues.  \"Distributed computing\" also refers to the use of distributed systems to solve computational problems. In \"distributed computing\", a problem is divided into many tasks, each of which is solved by one or more computers, which communicate with each other via message passing. "
    },
    {
        "id": "TASK_document-classification",
        "name": "Document Classification",
        "description": "**Document Classification** is a procedure of assigning one or more labels to a document from a predetermined set of labels.\n\n\n<span class=\"description-source\">Source: [Long-length Legal Document Classification ](https://arxiv.org/abs/1912.06905)</span>",
        "type": "Task",
        "probable_wikipedia": "Document classification",
        "score": "10.811133",
        "probable_description": " Document classification or document categorization is a problem in library science, information science and computer science. The task is to assign a document to one or more classes or categories. This may be done \"manually\" (or \"intellectually\") or algorithmically. The intellectual classification of documents has mostly been the province of library science, while the algorithmic classification of documents is mainly in information science and computer science. The problems are overlapping, however, and there is therefore interdisciplinary research on document classification.  The documents to be classified may be texts, images, music, etc. Each kind of document possesses its special classification problems. When not otherwise specified, text classification is implied.  Documents may be classified according to their subjects or according to other attributes (such as document type, author, printing year etc.). In the rest of this article only subject classification is considered. There are two main philosophies of subject classification of documents: the content-based approach and the request-based approach. "
    },
    {
        "id": "TASK_document-layout-analysis",
        "name": "Document Layout Analysis",
        "description": "\"Document Layout Analysis is performed to determine physical structure of a document, that is, to determine document components. These document components can consist of single connected components-regions [...] of\r\npixels that are adjacent to form single regions [...] , or group\r\nof text lines. A text line is a group of characters, symbols,\r\nand words that are adjacent, \u201crelatively close\u201d to each other\r\nand through which a straight line can be drawn (usually with\r\nhorizontal or vertical orientation).\"  L. O'Gorman, \"The document spectrum for page layout analysis,\" in IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 15, no. 11, pp. 1162-1173, Nov. 1993.",
        "type": "Task",
        "probable_wikipedia": "Document layout analysis",
        "score": "9.879239",
        "probable_description": " In computer vision, document layout analysis is the process of identifying and categorizing the regions of interest in the scanned image of a text document. A reading system requires the segmentation of text zones from non-textual ones and the arrangement in their correct reading order. Detection and labeling of the different zones (or blocks) as text body, illustrations, math symbols, and tables embedded in a document is called geometric layout analysis. But text zones play different logical roles inside the document (titles, captions, footnotes, etc.) and this kind of semantic labeling is the scope of the logical layout analysis.  Document layout analysis is the union of geometric and logical labeling. It is typically performed before a document image is sent to an OCR engine, but it can be used also to detect duplicate copies of the same document in large archives, or to index documents by their structure or pictorial content.  Document layout is formally defined in the international standard ISO 8613-1:1989. "
    },
    {
        "id": "TASK_document-summarization",
        "name": "Document Summarization",
        "description": "Automatic **Document Summarization** is the task of rewriting a document into its shorter form while still retaining its important content. The most popular two paradigms are extractive approaches and abstractive approaches. Extractive approaches generate summaries by extracting parts of the original document (usually sentences), while abstractive methods may generate new words or phrases which are not in the original document.\n\n\n<span class=\"description-source\">Source: [HIBERT: Document Level Pre-training of Hierarchical Bidirectional Transformers for Document Summarization ](https://arxiv.org/abs/1905.06566)</span>",
        "type": "Task",
        "probable_wikipedia": "Automatic summarization",
        "score": "2.326575",
        "probable_description": " Automatic summarization is the process of shortening a text document with software, in order to create a summary with the major points of the original document. Technologies that can make a coherent summary take into account variables such as length, writing style and syntax.  Automatic data summarization is part of machine learning and data mining. The main idea of summarization is to find a subset of data which contains the \"information\" of the entire set. Such techniques are widely used in industry today. Search engines are an example; others include summarization of documents, image collections and videos. Document summarization tries to create a representative summary or abstract of the entire document, by finding the most informative sentences, while in image summarization the system finds the most representative and important (i.e. salient) images. For surveillance videos, one might want to extract the important events from the uneventful context.  There are two general approaches to automatic summarization: extraction and abstraction. Extractive methods work by selecting a subset of existing words, phrases, or sentences in the original text to form the summary. In contrast, abstractive methods build an internal semantic representation and then use natural language generation techniques to create a summary that is closer to what a human might express. Such a summary might include verbal innovations. Research to date has focused primarily on extractive methods, which are appropriate for image collection summarization and video summarization. "
    },
    {
        "id": "TASK_domain-adaptation",
        "name": "Domain Adaptation",
        "description": "Domain adaptation is the task of adapting models across domains. This is motivated by the challenge where the test and training datasets fall from different data distributions due to some factor. Domain adaptation aims to build machine learning models that can be generalized into a target domain and dealing with the discrepancy across domain distributions. \r\n\r\nFurther readings:\r\n\r\n- [A Brief Review of Domain Adaptation](https://paperswithcode.com/paper/a-brief-review-of-domain-adaptation)\r\n\r\n<span style=\"color:grey; opacity: 0.6\">( Image credit: [Unsupervised Image-to-Image Translation Networks](https://arxiv.org/pdf/1703.00848v6.pdf) )</span>",
        "type": "Task",
        "probable_wikipedia": "Domain adaptation",
        "score": "10.752861",
        "probable_description": " Domain adaptation is a field associated with machine learning and transfer learning. This scenario arises when we aim at learning from a source data distribution a well performing model on a different (but related) target data distribution. For instance, one of the tasks of the common spam filtering problem consists in adapting a model from one user (the source distribution) to a new one who receives significantly different emails (the target distribution). Domain adaptation has also been shown to be beneficial for learning unrelated sources. Note that, when more than one source distribution is available the problem is referred to as multi-source domain adaptation. "
    },
    {
        "id": "TASK_dota-2",
        "name": "Dota 2",
        "description": "Dota 2 is a multiplayer online battle arena (MOBA). The task is to train one-or-more agents to play and win the game.\r\n\r\n<span style=\"color:grey; opacity: 0.6\">( Image credit: [OpenAI Five](https://openai.com/five/) )</span>",
        "type": "Task",
        "probable_wikipedia": "Dota 2",
        "score": "12.185472",
        "probable_description": " Dota 2 is a multiplayer online battle arena (MOBA) video game developed and published by Valve Corporation. The game is a sequel to \"Defense of the Ancients\" (\"DotA\"), which was a community-created mod for Blizzard Entertainment's \"\" and its expansion pack, \"The Frozen Throne\". \"Dota 2\" is played in matches between two teams of five players, with each team occupying and defending their own separate base on the map. Each of the ten players independently controls a powerful character, known as a \"hero\", who all have unique abilities and differing styles of play. During a match, players collect experience points and items for their heroes to successfully defeat the opposing team's heroes in player versus player combat. A team wins by being the first to destroy the other team's \"Ancient\", a large structure located within their base.  Development of \"Dota 2\" began in 2009 when IceFrog, lead designer of the original \"Defense of the Ancients\" mod, was hired by Valve to create a modernized remake for them in the Source game engine. It was officially released for Microsoft Windows, OS X, and Linux-based personal computers via the digital distribution platform Steam in July 2013, following a Windows-only open beta phase that began two years prior. As the game is fully free-to-play with no heroes needing to be bought or otherwise unlocked, revenue is instead made from microtransactions such as loot boxes, and a battle pass subscription system called Dota Plus, which all only offer non-gameplay altering virtual goods in return, such as hero cosmetics"
    },
    {
        "id": "TASK_driver-attention-monitoring",
        "name": "Driver Attention Monitoring",
        "description": "Driver attention monitoring is the task of monitoring the attention of a driver.\r\n\r\n<span style=\"color:grey; opacity: 0.6\">( Image credit: [Predicting Driver Attention in Critical Situations](https://arxiv.org/pdf/1711.06406v3.pdf) )</span>",
        "type": "Task",
        "probable_wikipedia": "Driver Monitoring System",
        "score": "3.6790903",
        "probable_description": " The Driver Monitoring System, also known as Driver Attention Monitor, is a vehicle safety system first introduced by Toyota in 2006 for its and Lexus latest models. It was first offered in Japan on the GS 450h. The system's functions co-operate with the Pre-Collision System (PCS). The system uses infrared sensors to monitor driver attentiveness. Specifically, the Driver Monitoring System includes a CCD camera placed on the steering column which is capable of eye tracking, via infrared LED detectors. If the driver is not paying attention to the road ahead and a dangerous situation is detected, the system will warn the driver by flashing lights, warning sounds. If no action is taken, the vehicle will apply the brakes (a warning alarm will sound followed by a brief automatic application of the braking system). This system is said to be the first of its kind.  In 2008, the Toyota Crown system went further and can detect if the driver is becoming sleepy by monitoring the eyelids. "
    },
    {
        "id": "TASK_drone-based-object-tracking",
        "name": "drone-based object tracking",
        "description": "drone-based object tracking",
        "type": "Task",
        "probable_wikipedia": "Unmanned aerial vehicle",
        "score": "2.689725",
        "probable_description": " An unmanned aerial vehicle (UAV) (or uncrewed aerial vehicle, commonly known as a drone) is an aircraft without a human pilot on board and a type of unmanned vehicle. UAVs are a component of an unmanned aircraft system (UAS); which include a UAV, a ground-based controller, and a system of communications between the two. The flight of UAVs may operate with various degrees of autonomy: either under remote control by a human operator or autonomously by onboard computers.<ref name=\"ICAO's circular 328 AN/190\"></ref>  Compared to crewed aircraft, UAVs were originally used for missions too \"dull, dirty or dangerous\" for humans. While they originated mostly in military applications, their use is rapidly expanding to commercial, scientific, recreational, agricultural, and other applications, such as policing and surveillance, product deliveries, aerial photography, smuggling, and drone racing. Civilian UAVs now vastly outnumber military UAVs, with estimates of over a million sold by 2015. "
    },
    {
        "id": "TASK_drone-controller",
        "name": "Drone Controller",
        "description": "",
        "type": "Task",
        "probable_wikipedia": "Unmanned aerial vehicle",
        "score": "2.3582242",
        "probable_description": " An unmanned aerial vehicle (UAV) (or uncrewed aerial vehicle, commonly known as a drone) is an aircraft without a human pilot on board and a type of unmanned vehicle. UAVs are a component of an unmanned aircraft system (UAS); which include a UAV, a ground-based controller, and a system of communications between the two. The flight of UAVs may operate with various degrees of autonomy: either under remote control by a human operator or autonomously by onboard computers.<ref name=\"ICAO's circular 328 AN/190\"></ref>  Compared to crewed aircraft, UAVs were originally used for missions too \"dull, dirty or dangerous\" for humans. While they originated mostly in military applications, their use is rapidly expanding to commercial, scientific, recreational, agricultural, and other applications, such as policing and surveillance, product deliveries, aerial photography, smuggling, and drone racing. Civilian UAVs now vastly outnumber military UAVs, with estimates of over a million sold by 2015. "
    },
    {
        "id": "TASK_drug-discovery",
        "name": "Drug Discovery",
        "description": "Drug discovery is the task of applying machine learning to discover new candidate drugs.\r\n\r\n<span style=\"color:grey; opacity: 0.6\">( Image credit: [A Turing Test for Molecular Generators](https://pubs.acs.org/doi/10.1021/acs.jmedchem.0c01148) )</span>",
        "type": "Task",
        "probable_wikipedia": "Drug discovery",
        "score": "11.388343",
        "probable_description": " In the fields of medicine, biotechnology and pharmacology, drug discovery is the process by which new candidate medications are discovered.  Historically, drugs were discovered by identifying the active ingredient from traditional remedies or by serendipitous discovery, as with penicillin. More recently, chemical libraries of synthetic small molecules, natural products or extracts were screened in intact cells or whole organisms to identify substances that had a desirable therapeutic effect in a process known as classical pharmacology. After sequencing of the human genome allowed rapid cloning and synthesis of large quantities of purified proteins, it has become common practice to use high throughput screening of large compounds libraries against isolated biological targets which are hypothesized to be disease-modifying in a process known as reverse pharmacology. Hits from these screens are then tested in cells and then in animals for efficacy.  Modern drug discovery involves the identification of screening hits, medicinal chemistry and optimization of those hits to increase the affinity, selectivity (to reduce the potential of side effects), efficacy/potency, metabolic stability (to increase the half-life), and oral bioavailability. Once a compound that fulfills all of these requirements has been identified, the process of drug development can continue, and, if successful, clinical trials. One or more of these steps may, but not necessarily, involve computer-aided drug design.  Modern drug discovery is thus usually a capital-intensive process that involves large investments by pharmaceutical industry corporations as well as national governments (who provide grants and loan guarantees). Despite advances in technology and understanding of biological systems, drug"
    },
    {
        "id": "METHOD_dtw",
        "name": "DTW",
        "full_name": "Dynamic Time Warping",
        "description": "Dynamic Time Warping (DTW) [1] is one of well-known distance measures between a pairwise of time series. The main idea of DTW is to compute the distance from the matching of similar elements between time series. It uses the dynamic programming technique to find the optimal temporal matching between elements of two time series.\nFor instance, similarities in walking could be detected using DTW, even if one person was walking faster than the other, or if there were accelerations and decelerations during the course of an observation. DTW has been applied to temporal sequences of video, audio, and graphics data \u2014 indeed, any data that can be turned into a linear sequence can be analyzed with DTW. A well known application has been automatic speech recognition, to cope with different speaking speeds. Other applications include speaker recognition and online signature recognition. It can also be used in partial shape matching application.\nIn general, DTW is a method that calculates an optimal match between two given sequences (e.g. time series) with certain restriction and rules:\n\nEvery index from the first sequence must be matched with one or more indices from the other sequence, and vice versa\nThe first index from the first sequence must be matched with the first index from the other sequence (but it does not have to be its only match)\nThe last index from the first sequence must be matched with the last index from the other sequence (but it does not have to be its only match)\nThe mapping of the indices from the first sequence to indices from the other sequence must be monotonically increasing, and vice versa, i.e. if j>i  are indices from the first sequence, then there must not be two indices l>k in the other sequence, such that index i is matched with index l and index j is matched with index k, and vice versa.\n\n[1] Sakoe, Hiroaki, and Seibi Chiba. \"Dynamic programming algorithm optimization for spoken word recognition.\" IEEE transactions on acoustics, speech, and signal processing 26, no. 1 (1978): 43-49.",
        "paper": null,
        "category": [
            "Time Series Analysis"
        ],
        "type": "Method",
        "probable_wikipedia": "Dynamic time warping",
        "score": "11.680642",
        "probable_description": " In time series analysis, dynamic time warping (DTW) is one of the algorithms for measuring similarity between two temporal sequences, which may vary in speed. For instance, similarities in walking could be detected using DTW, even if one person was walking faster than the other, or if there were accelerations and decelerations during the course of an observation. DTW has been applied to temporal sequences of video, audio, and graphics data \u2014 indeed, any data that can be turned into a linear sequence can be analyzed with DTW. A well known application has been automatic speech recognition, to cope with different speaking speeds. Other applications include speaker recognition and online signature recognition. It can also be used in partial shape matching application.  In general, DTW is a method that calculates an optimal match between two given sequences (e.g. time series) with certain restriction and rules:  The optimal match is denoted by the match that satisfies all the restrictions and the rules and that has the minimal cost, where the cost is computed as the sum of absolute differences, for each matched pair of indices, between their values.  The sequences are \"warped\" non-linearly in the time dimension to determine a measure of their similarity independent of certain non-linear variations in the time dimension. This sequence alignment method is often used in time series classification. Although DTW measures a distance-like quantity between two given sequences, it doesn't guarantee the triangle inequality to hold.  In addition to a similarity measure between the two sequences,"
    },
    {
        "id": "TASK_edge-computing",
        "name": "Edge-computing",
        "description": "Deep Learning on EDGE devices",
        "type": "Task",
        "probable_wikipedia": "Edge computing",
        "score": "7.046608",
        "probable_description": " Edge computing is a distributed computing paradigm which brings computation and data storage closer to the location where it is needed, to improve response times and save bandwidth. "
    },
    {
        "id": "TASK_edge-detection",
        "name": "Edge Detection",
        "description": "**Edge Detection** is a fundamental image processing technique which involves computing an image gradient to quantify the magnitude and direction of edges in an image. Image gradients are used in various downstream tasks in computer vision such as line detection, feature detection, and image classification.\r\n\r\n\r\n<span class=\"description-source\">Source: [Artistic Enhancement and Style Transfer of Image Edges using Directional Pseudo-coloring ](https://arxiv.org/abs/1906.07981)</span>\r\n\r\n( Image credit: [Kornia](https://github.com/kornia/kornia) )",
        "type": "Task",
        "probable_wikipedia": "Edge detection",
        "score": "12.012103",
        "probable_description": " Edge detection includes a variety of mathematical methods that aim at identifying points in a digital image at which the image brightness changes sharply or, more formally, has discontinuities. The points at which image brightness changes sharply are typically organized into a set of curved line segments termed \"edges\". The same problem of finding discontinuities in one-dimensional signals is known as step detection and the problem of finding signal discontinuities over time is known as change detection. Edge detection is a fundamental tool in image processing, machine vision and computer vision, particularly in the areas of feature detection and feature extraction. "
    },
    {
        "id": "TASK_eeg",
        "name": "EEG",
        "description": "Electroencephalogram (EEG) is a method of recording brain activity using electrophysiological indexes. When the brain is active, a large number of postsynaptic potentials generated synchronously by neurons are formed after summation. It records the changes of electric waves during brain activity and is the overall reflection of the electrophysiological activities of brain nerve cells on the surface of cerebral cortex or scalp. [1] \r\nBrain waves originate from the postsynaptic potential of the apical dendrites of pyramidal cells. The formation of synchronous rhythm of EEG is also related to the activity of nonspecific projection system of cortex and thalamus. EEG is the basic theoretical research of brain science. EEG monitoring is widely used in its clinical application.",
        "type": "Task",
        "probable_wikipedia": "Electroencephalography",
        "score": "10.9563265",
        "probable_description": " Electroencephalography (EEG) is an electrophysiological monitoring method to record electrical activity of the brain. It is typically noninvasive, with the electrodes placed along the scalp, although invasive electrodes are sometimes used, as in electrocorticography. EEG measures voltage fluctuations resulting from ionic current within the neurons of the brain. Clinically, EEG refers to the recording of the brain's spontaneous electrical activity over a period of time, as recorded from multiple electrodes placed on the scalp. Diagnostic applications generally focus either on event-related potentials or on the spectral content of EEG. The former investigates potential fluctuations time locked to an event, such as 'stimulus onset' or 'button press'. The latter analyses the type of neural oscillations (popularly called \"brain waves\") that can be observed in EEG signals in the frequency domain.  EEG is most often used to diagnose epilepsy, which causes abnormalities in EEG readings. It is also used to diagnose sleep disorders, depth of anesthesia, coma, encephalopathies, and brain death. EEG used to be a first-line method of diagnosis for tumors, stroke and other focal brain disorders, but this use has decreased with the advent of high-resolution anatomical imaging techniques such as magnetic resonance imaging (MRI) and computed tomography (CT). Despite limited spatial resolution, EEG continues to be a valuable tool for research and diagnosis. It is one of the few mobile techniques available and offers millisecond-range temporal resolution which is not possible with CT, PET or MRI.  Derivatives of the EEG technique include evoked potentials (EP), which involves averaging the EEG activity time-locked"
    },
    {
        "id": "TASK_electrocardiography-ecg",
        "name": "Electrocardiography (ECG)",
        "description": "",
        "type": "Task",
        "probable_wikipedia": "Electrocardiography",
        "score": "10.507699",
        "probable_description": " Electrocardiography is the process of producing an electrocardiogram (ECG or EKG), a recording \u2013 a graph of voltage versus time \u2013 of the electrical activity of the heart using electrodes placed on the skin. These electrodes detect the small electrical changes that are a consequence of cardiac muscle depolarization followed by repolarization during each cardiac cycle (heartbeat). Changes in the normal ECG pattern occur in numerous cardiac abnormalities, including cardiac rhythm disturbances (such as atrial fibrillation and ventricular tachycardia), inadequate coronary artery blood flow (such as myocardial ischemia and myocardial infarction), and electrolyte disturbances (such as hypokalemia and hyperkalemia).  In a conventional 12-lead ECG, ten electrodes are placed on the patient's limbs and on the surface of the chest. The overall magnitude of the heart's electrical potential is then measured from twelve different angles (\"leads\") and is recorded over a period of time (usually ten seconds). In this way, the overall magnitude and direction of the heart's electrical depolarization is captured at each moment throughout the cardiac cycle.  There are three main components to an ECG: the P wave, which represents the depolarization of the atria; the QRS complex, which represents the depolarization of the ventricles; and the T wave, which represents the repolarization of the ventricles.  During each heartbeat, a healthy heart has an orderly progression of depolarization that starts with pacemaker cells in the sinoatrial node, spreads throughout the atrium, passes through the atrioventricular node down into the bundle of His and into the Purkinje fibers, spreading down and to"
    },
    {
        "id": "TASK_electromyography-emg",
        "name": "Electromyography (EMG)",
        "description": "",
        "type": "Task",
        "probable_wikipedia": "Electromyography",
        "score": "12.483639",
        "probable_description": " Electromyography (EMG) is an electrodiagnostic medicine technique for evaluating and recording the electrical activity produced by skeletal muscles. EMG is performed using an instrument called an electromyograph to produce a record called an electromyogram. An electromyograph detects the electric potential generated by muscle cells when these cells are electrically or neurologically activated. The signals can be analyzed to detect medical abnormalities, activation level, or recruitment order, or to analyze the biomechanics of human or animal movement. "
    },
    {
        "id": "TASK_electron-tomography",
        "name": "Electron Tomography",
        "description": "",
        "type": "Task",
        "probable_wikipedia": "Electron tomography",
        "score": "10.6644945",
        "probable_description": " Electron tomography (ET) is a tomography technique for obtaining detailed 3D structures of sub-cellular macro-molecular objects. Electron tomography is an extension of traditional transmission electron microscopy and uses a transmission electron microscope to collect the data. In the process, a beam of electrons is passed through the sample at incremental degrees of rotation around the center of the target sample. This information is collected and used to assemble a three-dimensional image of the target. For biological applications, the typical resolution of ET systems are in the 5\u201320 nm range, suitable for examining supra-molecular multi-protein structures, although not the secondary and tertiary structure of an individual protein or polypeptide. "
    },
    {
        "id": "TASK_emotion-classification",
        "name": "Emotion Classification",
        "description": "Given an input, classify it as 'neutral or no emotion' or as one, or more, of several given emotions that best represent the mental state of the writer.",
        "type": "Task",
        "probable_wikipedia": "Emotion classification",
        "score": "11.094978",
        "probable_description": " Emotion classification, the means by which one may distinguish one emotion from another, is a contested issue in emotion research and in affective science. Researchers have approached the classification of emotions from one of two fundamental viewpoints:  "
    },
    {
        "id": "TASK_emotion-recognition",
        "name": "Emotion Recognition",
        "description": "**Emotion Recognition** is an important area of research to enable effective human-computer interaction. Human emotions can be detected using speech signal, facial expressions, body language, and electroencephalography (EEG). <span class=\"description-source\">Source: [Using Deep Autoencoders for Facial Expression Recognition ](https://arxiv.org/abs/1801.08329)</span>",
        "type": "Task",
        "probable_wikipedia": "Emotion recognition",
        "score": "11.650193",
        "probable_description": " Emotion recognition is the process of identifying human emotion, most typically from facial expressions as well as from verbal expressions. This is both something that humans do automatically but computational methodologies have also been developed. "
    },
    {
        "id": "TASK_emotion-recognition-in-context",
        "name": "Emotion Recognition in Context",
        "description": "",
        "type": "Task",
        "probable_wikipedia": "Emotion recognition",
        "score": "1.4123331",
        "probable_description": " Emotion recognition is the process of identifying human emotion, most typically from facial expressions as well as from verbal expressions. This is both something that humans do automatically but computational methodologies have also been developed. "
    },
    {
        "id": "TASK_ensemble-learning",
        "name": "Ensemble Learning",
        "description": "",
        "type": "Task",
        "probable_wikipedia": "Ensemble learning",
        "score": "11.149984",
        "probable_description": " In statistics and machine learning, ensemble methods use multiple learning algorithms to obtain better predictive performance than could be obtained from any of the constituent learning algorithms alone. Unlike a statistical ensemble in statistical mechanics, which is usually infinite, a machine learning ensemble consists of only a concrete finite set of alternative models, but typically allows for much more flexible structure to exist among those alternatives. "
    },
    {
        "id": "TASK_entity-disambiguation",
        "name": "Entity Disambiguation",
        "description": "**Entity Disambiguation** is the task of linking mentions of ambiguous entities to their referent entities in a knowledge base such as Wikipedia.\n\n\n<span class=\"description-source\">Source: [Leveraging Deep Neural Networks and Knowledge Graphs for Entity Disambiguation ](https://arxiv.org/abs/1504.07678)</span>",
        "type": "Task",
        "probable_wikipedia": "Entity linking",
        "score": "0.8318225",
        "probable_description": " In natural language processing, entity linking, also referred to as named entity linking (NEL), named entity disambiguation (NED), named entity recognition and disambiguation (NERD) or named entity normalization (NEN) is the task of assigning a unique identity to entities (such as famous individuals, locations, or companies) mentioned in text. For example, given the sentence \"\"Paris is the capital of France\"\", the idea is to determine that \"\"Paris\"\" refers to the city of Paris and not to Paris Hilton or any other entity that could be referred to as \"\"Paris\"\". Entity linking is different from named entity recognition (NER) in that NER identifies the occurrence of a named entity in text but it does not identify which specific entity it is (see Differences from other techniques). "
    },
    {
        "id": "TASK_entity-linking",
        "name": "Entity Linking",
        "description": "Assigning a unique identity to entities (such as famous individuals, locations, or companies) mentioned in text (Source: Wikipedia).",
        "type": "Task",
        "probable_wikipedia": "Entity linking",
        "score": "11.822739",
        "probable_description": " In natural language processing, entity linking, also referred to as named entity linking (NEL), named entity disambiguation (NED), named entity recognition and disambiguation (NERD) or named entity normalization (NEN) is the task of assigning a unique identity to entities (such as famous individuals, locations, or companies) mentioned in text. For example, given the sentence \"\"Paris is the capital of France\"\", the idea is to determine that \"\"Paris\"\" refers to the city of Paris and not to Paris Hilton or any other entity that could be referred to as \"\"Paris\"\". Entity linking is different from named entity recognition (NER) in that NER identifies the occurrence of a named entity in text but it does not identify which specific entity it is (see Differences from other techniques). "
    },
    {
        "id": "TASK_epidemiology",
        "name": "Epidemiology",
        "description": "**Epidemiology** is a scientific discipline that provides reliable knowledge for clinical medicine focusing on prevention, diagnosis and treatment of diseases. Research in Epidemiology aims at characterizing risk factors for the outbreak of diseases and at evaluating the efficiency of certain treatment strategies, e.g., to compare a new treatment with an established gold standard. This research is strongly hypothesis-driven and statistical analysis is the major tool for epidemiologists so far. Correlations between genetic factors, environmental factors, life style-related parameters, age and diseases are analyzed.\n\n\n<span class=\"description-source\">Source: [Visual Analytics of Image-Centric Cohort Studies in Epidemiology ](https://arxiv.org/abs/1501.04009)</span>",
        "type": "Task",
        "probable_wikipedia": "Epidemiology",
        "score": "12.235647",
        "probable_description": " Epidemiology is the study and analysis of the distribution (who, when, and where), patterns and determinants of health and disease conditions in defined populations.  It is the cornerstone of public health, and shapes policy decisions and evidence-based practice by identifying risk factors for disease and targets for preventive healthcare. Epidemiologists help with study design, collection, and statistical analysis of data, amend interpretation and dissemination of results (including peer review and occasional systematic review). Epidemiology has helped develop methodology used in clinical research, public health studies, and, to a lesser extent, basic research in the biological sciences.  Major areas of epidemiological study include disease causation, transmission, outbreak investigation, disease surveillance, environmental epidemiology, forensic epidemiology, occupational epidemiology, screening, biomonitoring, and comparisons of treatment effects such as in clinical trials. Epidemiologists rely on other scientific disciplines like biology to better understand disease processes, statistics to make efficient use of the data and draw appropriate conclusions, social sciences to better understand proximate and distal causes, and engineering for exposure assessment.  Epidemiology \"Epidemiology\", literally meaning \"the study of what is upon the people\", is derived , suggesting that it applies only to human populations. However, the term is widely used in studies of zoological populations (veterinary epidemiology), although the term \"epizoology\" is available, and it has also been applied to studies of plant populations (botanical or plant disease epidemiology).  The distinction between \"epidemic\" and \"endemic\" was first drawn by Hippocrates, to distinguish between diseases that are \"visited upon\" a population (epidemic) from those that \"reside within\""
    },
    {
        "id": "TASK_experimental-design",
        "name": "Experimental Design",
        "description": "",
        "type": "Task",
        "probable_wikipedia": "Design of experiments",
        "score": "9.125215",
        "probable_description": " The design of experiments (DOE, DOX, or experimental design) is the design of any task that aims to describe or explain the variation of information under conditions that are hypothesized to reflect the variation. The term is generally associated with experiments in which the design introduces conditions that directly affect the variation, but may also refer to the design of quasi-experiments, in which natural conditions that influence the variation are selected for observation.  In its simplest form, an experiment aims at predicting the outcome by introducing a change of the preconditions, which is represented by one or more independent variables, also referred to as \"input variables\" or \"predictor variables.\" The change in one or more independent variables is generally hypothesized to result in a change in one or more dependent variables, also referred to as \"output variables\" or \"response variables.\" The experimental design may also identify control variables that must be held constant to prevent external factors from affecting the results. Experimental design involves not only the selection of suitable independent, dependent, and control variables, but planning the delivery of the experiment under statistically optimal conditions given the constraints of available resources. There are multiple approaches for determining the set of design points (unique combinations of the settings of the independent variables) to be used in the experiment.  Main concerns in experimental design include the establishment of validity, reliability, and replicability. For example, these concerns can be partially addressed by carefully choosing the independent variable, reducing the risk of measurement error, and ensuring"
    },
    {
        "id": "TASK_face-detection",
        "name": "Face Detection",
        "description": "Face detection is the task of detecting faces in a photo or video (and distinguishing them from other objects).\r\n\r\n<span style=\"color:grey; opacity: 0.6\">( Image credit: [insightface](https://github.com/deepinsight/insightface) )</span>",
        "type": "Task",
        "probable_wikipedia": "Face detection",
        "score": "11.641516",
        "probable_description": " Face detection is a computer technology being used in a variety of applications that identifies human faces in digital images. Face detection also refers to the psychological process by which humans locate and attend to faces in a visual scene. "
    },
    {
        "id": "TASK_face-hallucination",
        "name": "Face Hallucination",
        "description": "Face hallucination is the task of generating high-resolution (HR) facial images from low-resolution (LR) inputs.\r\n\r\n<span style=\"color:grey; opacity: 0.6\">( Image credit: [Deep CNN Denoiser and Multi-layer Neighbor Component Embedding for Face Hallucination](https://arxiv.org/pdf/1806.10726v1.pdf) )</span>",
        "type": "Task",
        "probable_wikipedia": "Face hallucination",
        "score": "10.955734",
        "probable_description": " Face hallucination refers to any superresolution technique which applies specifically to faces. It comprises techniques which take noisy or low-resolution facial images, and convert them into high-resolution images using knowledge about typical facial features. It can be applied in facial recognition systems for identifying faces faster and more effectively. Due to the potential applications in facial recognition systems, face hallucination has become an active area of research. "
    },
    {
        "id": "TASK_facial-emotion-recognition",
        "name": "Facial Emotion Recognition",
        "description": "Emotion Recognition from facial images",
        "type": "Task",
        "probable_wikipedia": "Emotion recognition",
        "score": "2.096784",
        "probable_description": " Emotion recognition is the process of identifying human emotion, most typically from facial expressions as well as from verbal expressions. This is both something that humans do automatically but computational methodologies have also been developed. "
    },
    {
        "id": "TASK_facial-expression-recognition",
        "name": "Facial Expression Recognition",
        "description": "Facial expression recognition is the task of classifying the expressions on face images into various categories such as anger, fear, surprise, sadness, happiness and so on.\r\n\r\n<span style=\"color:grey; opacity: 0.6\">( Image credit: [DeXpression](https://arxiv.org/pdf/1509.05371v2.pdf) )</span>",
        "type": "Task",
        "probable_wikipedia": "Facial recognition system",
        "score": "0.27794954",
        "probable_description": " A facial recognition system is a technology capable of identifying or verifying a person from a digital image or a video frame from a video source. There are multiple methods in which facial recognition systems work, but in general, they work by comparing selected facial features from given image with faces within a database. It is also described as a Biometric Artificial Intelligence based application that can uniquely identify a person by analysing patterns based on the person's facial textures and shape.  While initially a form of computer application, it has seen wider uses in recent times on mobile platforms and in other forms of technology, such as robotics. It is typically used as access control in security systems and can be compared to other biometrics such as fingerprint or eye iris recognition systems. Although the accuracy of facial recognition system as a biometric technology is lower than iris recognition and fingerprint recognition, it is widely adopted due to its contactless and non-invasive process. Recently, it has also become popular as a commercial identification and marketing tool. Other applications include advanced human-computer interaction, video surveillance, automatic indexing of images, and video database, among others. "
    },
    {
        "id": "TASK_fad",
        "name": "FAD",
        "description": "",
        "type": "Task",
        "probable_wikipedia": "Fad",
        "score": "11.406201",
        "probable_description": " A fad, trend, or craze is any form of collective behavior that develops within a culture, a generation or social group in which a group of people enthusiastically follow an impulse for a finite period.  Fads are objects or behaviors that achieve short-lived popularity but fade away. Fads are often seen as sudden, quick-spreading, and short-lived. Fads include diets, clothing, hairstyles, toys, and more. Some popular fads throughout history are toys such as yo-yos, hula hoops, and fad dances such as the Macarena and the twist.  Similar to habits or customs but less durable, fads often result from an activity or behavior being perceived as emotionally popular or exciting within a peer group, or being deemed \"cool\" as often promoted by social networks. A fad is said to \"catch on\" when the number of people adopting it begins to increase to the point of being noteworthy. Fads often fade quickly when the perception of novelty is gone. "
    },
    {
        "id": "TASK_fault-detection",
        "name": "Fault Detection",
        "description": "",
        "type": "Task",
        "probable_wikipedia": "Fault detection and isolation",
        "score": "1.5319505",
        "probable_description": " Fault detection, isolation, and recovery (FDIR) is a subfield of control engineering which concerns itself with monitoring a system, identifying when a fault has occurred, and pinpointing the type of fault and its location. Two approaches can be distinguished: A direct pattern recognition of sensor readings that indicate a fault and an analysis of the discrepancy between the sensor readings and expected values, derived from some model. In the latter case, it is typical that a fault is said to be detected if the discrepancy or \"residual\" goes above a certain threshold. It is then the task of fault isolation to categorize the type of fault and its location in the machinery. Fault detection and isolation (FDI) techniques can be broadly classified into two categories. These include model-based FDI and signal processing based FDI. "
    },
    {
        "id": "TASK_feature-engineering",
        "name": "Feature Engineering",
        "description": "Feature engineering is the process of taking a dataset and constructing explanatory variables\u200a\u2014\u200afeatures\u200a\u2014\u200athat can be used to train a machine learning model for a prediction problem. Often, data is spread across multiple tables and must be gathered into a single table with rows containing the observations and features in the columns.\r\n\r\nThe traditional approach to feature engineering is to build features one at a time using domain knowledge, a tedious, time-consuming, and error-prone process known as manual feature engineering. The code for manual feature engineering is problem-dependent and must be re-written for each new dataset.",
        "type": "Task",
        "probable_wikipedia": "Feature engineering",
        "score": "11.738175",
        "probable_description": " Feature engineering is the process of using domain knowledge of the data to create features that make machine learning algorithms work. Feature engineering is fundamental to the application of machine learning, and is both difficult and expensive. The need for manual feature engineering can be obviated by automated feature learning.  Feature engineering is an informal topic, but it is considered essential in applied machine learning. "
    },
    {
        "id": "TASK_federated-learning",
        "name": "Federated Learning",
        "description": "Federated Learning is a framework to train a centralized model for a task where the data is de-centralized across different devices/ silos. \r\n\r\nThis helps preserve privacy of data on various devices as only the weight updates are shared with the centralized model so the data can remain on each device and we can still train a model using that data.",
        "type": "Task",
        "probable_wikipedia": "Federated learning",
        "score": "12.235045",
        "probable_description": " Federated learning designates a set of techniques striving to simultaneously train a single machine learning algorithm across multiple decentralized servers holding local data samples, without exchanging their data samples. This approach stands in contrast to traditional centralized machine learning techniques where all data samples are uploaded to one server, as well as to more classical decentralized approaches which assume that local data samples are identically distributed.  Federated learning enables multiple actors to build a common, robust machine learning model without sharing data samples, thus addressing critical issues such as data privacy, data security, data access rights and access to heterogeneous data. Its applications are spread over a number of industries including defense, telecommunications, IoT, or pharmaceutics.  = Definition = Federated learning aims at training a machine learning algorithm, for instance deep neural networks, on multiple local datasets contained in local nodes without exchanging data samples. The general principle consists in training local models on local data samples and exchanging parameters (e.g. the weights of a deep neural network) between these local models at some frequency to generate a global model.  Federated learning algorithms may use a central server that orchestrates the different steps of the algorithm and acts as a reference clock, or they may be peer-to-peer, where no such central server exists. In the non peer-to-peer case, a federated learning process can be broken down in multiple rounds, each consisting of 4 general steps. The main difference between federated learning and distributed learning lies in the assumptions made on the properties"
    },
    {
        "id": "TASK_fish-detection",
        "name": "Fish Detection",
        "description": "",
        "type": "Task",
        "probable_wikipedia": "Fluorescence in situ hybridization",
        "score": "3.5382667",
        "probable_description": " Fluorescence \"in situ\" hybridization (FISH) is a molecular cytogenetic technique that uses fluorescent probes that bind to only those parts of a nucleic acid sequence with a high degree of sequence complementarity. It was developed by biomedical researchers in the early 1980s to detect and localize the presence or absence of specific DNA sequences on chromosomes. Fluorescence microscopy can be used to find out where the fluorescent probe is bound to the chromosomes. FISH is often used for finding specific features in DNA for use in genetic counseling, medicine, and species identification. FISH can also be used to detect and localize specific RNA targets (mRNA, lncRNA and miRNA) in cells, circulating tumor cells, and tissue samples. In this context, it can help define the spatial-temporal patterns of gene expression within cells and tissues. "
    },
    {
        "id": "TASK_fps-games",
        "name": "FPS Games",
        "description": "First-person shooter (FPS) games Involve like call of duty so enjoy\r\n\r\n<span style=\"color:grey; opacity: 0.6\">( Image credit: [Procedural Urban Environments for FPS Games](https://arxiv.org/pdf/1604.05791v1.pdf) )</span>",
        "type": "Task",
        "probable_wikipedia": "First-person shooter",
        "score": "10.78396",
        "probable_description": " First-person shooter (FPS) is a video game genre centered around gun and other weapon-based combat in a first-person perspective; that is, the player experiences the action through the eyes of the protagonist. The genre shares common traits with other shooter games, which in turn makes it fall under the heading action game. Since the genre's inception, advanced 3D and pseudo-3D graphics have challenged hardware development, and multiplayer gaming has been integral.  The first-person shooter genre has been traced as far back as \"Maze War\", development of which began in 1973, and 1974's \"Spasim\". Later, and after more playful titles like \"MIDI Maze\" in 1987, the genre coalesced into a more violent form with 1992's \"Wolfenstein 3D\", which has been credited with creating the genre's basic archetype upon which subsequent titles were based. One such title, and the progenitor of the genre's wider mainstream acceptance and popularity was \"Doom,\" one of the most influential games in this genre; for some years, the term Doom clone was used to designate this genre due to \"Doom\"s influence. Corridor shooter was another common name for the genre in its early years, since processing limitations of the era's hardware meant that most of the action in the games had to take place in enclosed areas.  1998's \"Half-Life\"\u2014along with its 2004 sequel \"Half-Life 2\"\u2014enhanced the narrative and puzzle elements. In 1999, the \"Half-Life\" mod \"Counter-Strike\" was released and, together with \"Doom\", is perhaps one of the most influential first-person shooters. \"GoldenEye 007\", released in 1997, was a landmark first-person"
    },
    {
        "id": "TASK_future-prediction",
        "name": "Future prediction",
        "description": "",
        "type": "Task",
        "probable_wikipedia": "Prediction",
        "score": "4.862368",
        "probable_description": " A prediction (Latin \"pr\u00e6-\", \"before,\" and \"dicere\", \"to say\"), or forecast, is a statement about a future event. A prediction is often, but not always, based upon experience or knowledge. There is no universal agreement about the exact difference between the two terms; different authors and disciplines ascribe different connotations. (Contrast with estimation.)  Although future events are necessarily uncertain, so guaranteed accurate information about the future is in many cases impossible, prediction can be useful to assist in making plans about possible developments; Howard H. Stevenson writes that prediction in business \"... is at least two things: Important and hard.\" "
    },
    {
        "id": "TASK_game-of-chess",
        "name": "Game of Chess",
        "description": "Chess is a two-player strategy board game played on a chessboard, a checkered gameboard with 64 squares arranged in an 8\u00d78 grid. The idea of making a machine that could beat a Grandmaster human player was a fascination in the artificial community for decades. Famously IBM's DeepBlue beat Kasparov in the 1990s. More recently more human-like approaches such as AlphaZero have appeared.",
        "type": "Task",
        "probable_wikipedia": "Chess",
        "score": "3.5166829",
        "probable_description": " Chess is a two-player strategy board game played on a checkered board with 64\u00a0squares arranged in an 8\u00d78 grid. The game is played by millions of people worldwide. Chess is believed to be derived from the Indian game chaturanga sometime before the 7th\u00a0century. Chaturanga is also the likely ancestor of the Eastern strategy games xiangqi, janggi, and shogi. Chess reached Europe by the 9th\u00a0century, due to the Umayyad conquest of Hispania. The pieces assumed their current powers in Spain in the late 15th\u00a0century; the modern rules were standardized in the 19th\u00a0century.  Play involves no hidden information. Each player begins with 16 pieces: one king, one queen, two rooks, two knights, two bishops, and eight pawns. Each piece type moves differently, with the most powerful being the queen and the least powerful the pawn. The objective is to checkmate the opponent's king by placing it under an inescapable threat of capture. To this end, a player's pieces are used to attack and capture the opponent's pieces, while supporting each other. During the game, play typically involves pieces for the opponent's similar pieces, and finding and engineering opportunities to trade advantageously or to get a better position. In addition to checkmate, a player wins the game if the opponent , or (in a timed game) runs out of time. There are also several ways that a game can end in a draw.  The first generally recognized World Chess Champion, Wilhelm Steinitz, claimed his title in 1886. Since 1948, the World Championship has been regulated by"
    },
    {
        "id": "TASK_game-of-cricket",
        "name": "Game of Cricket",
        "description": "",
        "type": "Task",
        "probable_wikipedia": "Cricket",
        "score": "3.5508575",
        "probable_description": " Cricket is a bat-and-ball game played between two teams of eleven players on a field at the centre of which is a pitch with a wicket at each end, each comprising two bails balanced on three stumps. The batting side scores runs by striking the ball bowled at the wicket with the bat, while the bowling and fielding side tries to prevent this and dismiss each player (so they are \"out\"). Means of dismissal include being bowled, when the ball hits the stumps and dislodges the bails, and by the fielding side catching the ball after it is hit by the bat, but before it hits the ground. When ten players have been dismissed, the innings ends and the teams swap roles. The game is adjudicated by two umpires, aided by a third umpire and match referee in international matches. They communicate with two off-field scorers who record the match's statistical information.  There are various formats ranging from Twenty20, played over a few hours with each team batting for a single innings of 20 overs, to Test matches, played over five days with unlimited overs and the teams each batting for two innings of unlimited length. Traditionally cricketers play in all-white kit, but in limited overs cricket they wear club or team colours. In addition to the basic kit, some players wear protective gear to prevent injury caused by the ball, which is a hard, solid spheroid made of compressed leather with a slightly raised sewn seam enclosing a cork core which is layered"
    },
    {
        "id": "TASK_game-of-doom",
        "name": "Game of Doom",
        "description": "Doom is an FPS game : the task is typically to train an agent to navigate the game environment, and additionally, acquire points by eliminating enemies.\r\n\r\n<span style=\"color:grey; opacity: 0.6\">( Image credit: [Playing FPS Games with Deep Reinforcement Learning](https://arxiv.org/pdf/1609.05521v2.pdf) )</span>",
        "type": "Task",
        "probable_wikipedia": "Doom (1993 video game)",
        "score": "1.2890424",
        "probable_description": " Doom (stylized as DOOM or DooM in other media) is a 1993 first-person shooter (FPS) game developed by id Software for MS-DOS. It is one of the most significant games in video game history, and is frequently cited as one of the greatest games of all time.  Players assume the role of a space marine, popularly known as \"Doomguy\", fighting his way through hordes of invading demons from Hell. The first episode, comprising nine levels, was distributed freely as shareware and played by an estimated 15\u201320\u00a0million people within two years; the full game, with two further episodes, were sold via mail order. An updated version with an additional episode, \"Ultimate Doom\", was released in 1995 and sold at retail.  Along with its predecessor \"Wolfenstein 3D\", \"Doom\" helped define FPS genre and inspired numerous similar games, known as \"\"Doom\" clones\". It pioneered online distribution and technologies including 3D graphics, networked multiplayer gaming, and support for customized modifications via packaged files (WADs). Its graphic violence and hellish imagery made it the subject of controversy.  The \"Doom\" franchise continued with \"\" (1994) and numerous expansion packs, including \"Master Levels for Doom II\" (1995) and \"Final Doom\" (1996), all ported to numerous platforms. The source code was released in 1997, inspiring further adaptations. Id returned to the franchise with \"Doom 3\" (2004), a horror-focused retelling using the id Tech 4 engine, followed by a 2005 \"Doom\" film. A 2016 reboot, also titled \"Doom,\" powered by id Tech 6, returned to the fast-paced action of the first"
    },
    {
        "id": "TASK_game-of-football",
        "name": "Game of Football",
        "description": "",
        "type": "Task",
        "probable_wikipedia": "Association football",
        "score": "7.522149",
        "probable_description": " Association football, more commonly known as football or soccer, is a team sport played with a spherical ball between two teams of eleven players. It is played by 250 million players in over 200 countries and dependencies, making it the world's most popular sport. The game is played on a rectangular field called a pitch with a goal at each end. The object of the game is to score by moving the ball beyond the goal line into the opposing goal.  Association football is one of a family of football codes, which emerged from various ball games played worldwide since antiquity. The modern game traces its origins to 1863 when the Laws of the Game were originally codified in England by The Football Association.  Players are not allowed to touch the ball with hands or arms while it is in play, except for the goalkeepers within the penalty area. Other players mainly use their feet to strike or pass the ball, but may also use any other part of their body except the hands and the arms. The team that scores most goals by the end of the match wins. If the score is level at the end of the game, either a draw is declared or the game goes into extra time or a penalty shootout depending on the format of the competition. Association football is governed internationally by the International Federation of Association Football (FIFA; ), which organises World Cups for both men and women every four years. "
    },
    {
        "id": "TASK_game-of-go",
        "name": "Game of Go",
        "description": "Go is an abstract strategy board game for two players, in which the aim is to surround more territory than the opponent. The task is to train an agent to play the game and be superior to other players.",
        "type": "Task",
        "probable_wikipedia": "Go (game)",
        "score": "2.8496602",
        "probable_description": " Go is an abstract strategy board game for two players, in which the aim is to surround more territory than the opponent. The game was invented in China more than 2,500 years ago and is believed to be the oldest board game continuously played to the present day. A 2016 survey by the International Go Federation's 75 member nations found that there are over 46 million people worldwide who know how to play Go and over 20 million current players, the majority of whom live in East Asia.  The playing pieces are called \"stones\". One player uses the white stones and the other, black. The players take turns placing the stones on the vacant intersections (\"\"points\"\") of a board. Once placed on the board, stones may not be moved, but stones are removed from the board if \"captured\". Capture happens when a stone or group of stones is surrounded by opposing stones on all orthogonally-adjacent points. The game proceeds until neither player wishes to make another move. When a game concludes, the winner is determined by counting each player's surrounded territory along with captured stones and komi (points added to the score of the player with the white stones as compensation for playing second). Games may also be terminated by resignation. A teacher might simplify the explanation by saying to a student \"you may place your stone on any point on the board, but if I surround that stone, I will remove it.\"  The standard Go board has a 19\u00d719 grid of lines,"
    },
    {
        "id": "TASK_game-of-poker",
        "name": "Game of Poker",
        "description": "",
        "type": "Task",
        "probable_wikipedia": "Poker",
        "score": "5.7143273",
        "probable_description": " Poker is a family of card games that combines gambling, strategy, and skill. All poker variants involve betting as an intrinsic part of play, and determine the winner of each hand according to the combinations of players' cards, at least some of which remain hidden until the end of the hand. Poker games vary in the number of cards dealt, the number of shared or \"community\" cards, the number of cards that remain hidden, and the betting procedures.  In most modern poker games the first round of betting begins with one or more of the players making some form of a forced bet (the \"blind\" or \"ante\"). In standard poker, each player bets according to the rank they believe their hand is worth as compared to the other players. The action then proceeds clockwise as each player in turn must either match (or \"call\") the maximum previous bet, or fold, losing the amount bet so far and all further involvement in the hand. A player who matches a bet may also \"raise\" (increase) the bet. The betting round ends when all players have either called the last bet or folded. If all but one player folds on any round, the remaining player collects the pot without being required to reveal their hand. If more than one player remains in contention after the final betting round, a showdown takes place where the hands are revealed, and the player with the winning hand takes the pot.  With the exception of initial forced bets, money is"
    },
    {
        "id": "TASK_game-of-shogi",
        "name": "Game of Shogi",
        "description": "",
        "type": "Task",
        "probable_wikipedia": "Shogi",
        "score": "2.3148296",
        "probable_description": " Shogi was the earliest chess variant to allow captured pieces to be returned to the board by the capturing player. This drop rule is speculated to have been invented in the 15th century and possibly connected to the practice of 15th century mercenaries switching loyalties when captured instead of being killed.  The earliest predecessor of the game, chaturanga, originated in India in the 6th century. Shogi in its present form was played as early as the 16th century, while a direct ancestor without the drop rule was recorded from 1210 in a historical document \"Nichureki\", which is an edited copy of \"Shochureki\" and \"Kaichureki\" from the late Heian period (c. 1120). "
    },
    {
        "id": "TASK_gaussian-processes",
        "name": "Gaussian Processes",
        "description": "**Gaussian Processes** is a powerful framework for several machine learning tasks such as regression, classification and inference. Given a finite set of input output training data that is generated out of a fixed (but possibly unknown) function, the framework models the unknown function as a stochastic process such that the training outputs are a finite number of jointly Gaussian random variables, whose properties can then be used to infer the statistics (the mean and variance) of the function at test values of input.\n\n\n<span class=\"description-source\">Source: [Sequential Randomized Matrix Factorization for Gaussian Processes: Efficient Predictions and Hyper-parameter Optimization ](https://arxiv.org/abs/1711.06989)</span>",
        "type": "Task",
        "probable_wikipedia": "Gaussian process",
        "score": "10.272404",
        "probable_description": " In probability theory and statistics, a Gaussian process is a stochastic process (a collection of random variables indexed by time or space), such that every finite collection of those random variables has a multivariate normal distribution, i.e. every finite linear combination of them is normally distributed. The distribution of a Gaussian process is the joint distribution of all those (infinitely many) random variables, and as such, it is a distribution over functions with a continuous domain, e.g. time or space.  A machine-learning algorithm that involves a Gaussian process uses lazy learning and a measure of the similarity between points (the \"kernel function\") to predict the value for an unseen point from training data. The prediction is not just an estimate for that point, but also has uncertainty information\u2014it is a one-dimensional Gaussian distribution (which is the marginal distribution at that point).  For some kernel functions, matrix algebra can be used to calculate the predictions using the technique of kriging. When a parameterised kernel is used, optimisation software is typically used to fit a Gaussian process model.  The concept of Gaussian processes is named after Carl Friedrich Gauss because it is based on the notion of the Gaussian distribution (normal distribution). Gaussian processes can be seen as an infinite-dimensional generalization of multivariate normal distributions.  Gaussian processes are useful in statistical modelling, benefiting from properties inherited from the normal distribution. For example, if a random process is modelled as a Gaussian process, the distributions of various derived quantities can be obtained explicitly. Such"
    },
    {
        "id": "TASK_gesture-recognition",
        "name": "Gesture Recognition",
        "description": "**Gesture Recognition** is an active field of research with applications such as automatic recognition of sign language, interaction of humans and robots or for new ways of controlling video games.\n\n\n<span class=\"description-source\">Source: [Gesture Recognition in RGB Videos Using Human Body Keypoints and Dynamic Time Warping ](https://arxiv.org/abs/1906.12171)</span>",
        "type": "Task",
        "probable_wikipedia": "Gesture recognition",
        "score": "10.602058",
        "probable_description": " Gesture recognition is a topic in computer science and language technology with the goal of interpreting human gestures via mathematical algorithms. Gestures can originate from any bodily motion or state but commonly originate from the face or hand. Current focuses in the field include emotion recognition from face and hand gesture recognition. Users can use simple gestures to control or interact with devices without physically touching them. Many approaches have been made using cameras and computer vision algorithms to interpret sign language. However, the identification and recognition of posture, gait, proxemics, and human behaviors is also the subject of gesture recognition techniques. Gesture recognition can be seen as a way for computers to begin to understand human body language, thus building a richer bridge between machines and humans than primitive text user interfaces or even GUIs (graphical user interfaces), which still limit the majority of input to keyboard and mouse and interact naturally without any mechanical devices. Using the concept of gesture recognition, it is possible to point a finger at this point will move accordingly. This could make conventional input on devices such and even redundant. "
    },
    {
        "id": "TASK_graph-embedding",
        "name": "Graph Embedding",
        "description": "Graph embeddings learn a mapping from a network to a vector space, while preserving relevant network properties.\r\n\r\n<span style=\"color:grey; opacity: 0.6\">( Image credit: [GAT](https://github.com/PetarV-/GAT) )</span>",
        "type": "Task",
        "probable_wikipedia": "Graph embedding",
        "score": "8.072085",
        "probable_description": " In topological graph theory, an embedding (also spelled imbedding) of a graph formula_1 on a surface formula_2 is a representation of formula_1 on formula_2 in which points of formula_2 are associated with vertices and simple arcs (homeomorphic images of formula_6) are associated with edges in such a way that:  Here a surface is a compact, connected formula_9-manifold.  Informally, an embedding of a graph into a surface is a drawing of the graph on the surface in such a way that its edges may intersect only at their endpoints. It is well known that any finite graph can be embedded in 3-dimensional Euclidean space formula_10 and planar graphs can be embedded in 2-dimensional Euclidean space formula_11  Often, an embedding is regarded as an equivalence class (under homeomorphisms of formula_2) of representations of the kind just described.  Some authors define a weaker version of the definition of \"graph embedding\" by omitting the non-intersection condition for edges. In such contexts the stricter definition is described as \"non-crossing graph embedding\".  This article deals only with the strict definition of graph embedding. The weaker definition is discussed in the articles \"graph drawing\" and \"crossing number\". "
    },
    {
        "id": "TASK_graph-matching",
        "name": "Graph Matching",
        "description": "**Graph Matching** is the problem of finding correspondences between two sets of vertices while preserving complex relational information among them. Since the graph structure has a strong capacity to represent objects and robustness to severe deformation and outliers, it is frequently adopted to formulate various correspondence problems in the field of computer vision. Theoretically, the Graph Matching problem can be solved by exhaustively searching the entire solution space. However, this approach is infeasible in practice because the solution space expands exponentially as the size of input data increases. For that reason, previous studies have attempted to solve the problem by using various approximation techniques.\n\n\n<span class=\"description-source\">Source: [Consistent Multiple Graph Matching with Multi-layer Random Walks Synchronization ](https://arxiv.org/abs/1712.02575)</span>",
        "type": "Task",
        "probable_wikipedia": "Graph matching",
        "score": "10.765824",
        "probable_description": " Graph matching is the problem of finding a similarity between graphs.  Graphs are commonly used to encode structural information in many fields, including computer vision and pattern recognition, and graph matching is an important tool in these areas. In these areas it is commonly assumed that the comparison is between the \"data graph\" and the \"model graph\".  The case of exact graph matching is known as the graph isomorphism problem. The problem of exact matching of a graph to a part of another graph is called subgraph isomorphism problem.  The inexact graph matching refers to matching problems when exact matching is impossible, e.g., when the number of vertices in the two graphs are different. In this case it is required to find the best possible match. For example, in image recognition applications, the results of image segmentation in image processing typically produces data graphs with the numbers of vertices much larger than in the model graphs data expected to match against. In the case of attributed graphs, even if the numbers of vertices and edges are the same, the matching still may be only inexact.  Two categories of search methods are the ones based on identification of possible and impossible pairings of vertices between the two graphs and methods which formulate graph matching as an optimization problem. Graph edit distance is one of similarity measures suggested for graph matching. The class of algorithms is called error-tolerant graph matching.  "
    },
    {
        "id": "TASK_graph-partitioning",
        "name": "graph partitioning",
        "description": "Graph Partitioning is generally the first step of distributed graph computing tasks. The targets are load-balance and minimizing the communication volume.",
        "type": "Task",
        "probable_wikipedia": "Graph partition",
        "score": "8.164548",
        "probable_description": " In mathematics, a graph partition is the reduction of a graph to a smaller graph by partitioning its nodes into mutually exclusive groups. Edges of the original graph that cross between the groups will produce edges in the partitioned graph. If the number of resulting edges is small compared to the original graph, then the partitioned graph may be better suited for analysis and problem-solving than the original. Finding a partition that simplifies graph analysis is a hard problem, but one that has applications to scientific computing, VLSI circuit design, and task scheduling in multi-processor computers, among others. Recently, the graph partition problem has gained importance due to its application for clustering and detection of cliques in social, pathological and biological networks. For a survey on recent trends in computational methods and applications see . "
    },
    {
        "id": "TASK_grayscale-image-denoising",
        "name": "Grayscale Image Denoising",
        "description": "",
        "type": "Task",
        "probable_wikipedia": "Grayscale",
        "score": "0.7276563",
        "probable_description": " In digital photography, computer-generated imagery, and colorimetry, a grayscale or greyscale image is one in which the value of each pixel is a single sample representing only an \"amount\" of light, that is, it carries only intensity information. Grayscale images, a kind of black-and-white or gray monochrome, are composed exclusively of shades of gray. The contrast ranges from black at the weakest intensity to white at the strongest.  Grayscale images are distinct from one-bit bi-tonal black-and-white images which, in the context of computer imaging, are images with only two colors: black and white (also called \"bilevel\" or \"binary images\"). Grayscale images have many shades of gray in between.  Grayscale images can be the result of measuring the intensity of light at each pixel according to a particular weighted combination of frequencies (or wavelengths), and in such cases they are monochromatic proper when only a single frequency (in practice, a narrow band of frequencies) is captured. The frequencies can in principle be from anywhere in the electromagnetic spectrum (e.g. infrared, visible light, ultraviolet, etc.).  A colorimetric (or more specifically photometric) grayscale image is an image that has a defined grayscale colorspace, which maps the stored numeric sample values to the achromatic channel of a standard colorspace, which itself is based on measured properties of human vision.  If the original color image has no defined colorspace, or if the grayscale image is not intended to have the same human-perceived achromatic intensity as the color image, then there is no unique mapping from such"
    },
    {
        "id": "TASK_gunshot-detection",
        "name": "Gunshot Detection",
        "description": "",
        "type": "Task",
        "probable_wikipedia": "Gunfire locator",
        "score": "5.998704",
        "probable_description": " A gunfire locator or gunshot detection system is a system that detects and conveys the location of gunfire or other weapon fire using acoustic, optical, or potentially other types of sensors, as well as a combination of such sensors. These systems are used by law enforcement, security, military and businesses to identify the source and, in some cases, the direction of gunfire and/or the type of weapon fired. Most systems possess three main components:   Systems used in urban settings integrate a geographic information system so the display includes a map and address location of each incident. "
    },
    {
        "id": "TASK_handwriting-recognition",
        "name": "Handwriting Recognition",
        "description": "Image source: [Handwriting Recognition of Historical Documents with few labeled data](https://arxiv.org/pdf/1811.07768v1.pdf)",
        "type": "Task",
        "probable_wikipedia": "Handwriting recognition",
        "score": "11.926944",
        "probable_description": " Handwriting recognition (HWR), also known as Handwritten Text Recognition (HTR), is the ability of a computer to receive and interpret intelligible handwritten input from sources such as paper documents, photographs, touch-screens and other devices. The image of the written text may be sensed \"off line\" from a piece of paper by optical scanning (optical character recognition) or intelligent word recognition. Alternatively, the movements of the pen tip may be sensed \"on line\", for example by a pen-based computer screen surface, a generally easier task as there are more clues available. A handwriting recognition system handles formatting, performs correct segmentation into characters, and finds the most plausible words. "
    },
    {
        "id": "TASK_handwritten-text-recognition",
        "name": "Handwritten Text Recognition",
        "description": "",
        "type": "Task",
        "probable_wikipedia": "Handwriting recognition",
        "score": "8.663055",
        "probable_description": " Handwriting recognition (HWR), also known as Handwritten Text Recognition (HTR), is the ability of a computer to receive and interpret intelligible handwritten input from sources such as paper documents, photographs, touch-screens and other devices. The image of the written text may be sensed \"off line\" from a piece of paper by optical scanning (optical character recognition) or intelligent word recognition. Alternatively, the movements of the pen tip may be sensed \"on line\", for example by a pen-based computer screen surface, a generally easier task as there are more clues available. A handwriting recognition system handles formatting, performs correct segmentation into characters, and finds the most plausible words. "
    },
    {
        "id": "TASK_heart-rate-variability",
        "name": "Heart Rate Variability",
        "description": "Heart rate variability (HRV) is the physiological phenomenon of variation in the time interval between heartbeats. It is measured by the variation in the beat-to-beat interval.",
        "type": "Task",
        "probable_wikipedia": "Heart rate variability",
        "score": "11.73414",
        "probable_description": " Heart rate variability (HRV) is the physiological phenomenon of variation in the time interval between heartbeats. It is measured by the variation in the beat-to-beat interval.  Other terms used include: \"cycle length variability\", \"RR variability\" (where R is a point corresponding to the peak of the QRS complex of the ECG wave; and RR is the interval between successive Rs), and \"heart period variability\".  Methods used to detect beats include: ECG, blood pressure, ballistocardiograms, and the pulse wave signal derived from a photoplethysmograph (PPG). ECG is considered superior because it provides a clear waveform, which makes it easier to exclude heartbeats not originating in the sinoatrial node. The term \"NN\" is used in place of RR to emphasize the fact that the processed beats are \"normal\" beats. "
    },
    {
        "id": "TASK_hippocampus",
        "name": "Hippocampus",
        "description": "",
        "type": "Task",
        "probable_wikipedia": "Hippocampus",
        "score": "11.813431",
        "probable_description": " The hippocampus (from the Greek \u03b9\u03c0\u03c0\u03bf\u03ba\u03b1\u03bc\u03c0\u03bf\u03c2, \"seahorse\") is a major component of the brain of humans and other vertebrates. Humans and other mammals have two hippocampi, one in each side of the brain. The hippocampus is part of the limbic system, and plays important roles in the consolidation of information from short-term memory to long-term memory, and in spatial memory that enables navigation. The hippocampus is located under the cerebral cortex in the allocortex, and in primates it is in the medial temporal lobe. It contains two main interlocking parts: the hippocampus proper (also called Ammon's horn) and the dentate gyrus.  In Alzheimer's disease (and other forms of dementia), the hippocampus is one of the first regions of the brain to suffer damage; short-term memory loss and disorientation are included among the early symptoms. Damage to the hippocampus can also result from oxygen starvation (hypoxia), encephalitis, or medial temporal lobe epilepsy. People with extensive, bilateral hippocampal damage may experience anterograde amnesia: the inability to form and retain new memories.  Since different neuronal cell types are neatly organized into layers in the hippocampus, it has frequently been used as a model system for studying neurophysiology. The form of neural plasticity known as long-term potentiation (LTP) was initially discovered to occur in the hippocampus and has often been studied in this structure. LTP is widely believed to be one of the main neural mechanisms by which memories are stored in the brain.  In rodents as model organisms, the hippocampus has been studied extensively as"
    },
    {
        "id": "TASK_human-detection",
        "name": "Human Detection",
        "description": "",
        "type": "Task",
        "probable_wikipedia": "Human sensing",
        "score": "4.5961504",
        "probable_description": " Human sensing (also called human detection or human presence detection) encompasses a range of technologies for detecting the presence of a human body in an area of space, typically without the intentional participation of the detected person. Common applications include search and rescue, surveillance, and customer analytics (for example, people counters).  Modern technologies proposed or deployed for human sensing include:  "
    },
    {
        "id": "TASK_human-dynamics",
        "name": "Human Dynamics",
        "description": "",
        "type": "Task",
        "probable_wikipedia": "Human dynamics",
        "score": "8.72828",
        "probable_description": " Human dynamics refer to a branch of complex systems research in statistical physics such as the movement of crowds and queues and other systems of complex human interactions including statistical modelling of human networks, including interactions over communications networks. "
    },
    {
        "id": "TASK_humanitarian",
        "name": "Humanitarian",
        "description": "",
        "type": "Task",
        "probable_wikipedia": "Humanitarianism",
        "score": "8.872348",
        "probable_description": " Humanitarianism is an active belief in the value of human life, whereby humans practice benevolent treatment and provide assistance to other humans, in order to better humanity for moral, altruistic and logical reasons. It is the philosophical belief in movement toward the improvement of the human race in a variety of areas, used to describe a wide number of activities relating specifically to human welfare. A practitioner is known as a humanitarian. "
    },
    {
        "id": "TASK_human-parsing",
        "name": "Human Parsing",
        "description": "Human parsing is the task of segmenting a human image into different fine-grained semantic parts such as head, torso, arms and legs.\r\n\r\n<span style=\"color:grey; opacity: 0.6\">( Image credit: [Multi-Human-Parsing (MHP)\r\n](https://github.com/ZhaoJ9014/Multi-Human-Parsing) )</span>",
        "type": "Task",
        "probable_wikipedia": "Parsing",
        "score": "5.4999075",
        "probable_description": " Parsing, syntax analysis, or syntactic analysis is the process of analysing a string of symbols, either in natural language, computer languages or data structures, conforming to the rules of a formal grammar. The term \"parsing\" comes from Latin \"pars\" (\"orationis\"), meaning part (of speech).  The term has slightly different meanings in different branches of linguistics and computer science. Traditional sentence parsing is often performed as a method of understanding the exact meaning of a sentence or word, sometimes with the aid of devices such as sentence diagrams. It usually emphasizes the importance of grammatical divisions such as subject and predicate.  Within computational linguistics the term is used to refer to the formal analysis by a computer of a sentence or other string of words into its constituents, resulting in a parse tree showing their syntactic relation to each other, which may also contain semantic and other information.  The term is also used in psycholinguistics when describing language comprehension. In this context, parsing refers to the way that human beings analyze a sentence or phrase (in spoken language or text) \"in terms of grammatical constituents, identifying the parts of speech, syntactic relations, etc.\" This term is especially common when discussing what linguistic cues help speakers to interpret garden-path sentences.  Within computer science, the term is used in the analysis of computer languages, referring to the syntactic analysis of the input code into its component parts in order to facilitate the writing of compilers and interpreters. The term may also be used to"
    },
    {
        "id": "TASK_human-robot-interaction",
        "name": "Human robot  interaction",
        "description": "",
        "type": "Task",
        "probable_wikipedia": "Human\u2013robot interaction",
        "score": "8.753265",
        "probable_description": " Human\u2013robot interaction is the study of interactions between humans and robots. It is often referred as HRI by researchers. Human\u2013robot interaction is a multidisciplinary field with contributions from human\u2013computer interaction, artificial intelligence, robotics, natural language understanding, design, and social sciences. "
    },
    {
        "id": "TASK_hybrid-positioning",
        "name": "Hybrid Positioning",
        "description": "Hybrid Positioning using CV and dead reckoning",
        "type": "Task",
        "probable_wikipedia": "Hybrid positioning system",
        "score": "3.9614062",
        "probable_description": " Hybrid positioning systems are systems for finding the location of a mobile device using several different positioning technologies. Usually GPS (Global Positioning System) is one major component of such systems, combined with cell tower signals, wireless internet signals, Bluetooth sensors, IP addresses and network environment data.  These systems are specifically designed to overcome the limitations of GPS, which is very exact in open areas, but works poorly indoors or between tall buildings (the urban canyon effect). By comparison, cell tower signals are not hindered by buildings or bad weather, but usually provide less precise positioning. Wi-Fi positioning systems may give very exact positioning, in urban areas with high Wi-Fi density - and depend on a comprehensive database of Wi-Fi access points.  Hybrid positioning systems are increasingly being explored for certain civilian and commercial location-based services and location-based media, which need to work well in urban areas in order to be commercially and practically viable.  Early works in this area include the Place Lab project, which started on 2003 and went inactive in 2006. Later methods let smartphones combine the accuracy of GPS with the low power consumption of cell-ID transition point finding.   "
    },
    {
        "id": "TASK_hyperparameter-optimization",
        "name": "Hyperparameter Optimization",
        "description": "**Hyperparameter Optimization** is the problem of choosing a set of optimal hyperparameters for a learning algorithm. Whether the algorithm is suitable for the data directly depends on hyperparameters, which directly influence overfitting or underfitting. Each model requires different assumptions, weights or training speeds for different types of data under the conditions of a given loss function.\r\n\r\n\r\n<span class=\"description-source\">Source: [Data-driven model for fracturing design optimization: focus on building digital database and production forecast ](https://arxiv.org/abs/1910.14499)</span>",
        "type": "Task",
        "probable_wikipedia": "Hyperparameter optimization",
        "score": "12.207422",
        "probable_description": " In machine learning, hyperparameter optimization or tuning is the problem of choosing a set of optimal hyperparameters for a learning algorithm. A hyperparameter is a parameter whose value is used to control the learning process. By contrast, the values of other parameters (typically node weights) are learned.  The same kind of machine learning model can require different constraints, weights or learning rates to generalize different data patterns. These measures are called hyperparameters, and have to be tuned so that the model can optimally solve the machine learning problem. Hyperparameter optimization finds a tuple of hyperparameters that yields an optimal model which minimizes a predefined loss function on given independent data. The objective function takes a tuple of hyperparameters and returns the associated loss. Cross-validation is often used to estimate this generalization performance. "
    },
    {
        "id": "TASK_hyperspectral-image-analysis",
        "name": "Hyperspectral image analysis",
        "description": "",
        "type": "Task",
        "probable_wikipedia": "Hyperspectral imaging",
        "score": "0.13542075",
        "probable_description": " Hyperspectral imaging, like other spectral imaging, collects and processes information from across the electromagnetic spectrum. The goal of hyperspectral imaging is to obtain the spectrum for each pixel in the image of a scene, with the purpose of finding objects, identifying materials, or detecting processes. There are two general branches of spectral imagers. There are push broom scanners and the related whisk broom scanners, which read images over time, and snapshot hyperspectral imaging, which uses a staring array to generate an image in an instant.  Whereas the human eye sees color of visible light in mostly three bands (long wavelengths - perceived as red, medium wavelengths - perceived as green, and short wavelengths - perceived as blue), spectral imaging divides the spectrum into many more bands. This technique of dividing images into bands can be extended beyond the visible. In hyperspectral imaging, the recorded spectra have fine wavelength resolution and cover a wide range of wavelengths. Hyperspectral imaging measures continuous spectral bands, as opposed to multispectral imaging which measures spaced spectral bands.  Engineers build hyperspectral sensors and processing systems for applications in astronomy, agriculture, molecular biology, biomedical imaging, geosciences, physics, and surveillance. Hyperspectral sensors look at objects using a vast portion of the electromagnetic spectrum. Certain objects leave unique 'fingerprints' in the electromagnetic spectrum. Known as spectral signatures, these 'fingerprints' enable identification of the materials that make up a scanned object. For example, a spectral signature for oil helps geologists find new oil fields. "
    },
    {
        "id": "TASK_image-compression",
        "name": "Image Compression",
        "description": "**Image Compression** is an application of data compression for digital images to lower their storage and/or transmission requirements.\n\n\n<span class=\"description-source\">Source: [Variable Rate Deep Image Compression With a Conditional Autoencoder ](https://arxiv.org/abs/1909.04802)</span>",
        "type": "Task",
        "probable_wikipedia": "Image compression",
        "score": "11.64199",
        "probable_description": " Image compression is a type of data compression applied to digital images, to reduce their cost for storage or transmission. Algorithms may take advantage of visual perception and the statistical properties of image data to provide superior results compared with generic data compression methods which are used for other digital data. "
    },
    {
        "id": "TASK_image-cropping",
        "name": "Image Cropping",
        "description": "**Image Cropping** is a common photo manipulation process, which improves the overall composition by removing unwanted regions. Image Cropping is widely used in photographic, film processing, graphic design, and printing businesses.\n\n\n<span class=\"description-source\">Source: [Listwise View Ranking for Image Cropping ](https://arxiv.org/abs/1905.05352)</span>",
        "type": "Task",
        "probable_wikipedia": "Cropping (image)",
        "score": "9.117927",
        "probable_description": " Cropping is the removal of unwanted outer areas from a photographic or illustrated image. The process usually consists of the removal of some of the peripheral areas of an image to remove extraneous trash from the picture, to improve its framing, to change the aspect ratio, or to accentuate or isolate the subject matter from its background. Depending on the application, this can be performed on a physical photograph, artwork, or film footage, or it can be achieved digitally by using image editing software. The process of cropping is common to the photographic, film processing, broadcasting, graphic design, and printing businesses. "
    },
    {
        "id": "TASK_image-deblurring",
        "name": "Image Deblurring",
        "description": "",
        "type": "Task",
        "probable_wikipedia": "Deblurring",
        "score": "8.095673",
        "probable_description": " Deblurring is the process of removing blurring artifacts from images, such as blur caused by defocus aberration or motion blur. The blur is typically modeled as the convolution of a (sometimes space- or time-varying) point spread function with a hypothetical sharp input image, where both the sharp input image (which is to be recovered) and the point spread function are unknown. This is an example of an inverse problem. In almost all cases, there is insufficient information in the blurred image to uniquely determine a plausible original image, making it an ill-posed problem. In addition the blurred image contains additional noise which complicates the task of determining the original image. This is generally solved by the use of a regularization term to attempt to eliminate implausible solutions. This problem is analogous to echo removal in the signal processing domain. Nevertheless, when coherent beam is used for imaging, the point spread function can be modeled mathematically. By proper deconvolution of the point spread function and the image, the resolution can be enhanced several times.   "
    },
    {
        "id": "TASK_image-defocus-deblurring",
        "name": "Image Defocus Deblurring",
        "description": "",
        "type": "Task",
        "probable_wikipedia": "Deblurring",
        "score": "0.088072106",
        "probable_description": " Deblurring is the process of removing blurring artifacts from images, such as blur caused by defocus aberration or motion blur. The blur is typically modeled as the convolution of a (sometimes space- or time-varying) point spread function with a hypothetical sharp input image, where both the sharp input image (which is to be recovered) and the point spread function are unknown. This is an example of an inverse problem. In almost all cases, there is insufficient information in the blurred image to uniquely determine a plausible original image, making it an ill-posed problem. In addition the blurred image contains additional noise which complicates the task of determining the original image. This is generally solved by the use of a regularization term to attempt to eliminate implausible solutions. This problem is analogous to echo removal in the signal processing domain. Nevertheless, when coherent beam is used for imaging, the point spread function can be modeled mathematically. By proper deconvolution of the point spread function and the image, the resolution can be enhanced several times.   "
    },
    {
        "id": "TASK_image-inpainting",
        "name": "Image Inpainting",
        "description": "**Image Inpainting** is a task of reconstructing missing regions in an image. It is an important problem in computer vision and an essential functionality in many imaging and graphics applications, e.g. object removal, image restoration, manipulation, re-targeting, compositing, and image-based rendering.\r\n\r\n<span class=\"description-source\">Source: [High-Resolution Image Inpainting with Iterative Confidence Feedback and Guided Upsampling ](https://arxiv.org/abs/2005.11742)</span>\r\n\r\nImage source: [High-Resolution Image Inpainting with Iterative Confidence Feedback and Guided Upsampling](https://arxiv.org/pdf/2005.11742.pdf)",
        "type": "Task",
        "probable_wikipedia": "Inpainting",
        "score": "3.9450288",
        "probable_description": " Inpainting is the process of reconstructing lost or deteriorated parts of images and videos. In the museum world, in the case of a valuable painting, this task would be carried out by a skilled art conservator or art restorer. In the digital world, inpainting (also known as \"image interpolation\" or \"video interpolation\") refers to the application of sophisticated algorithms to replace lost or corrupted parts of the image data (mainly small regions or to remove small defects). "
    },
    {
        "id": "TASK_image-matting",
        "name": "Image Matting",
        "description": "**Image Matting** is the process of accurately estimating the foreground object in images and videos. It is a very important technique in image and video editing applications, particularly in film production for creating visual effects. In case of image segmentation, we segment the image into foreground and background by labeling the pixels. Image segmentation generates a binary image, in which a pixel either belongs to foreground or background. However, Image Matting is different from the image segmentation, wherein some pixels may belong to foreground as well as background, such pixels are called partial or mixed pixels. In order to fully separate the foreground from the background in an image, accurate estimation of the alpha values for partial or mixed pixels is necessary.\r\n\r\n\r\n<span class=\"description-source\">Source: [Automatic Trimap Generation for Image Matting ](https://arxiv.org/abs/1707.00333)</span>\r\n\r\n<span class=\"description-source\">Image Source: [Real-Time High-Resolution Background Matting](https://arxiv.org/pdf/2012.07810v1.pdf)</span>",
        "type": "Task",
        "probable_wikipedia": "Matte (filmmaking)",
        "score": "0.4927462",
        "probable_description": " Mattes are used in photography and special effects filmmaking to combine two or more image elements into a single, final image. Usually, mattes are used to combine a foreground image (e.g. actors on a set or a spaceship) with a background image (e.g. a scenic vista or a starfield with planets). In this case, the matte is the background painting. In film and stage, mattes can be physically huge sections of painted canvas, portraying large scenic expanses of landscapes.  In film, the principle of a matte requires masking certain areas of the film emulsion to selectively control which areas are exposed. However, many complex special-effects scenes have included dozens of discrete image elements, requiring very complex use of mattes, and layering mattes on top of one another. For an example of a simple matte, we may wish to depict a group of actors in front of a store, with a massive city and sky visible above the store's roof. We would have two images\u2014the actors on the set, and the image of the city\u2014to combine onto a third. This would require two masks/mattes. One would mask everything above the store's roof, and the other would mask everything below it. By using these masks/mattes when copying these images onto the third, we can combine the images without creating ghostly double-exposures. In film, this is an example of a static matte, where the shape of the mask does not change from frame to frame. Other shots may require mattes that change, to mask the shapes of"
    },
    {
        "id": "TASK_image-morphing",
        "name": "Image Morphing",
        "description": "",
        "type": "Task",
        "probable_wikipedia": "Morphing",
        "score": "0.7533009",
        "probable_description": " Morphing is a special effect in motion pictures and animations that changes (or morphs) one image or shape into another through a seamless transition. Traditionally such a depiction would be achieved through cross-fading techniques on film. Since the early 1990s, this has been replaced by computer software to create more realistic transitions. "
    },
    {
        "id": "TASK_image-registration",
        "name": "Image Registration",
        "description": "Image registration is the process of transforming different sets of data into one coordinate system. Data may be multiple photographs, data from different sensors, times, depths, or viewpoints. It is used in computer vision, medical imaging, and compiling and analyzing images and data from satellites. Registration is necessary in order to be able to compare or integrate the data obtained from these different measurements.\r\n\r\nSource: [Image registration | Wikipedia](https://en.wikipedia.org/wiki/Image_registration)\r\n\r\n( Image credit: [Kornia](https://github.com/kornia/kornia) )",
        "type": "Task",
        "probable_wikipedia": "Image registration",
        "score": "12.454736",
        "probable_description": " Image registration is the process of transforming different sets of data into one coordinate system. Data may be multiple photographs, data from different sensors, times, depths, or viewpoints. It is used in computer vision, medical imaging, military automatic target recognition, and compiling and analyzing images and data from satellites. Registration is necessary in order to be able to compare or integrate the data obtained from these different measurements. "
    },
    {
        "id": "TASK_image-restoration",
        "name": "Image Restoration",
        "description": "**Image Restoration** is a family of inverse problems for obtaining a high quality image from a corrupted input image. Corruption may occur due to the image-capture process (e.g., noise, lens blur), post-processing (e.g., JPEG compression), or photography in non-ideal conditions (e.g., haze, motion blur).\n\n\n<span class=\"description-source\">Source: [Blind Image Restoration without Prior Knowledge ](https://arxiv.org/abs/2003.01764)</span>",
        "type": "Task",
        "probable_wikipedia": "Image restoration",
        "score": "12.019191",
        "probable_description": " Image Restoration is the operation of taking a corrupt/noisy image and estimating the clean, original image. Corruption may come in many forms such as motion blur, noise and camera mis-focus. Image restoration is performed by reversing the process that blurred the image and such is performed by imaging a point source and use the point source image, which is called the Point Spread Function (PSF) to restore the image information lost to the blurring process.  Image restoration is different from image enhancement in that the latter is designed to emphasize features of the image that make the image more pleasing to the observer, but not necessarily to produce realistic data from a scientific point of view. Image enhancement techniques (like contrast stretching or de-blurring by a nearest neighbor procedure) provided by imaging packages use no \"a priori\" model of the process that created the image.  With image enhancement noise can effectively be removed by sacrificing some resolution, but this is not acceptable in many applications. In a fluorescence microscope, resolution in the z-direction is bad as it is. More advanced image processing techniques must be applied to recover the object.  The objective of image restoration techniques is to reduce noise and recover resolution loss. Image processing techniques are performed either in the image domain or the frequency domain. The most straightforward and a conventional technique for image restoration is deconvolution, which is performed in the frequency domain and after computing the Fourier transform of both the image and the PSF and undo"
    },
    {
        "id": "TASK_image-retrieval",
        "name": "Image Retrieval",
        "description": "Image retrieval systems aim to find similar images to a query image among an image dataset.\r\n\r\n<span style=\"color:grey; opacity: 0.6\">( Image credit: [DELF](https://github.com/tensorflow/models/tree/master/research/delf) )</span>",
        "type": "Task",
        "probable_wikipedia": "Image retrieval",
        "score": "11.372308",
        "probable_description": " An image retrieval system is a computer system for browsing, searching and retrieving images from a large database of digital images. Most traditional and common methods of image retrieval utilize some method of adding metadata such as captioning, keywords, title or descriptions to the images so that retrieval can be performed over the annotation words. Manual image annotation is time-consuming, laborious and expensive; to address this, there has been a large amount of research done on automatic image annotation. Additionally, the increase in social web applications and the semantic web have inspired the development of several web-based image annotation tools.  The first microcomputer-based image database retrieval system was developed at MIT, in the 1990s, by Banireddy Prasaad, Amar Gupta, Hoo-min Toong, and Stuart Madnick.  A 2008 survey article documented progresses after 2007. "
    },
    {
        "id": "TASK_image-smoothing",
        "name": "image smoothing",
        "description": "",
        "type": "Task",
        "probable_wikipedia": "Smoothing",
        "score": "4.4324546",
        "probable_description": " In statistics and image processing, to smooth a data set is to create an approximating function that attempts to capture important patterns in the data, while leaving out noise or other fine-scale structures/rapid phenomena. In smoothing, the data points of a signal are modified so individual points (presumably because of noise) are reduced, and points that are lower than the adjacent points are increased leading to a smoother signal. Smoothing may be used in two important ways that can aid in data analysis (1) by being able to extract more information from the data as long as the assumption of smoothing is reasonable and (2) by being able to provide analyses that are both flexible and robust. Many different algorithms are used in smoothing.  Smoothing may be distinguished from the related and partially overlapping concept of curve fitting in the following ways: However, the terminology used across applications is mixed. For example, use of an interpolating spline fits a smooth curve exactly through the given data points and is sometimes called \"smoothing\". "
    },
    {
        "id": "TASK_image-stitching",
        "name": "Image Stitching",
        "description": "**Image Stitching** is a process of composing multiple images with narrow but overlapping fields of view to create a larger image with a wider field of view.\r\n\r\n\r\n<span class=\"description-source\">Source: [Single-Perspective Warps in Natural Image Stitching ](https://arxiv.org/abs/1802.04645)</span>\r\n\r\n( Image credit: [Kornia](https://github.com/kornia/kornia) )",
        "type": "Task",
        "probable_wikipedia": "Image stitching",
        "score": "11.920213",
        "probable_description": " Image stitching or photo stitching is the process of combining multiple photographic images with overlapping fields of view to produce a segmented panorama or high-resolution image. Commonly performed through the use of computer software, most approaches to image stitching require nearly exact overlaps between images and identical exposures to produce seamless results, although some stitching algorithms actually benefit from differently exposed images by doing high-dynamic-range-imaging in regions of overlap. Some digital cameras can stitch their photos internally.  Image stitching is widely used in modern applications, such as the following: "
    },
    {
        "id": "TASK_image-super-resolution",
        "name": "Image Super-Resolution",
        "description": "In this task, we try to upsample the image and create the high resolution image with help of a low resolution image.",
        "type": "Task",
        "probable_wikipedia": "Super-resolution imaging",
        "score": "0.3601691",
        "probable_description": " Super-resolution imaging (SR) is a class of techniques that enhance the resolution of an imaging system. In some SR techniques\u2014termed \"optical\" SR\u2014the diffraction limit of systems is transcended, while in others\u2014\"geometrical\" SR\u2014the resolution of digital imaging sensors is enhanced.  In some radar and sonar imaging applications (\"e.g.\", magnetic resonance imaging (MRI), high-resolution computed tomography), subspace decomposition-based methods (\"e.g.\", MUSIC) and compressed sensing-based algorithms (\"e.g.\", SAMV) are employed to achieve SR over standard periodogram algorithm.  Super-resolution imaging techniques are used in general image processing and in super-resolution microscopy. "
    },
    {
        "id": "TASK_imputation",
        "name": "Imputation",
        "description": "Substituting missing data with values according to some criteria.",
        "type": "Task",
        "probable_wikipedia": "Imputation (statistics)",
        "score": "11.0158615",
        "probable_description": " In statistics, imputation is the process of replacing missing data with substituted values. When substituting for a data point, it is known as \"unit imputation\"; when substituting for a component of a data point, it is known as \"item imputation\". There are three main problems that missing data causes: missing data can introduce a substantial amount of bias, make the handling and analysis of the data more arduous, and create reductions in efficiency. Because missing data can create problems for analyzing data, imputation is seen as a way to avoid pitfalls involved with listwise deletion of cases that have missing values. That is to say, when one or more values are missing for a case, most statistical packages default to discarding any case that has a missing value, which may introduce bias or affect the representativeness of the results. Imputation preserves all cases by replacing missing data with an estimated value based on other available information. Once all missing values have been imputed, the data set can then be analysed using standard techniques for complete data. Imputation theory is constantly developing and thus requires consistent attention to new information regarding the subject. There have been many theories embraced by scientists to account for missing data but the majority of them introduce large amounts of bias. A few of the well known attempts to deal with missing data include: hot deck and cold deck imputation; listwise and pairwise deletion; mean imputation; regression imputation; last observation carried forward; stochastic imputation; and multiple imputation. "
    },
    {
        "id": "TASK_incremental-learning",
        "name": "Incremental Learning",
        "description": "Incremental learning aims to develop artificially intelligent systems that can continuously learn to address new tasks from new data while preserving knowledge learned from previously learned tasks.",
        "type": "Task",
        "probable_wikipedia": "Incremental learning",
        "score": "11.834853",
        "probable_description": " In computer science, incremental learning is a method of machine learning in which input data is continuously used to extend the existing model's knowledge i.e. to further train the model. It represents a dynamic technique of supervised learning and unsupervised learning that can be applied when training data becomes available gradually over time or its size is out of system memory limits. Algorithms that can facilitate incremental learning are known as incremental machine learning algorithms.  Many traditional machine learning algorithms inherently support incremental learning. Other algorithms can be adapted to facilitate incremental learning. Examples of incremental algorithms include decision trees (IDE4, ID5R), decision rules, artificial neural networks (RBF networks, Learn++, Fuzzy ARTMAP, TopoART, and IGNG) or the incremental SVM.  The aim of incremental learning is for the learning model to adapt to new data without forgetting its existing knowledge, it does not retrain the model. Some incremental learners have built-in some parameter or assumption that controls the relevancy of old data, while others, called stable incremental machine learning algorithms, learn representations of the training data that are not even partially forgotten over time. Fuzzy ART and TopoART are two examples for this second approach.  Incremental algorithms are frequently applied to data streams or big data, addressing issues in data availability and resource scarcity respectively. Stock trend prediction and user profiling are some examples of data streams where new data becomes continuously available. Applying incremental learning to big data aims to produce faster classification or forecasting times.   "
    },
    {
        "id": "TASK_inductive-logic-programming",
        "name": "Inductive logic programming",
        "description": "",
        "type": "Task",
        "probable_wikipedia": "Inductive logic programming",
        "score": "11.939054",
        "probable_description": " Inductive logic programming (ILP) is a subfield of symbolic artificial intelligence which uses logic programming as a uniform representation for examples, background knowledge and hypotheses. Given an encoding of the known background knowledge and a set of examples represented as a logical database of facts, an ILP system will derive a hypothesised logic program which entails all the positive and none of the negative examples.   Inductive logic programming is particularly useful in bioinformatics and natural language processing. Gordon Plotkin and Ehud Shapiro laid the initial theoretical foundation for inductive machine learning in a logical setting. Shapiro built their first implementation (Model Inference System) in 1981: a Prolog program that inductively inferred logic programs from positive and negative examples. The term \"Inductive Logic Programming\" was first introduced in a paper by Stephen Muggleton in 1991. Muggleton also founded the annual international conference on Inductive Logic Programming, introduced the theoretical ideas of Predicate Invention, Inverse resolution, and Inverse entailment. Muggleton implemented Inverse entailment first in the PROGOL system. The term \"\"inductive\"\" here refers to philosophical (i.e. suggesting a theory to explain observed facts) rather than mathematical (i.e. proving a property for all members of a well-ordered set) induction. "
    },
    {
        "id": "TASK_industrial-robots",
        "name": "Industrial Robots",
        "description": "An industrial robot is a robot system used for manufacturing. Industrial robots are automated, programmable and capable of movement on three or more axes.",
        "type": "Task",
        "probable_wikipedia": "Industrial robot",
        "score": "11.020012",
        "probable_description": " An industrial robot is a robot system used for manufacturing. Industrial robots are automated, programmable and capable of movement on three or more axis.  Typical applications of robots include welding, painting, assembly, pick and place for printed circuit boards, packaging and labeling, palletizing, product inspection, and testing; all accomplished with high endurance, speed, and precision. They can assist in material handling.  In the year 2015, an estimated 1.64 million industrial robots were in operation worldwide according to International Federation of Robotics (IFR). "
    },
    {
        "id": "TASK_inference-attack",
        "name": "Inference Attack",
        "description": "",
        "type": "Task",
        "probable_wikipedia": "Inference attack",
        "score": "12.02492",
        "probable_description": " An Inference Attack is a data mining technique performed by analyzing data in order to illegitimately gain knowledge about a subject or database. A subject's sensitive information can be considered as leaked if an adversary can infer its real value with a high confidence. This is an example of breached information security. An Inference attack occurs when a user is able to infer from trivial information more robust information about a database without directly accessing it. The object of Inference attacks is to piece together information at one security level to determine a fact that should be protected at a higher security level.  "
    },
    {
        "id": "TASK_information-extraction",
        "name": "Information Extraction",
        "description": "Information extraction is the task of automatically extracting structured information from unstructured and / or semi-structured machine-readable documents and other electronically represented sources (Source: Wikipedia).",
        "type": "Task",
        "probable_wikipedia": "Information extraction",
        "score": "12.275441",
        "probable_description": " Information extraction (IE) is the task of automatically extracting structured information from unstructured and/or semi-structured machine-readable documents. In most of the cases this activity concerns processing human language texts by means of natural language processing (NLP). Recent activities in multimedia document processing like automatic annotation and content extraction out of images/audio/video/documents could be seen as information extraction  Due to the difficulty of the problem, current approaches to IE focus on narrowly restricted domains. An example is the extraction from newswire reports of corporate mergers, such as denoted by the formal relation: from an online news sentence such as:  A broad goal of IE is to allow computation to be done on the previously unstructured data. A more specific goal is to allow logical reasoning to draw inferences based on the logical content of the input data. Structured data is semantically well-defined data from a chosen target domain, interpreted with respect to category and context.  Information Extraction is the part of a greater puzzle which deals with the problem of devising automatic methods for text management, beyond its transmission, storage and display. The discipline of information retrieval (IR) has developed automatic methods, typically of a statistical flavor, for indexing large document collections and classifying documents. Another complementary approach is that of natural language processing (NLP) which has solved the problem of modelling human language processing with considerable success when taking into account the magnitude of the task. In terms of both difficulty and emphasis, IE deals with tasks in between both IR and"
    },
    {
        "id": "TASK_information-retrieval",
        "name": "Information Retrieval",
        "description": "Information retrieval is the task of ranking a list of documents or search results in response to a query\r\n\r\n<span style=\"color:grey; opacity: 0.6\">( Image credit: [sudhanshumittal](https://github.com/sudhanshumittal/Information-retrieval-system) )</span>",
        "type": "Task",
        "probable_wikipedia": "Information retrieval",
        "score": "10.852772",
        "probable_description": " Information retrieval (IR) is the activity of obtaining information system resources that are relevant to an information need from a collection of those resources. Searches can be based on full-text or other content-based indexing. Information retrieval is the science of searching for information in a document, searching for documents themselves, and also searching for the metadata that describes data, and for databases of texts, images or sounds.  Automated information retrieval systems are used to reduce what has been called information overload. An IR system is a software system that provides access to books, journals and other documents; stores and manages those documents. Web search engines are the most visible IR applications. "
    },
    {
        "id": "TASK_interest-point-detection",
        "name": "Interest Point Detection",
        "description": "",
        "type": "Task",
        "probable_wikipedia": "Interest point detection",
        "score": "10.099195",
        "probable_description": " Interest point detection is a recent terminology in computer vision that refers to the detection of interest points for subsequent processing. An interest point is a point in the image which in general can be characterized as follows:   Historically, the notion of interest points goes back to the earlier notion of corner detection, where corner features were in early work detected with the primary goal of obtaining robust, stable and well-defined image features for object tracking and recognition of three-dimensional CAD-like objects from two-dimensional images. In practice, however, most corner detectors are sensitive not specifically to corners, but to local image regions which have a high degree of variation in all directions. The use of interest points also goes back to the notion of regions of interest, which have been used to signal the presence of objects, often formulated in terms of the output of a blob detection step. While blob detectors have not always been included within the class of interest point operators, there is no rigorous reason for excluding blob descriptors from this class. For the most common types of blob detectors (see the article on blob detection), each blob descriptor has a well-defined point, which may correspond to a local maximum, a local maximum in the operator response or a centre of gravity of a non-infinitesimal region. In all other respects, the blob descriptors also satisfy the criteria of an interest point defined above. "
    },
    {
        "id": "TASK_iris-recognition",
        "name": "Iris Recognition",
        "description": "",
        "type": "Task",
        "probable_wikipedia": "Iris recognition",
        "score": "11.733062",
        "probable_description": " Iris recognition is an automated method of biometric identification that uses mathematical pattern-recognition techniques on video images of one or both of the irises of an individual's eyes, whose complex patterns are unique, stable, and can be seen from some distance.  Retinal scanning is a different, ocular-based biometric technology that uses the unique patterns on a person's retina blood vessels and is often confused with iris recognition. Iris recognition uses video camera technology with subtle near infrared illumination to acquire images of the detail-rich, intricate structures of the iris which are visible externally. Digital templates encoded from these patterns by mathematical and statistical algorithms allow the identification of an individual or someone pretending to be that individual. Databases of enrolled templates are searched by matcher engines at speeds measured in the millions of templates per second per (single-core) CPU, and with remarkably low false match rates.  Several hundred million persons in several countries around the world have been enrolled in iris recognition systems for convenience purposes such as passport-free automated border-crossings and some national ID programs. A key advantage of iris recognition, besides its speed of matching and its extreme resistance to false matches, is the stability of the iris as an internal and protected, yet externally visible organ of the eye. "
    },
    {
        "id": "TASK_jsoniq-query-execution",
        "name": "JSONiq Query Execution",
        "description": "Execute JSONiq query, typically on semi-structured JSON data",
        "type": "Task",
        "probable_wikipedia": "JSONiq",
        "score": "2.028126",
        "probable_description": " JSONiq is a query and functional programming language that is designed to declaratively query and transform collections of hierarchical and heterogeneous data in format of JSON, XML, as well as unstructured, textual data.  JSONiq is an open specification published under the Creative Commons Attribution-ShareAlike 3.0 license. It is based on the XQuery language, with which it shares the same core expressions and operations on atomic types. JSONiq comes in two syntactical flavors, which both support JSON and XML natively.  "
    },
    {
        "id": "TASK_keyword-extraction",
        "name": "Keyword Extraction",
        "description": "Keyword extraction is tasked with the automatic identification of terms that best describe the subject of a document (Source: Wikipedia).",
        "type": "Task",
        "probable_wikipedia": "Keyword extraction",
        "score": "12.558048",
        "probable_description": " Keyword extraction is tasked with the automatic identification of terms that best describe the subject of a document. \"Key phrases\", \"key terms\", \"key segments\" or just \"keywords\" are the terminology which is used for defining the terms that represent the most relevant information contained in the document. Although the terminology is different, function is the same: characterization of the topic discussed in a document. The task of keyword extraction is an important problem in Text Mining, Information Retrieval and Natural Language Processing. "
    },
    {
        "id": "TASK_keyword-spotting",
        "name": "Keyword Spotting",
        "description": "In speech processing, keyword spotting deals with the identification of keywords in utterances.\r\n\r\n<span style=\"color:grey; opacity: 0.6\">( Image credit: [Simon Grest](https://github.com/simongrest/kaggle-freesound-audio-tagging-2019) )</span>",
        "type": "Task",
        "probable_wikipedia": "Keyword spotting",
        "score": "12.577144",
        "probable_description": " Keyword spotting (or more simply, word spotting) is a problem that was historically first defined in the context of speech processing . In speech processing, keyword spotting deals with the identification of keywords in utterances.  Keyword spotting is also defined as a separate, but related, problem in the context of document image processing . In document image processing, keyword spotting is the problem of finding all instances of a query word that exist in a scanned document image, without fully recognizing it. "
    },
    {
        "id": "TASK_kidney-function",
        "name": "Kidney Function",
        "description": "Continuous prediction of urine production in the next 2h as an average rate in ml/kg/h. The task is predicted at irregular intervals.",
        "type": "Task",
        "probable_wikipedia": "Renal function",
        "score": "11.353601",
        "probable_description": " Renal function, in nephrology, is an indication of the kidney's condition and its role in renal physiology. Glomerular filtration rate (GFR) describes the flow rate of filtered fluid through the kidney. Creatinine clearance rate (C or CrCl) is the volume of blood plasma that is cleared of creatinine per unit time and is a useful measure for approximating the GFR. Creatinine clearance exceeds GFR due to creatinine secretion, which can be blocked by cimetidine. In alternative fashion, overestimation by older serum creatinine methods resulted in an underestimation of creatinine clearance, which provided a less biased estimate of GFR. Both GFR and C may be accurately calculated by comparative measurements of substances in the blood and urine, or estimated by formulas using just a blood test result (eGFR and eC).  The results of these tests are used to assess the excretory function of the kidneys. Staging of chronic kidney disease is based on categories of GFR as well as albuminuria and cause of kidney disease. Dosage of drugs that are excreted primarily via urine may need to be modified based on either GFR or creatinine clearance. "
    },
    {
        "id": "TASK_klondike",
        "name": "Klondike",
        "description": "The most commonly played game in the family of Solitaire card games.",
        "type": "Task",
        "probable_wikipedia": "Klondike (solitaire)",
        "score": "10.285175",
        "probable_description": " Klondike (North America) or Canfield (traditional) is a patience game (solitaire card game). In the U.S. and Canada, Klondike is the best-known solitaire card game, to the point that the term \"solitaire\", in the absence of additional qualifiers, typically refers to Klondike. Equally in the UK, it is often just known as \"patience\". Meanwhile, elsewhere the game is known as American Patience. The game rose to fame in the late 19th century, being named \"Klondike\" after the Canadian region where a gold rush happened. It is rumored that the game was either created or popularized by the prospectors in Klondike. "
    },
    {
        "id": "TASK_knowledge-graphs",
        "name": "Knowledge Graphs",
        "description": "",
        "type": "Task",
        "probable_wikipedia": "Knowledge Graph",
        "score": "9.003076",
        "probable_description": " The Knowledge Graph is a knowledge base used by Google and its services to enhance its search engine's results with information gathered from a variety of sources. The information is presented to users in an infobox next to the search results. Knowledge Graph infoboxes were added to Google's search engine in May 2012, starting in the United States, with international expansion by the end of the year. The information covered by the Knowledge Graph grew significantly after launch, tripling its size within seven months (covering 570 million entities and 18 billion facts) and answering \"roughly one-third\" of the 100 billion monthly searches Google processed in May 2016. The Knowledge Graph has been criticized for providing answers without source attribution or citation.  Information from the Knowledge Graph is presented as a box, which Google has referred to as the \"knowledge panel\", to the right (top on mobile) of search results. According to Google, this information is retrieved from many sources, including the \"CIA World Factbook\", Wikidata, and Wikipedia. In October 2016, Google announced that the Knowledge Graph held over 70 billion facts. There is no official documentation on the technology used for the Knowledge Graph implementation.  Information from the Knowledge Graph is used to answer direct spoken questions in Google Assistant and Google Home voice queries. "
    },
    {
        "id": "TASK_knowledge-graphs-data-curation",
        "name": "Knowledge Graphs Data Curation",
        "description": "",
        "type": "Task",
        "probable_wikipedia": "Knowledge Graph",
        "score": "0.92900985",
        "probable_description": " The Knowledge Graph is a knowledge base used by Google and its services to enhance its search engine's results with information gathered from a variety of sources. The information is presented to users in an infobox next to the search results. Knowledge Graph infoboxes were added to Google's search engine in May 2012, starting in the United States, with international expansion by the end of the year. The information covered by the Knowledge Graph grew significantly after launch, tripling its size within seven months (covering 570 million entities and 18 billion facts) and answering \"roughly one-third\" of the 100 billion monthly searches Google processed in May 2016. The Knowledge Graph has been criticized for providing answers without source attribution or citation.  Information from the Knowledge Graph is presented as a box, which Google has referred to as the \"knowledge panel\", to the right (top on mobile) of search results. According to Google, this information is retrieved from many sources, including the \"CIA World Factbook\", Wikidata, and Wikipedia. In October 2016, Google announced that the Knowledge Graph held over 70 billion facts. There is no official documentation on the technology used for the Knowledge Graph implementation.  Information from the Knowledge Graph is used to answer direct spoken questions in Google Assistant and Google Home voice queries. "
    },
    {
        "id": "TASK_knowledge-tracing",
        "name": "Knowledge Tracing",
        "description": "**Knowledge Tracing** is the task of modelling student knowledge over time so that we can accurately predict how students will perform on future interactions. Improvement on this task means that resources can be suggested to students based on their individual needs, and content which is predicted to be too easy or too hard can be skipped or delayed.\n\n\n<span class=\"description-source\">Source: [Deep Knowledge Tracing ](https://arxiv.org/abs/1506.05908)</span>",
        "type": "Task",
        "probable_wikipedia": "Bayesian Knowledge Tracing",
        "score": "1.2186273",
        "probable_description": " Bayesian Knowledge Tracing is an algorithm used in many intelligent tutoring systems to model each learner's mastery of the knowledge being tutored.  It models student knowledge in a Hidden Markov Model as a latent variable, updated by observing the correctness of each student's interaction in which they apply the skill in question.  BKT assumes that student knowledge is represented as a set of binary variables, one per skill, where the skill is either mastered by the student or not. Observations in BKT are also binary: a student gets a problem/step either right or wrong. Intelligent tutoring systems often use BKT for mastery learning and problem sequencing. In its most common implementation, BKT has only skill-specific parameters. "
    },
    {
        "id": "TASK_landmine",
        "name": "Landmine",
        "description": "",
        "type": "Task",
        "probable_wikipedia": "Land mine",
        "score": "10.4219675",
        "probable_description": " A land mine is an explosive device concealed under or on the ground and designed to destroy or disable enemy targets, ranging from combatants to vehicles and tanks, as they pass over or near it. Such a device is typically detonated automatically by way of pressure when a target steps on it or drives over it, although other detonation mechanisms are also sometimes used. A land mine may cause damage by direct blast effect, by fragments that are thrown by the blast, or by both.  The use of land mines is controversial because of their potential as indiscriminate weapons. They can remain dangerous many years after a conflict has ended, harming civilians and the economy. 78 countries are contaminated with land mines and 15,000\u201320,000 people are killed every year while countless more are maimed. Approximately 80% of land mine casualties are civilian, with children as the most affected age group. Most killings occur in times of peace. With pressure from a number of campaign groups organised through the International Campaign to Ban Landmines, a global movement to prohibit their use led to the 1997 Convention on the Prohibition of the Use, Stockpiling, Production and Transfer of Anti-Personnel Mines and on their Destruction, also known as the \"Ottawa Treaty\". To date, 164 nations have signed the treaty the notable exceptions of, \"inter alia\", China, the Russian Federation, and the United States. "
    },
    {
        "id": "TASK_language-acquisition",
        "name": "Language Acquisition",
        "description": "Language acquisition refers to tasks related to the learning of a second language.",
        "type": "Task",
        "probable_wikipedia": "Language acquisition",
        "score": "10.209902",
        "probable_description": " Language acquisition is the process by which humans acquire the capacity to perceive and comprehend language (in other words, gain the ability to be aware of language and to understand it), as well as to produce and use words and sentences to communicate.  Language acquisition involves structures, rules and representation. The capacity to successfully use language requires one to acquire a range of tools including phonology, morphology, syntax, semantics, and an extensive vocabulary. Language can be vocalized as in speech, or manual as in sign. Human language capacity is represented in the brain. Even though human language capacity is finite, one can say and understand an infinite number of sentences, which is based on a syntactic principle called recursion. Evidence suggests that every individual has three recursive mechanisms that allow sentences to go indeterminately. These three mechanisms are: \"relativization\", \"complementation\" and \"coordination\".  There are two main guiding principles in first-language acquisition: speech perception always precedes speech production and the gradually evolving system by which a child learns a language is built up one step at a time, beginning with the distinction between individual phonemes.  Linguists who are interested in child language acquisition for many years question how language is acquired, Lidz et al. states \"The question of how these structures are acquired, then, is more properly understood as the question of how a learner takes the surface forms in the input and converts them into abstract linguistic rules and representations.\"  Language acquisition usually refers to first-language acquisition, which studies infants' acquisition"
    },
    {
        "id": "TASK_language-identification",
        "name": "Language Identification",
        "description": "Language identification is the task of determining the language of a text.",
        "type": "Task",
        "probable_wikipedia": "Language identification",
        "score": "11.802384",
        "probable_description": " In natural language processing, language identification or language guessing is the problem of determining which natural language given content is in. Computational approaches to this problem view it as a special case of text categorization, solved with various statistical methods. "
    },
    {
        "id": "TASK_league-of-legends",
        "name": "League of Legends",
        "description": "",
        "type": "Task",
        "probable_wikipedia": "League of Legends",
        "score": "11.680583",
        "probable_description": " League of Legends (abbreviated LoL) is a multiplayer online battle arena video game developed and published by Riot Games for Microsoft Windows and macOS. The game follows a freemium model and is supported by microtransactions, and was inspired by the \"\" mod, \"Defense of the Ancients\".  In \"League of Legends\", players assume the role of an unseen \"summoner\" that controls a \"champion\" with unique abilities and battle against a team of other players or computer-controlled champions. The goal is usually to destroy the opposing team's \"Nexus\", a structure that lies at the heart of a base protected by defensive structures, although other distinct game modes exist as well. Each \"League of Legends\" match is discrete, with all champions starting off fairly weak but increases in strength by accumulating items and experience over the course of the game. The champions and setting blend a variety of elements, including high fantasy, steampunk, and Lovecraftian horror.  \"League of Legends\" was generally well received upon its release in 2009, and has since grown in popularity, with an active and expansive fanbase. By July 2012, \"League of Legends\" was the most played PC game in North America and Europe in terms of the number of hours played. In January 2014, over 67 million people played \"League of Legends\" per month, 27 million per day, and over 7.5 million concurrently during peak hours. \"League\" has among the largest footprints of any game in streaming media communities on platforms such as YouTube and Twitch.tv; it routinely ranks first in the"
    },
    {
        "id": "TASK_learning-theory",
        "name": "Learning Theory",
        "description": "Learning theory",
        "type": "Task",
        "probable_wikipedia": "Learning theory (education)",
        "score": "5.3142424",
        "probable_description": " Learning Theory describe how students absorb, process, and retain knowledge during learning. Cognitive, emotional, and environmental influences, as well as prior experience, all play a part in how understanding, or a world view, is acquired or changed and knowledge and skills retained.  Behaviorists look at learning as an aspect of conditioning and advocate a system of rewards and targets in education. Educators who embrace cognitive theory believe that the definition of learning as a change in behaviour is too narrow, and study the learner rather than their environment\u2014and in particular the complexities of human memory. Those who advocate constructivism believe that a learner's ability to learn relies largely on what they already know and understand, and the acquisition of knowledge should be an individually tailored process of construction. Transformative learning theory focuses on the often-necessary change required in a learner's preconceptions and world view. Geographical learning theory focuses on the ways that contexts and environments shape the learning process.  Outside the realm of educational psychology, techniques to directly observe the functioning of the brain during the learning process, such as event-related potential and functional magnetic resonance imaging, are used in educational neuroscience. The theory of multiple intelligences, where learning is seen as the interaction between dozens of different functional areas in the brain each with their own individual strengths and weaknesses in any particular human learner, has also been proposed, but empirical research has found the theory to be unsupported by evidence. "
    },
    {
        "id": "TASK_learning-to-rank",
        "name": "Learning-To-Rank",
        "description": "Learning to rank is the application of machine learning to build ranking models. Some common use cases for ranking models are information retrieval (e.g., web search) and news feeds application (think Twitter, Facebook, Instagram).",
        "type": "Task",
        "probable_wikipedia": "Learning to rank",
        "score": "12.088839",
        "probable_description": " Learning to rank or machine-learned ranking (MLR) is the application of machine learning, typically supervised, semi-supervised or reinforcement learning, in the construction of ranking models for information retrieval systems. Training data consists of lists of items with some partial order specified between items in each list. This order is typically induced by giving a numerical or ordinal score or a binary judgment (e.g. \"relevant\" or \"not relevant\") for each item. The ranking model's purpose is to rank, i.e. produce a permutation of items in new, unseen lists in a way which is \"similar\" to rankings in the training data in some sense. "
    },
    {
        "id": "TASK_legged-robots",
        "name": "Legged Robots",
        "description": "",
        "type": "Task",
        "probable_wikipedia": "Legged robot",
        "score": "10.643716",
        "probable_description": " Legged robots are a type of mobile robot, which use articulated limbs, such as leg mechanisms, to provide locomotion. They are more versatile than wheeled robots and can traverse many different terrains, though these advantages require increased complexity and power consumption. Legged robots often imitate legged animals, such as humans or insects, in an example of biomimicry."
    },
    {
        "id": "TASK_lemmatization",
        "name": "Lemmatization",
        "description": "**Lemmatization** is a process of determining a base or dictionary form (lemma) for a given surface form. Especially for languages with rich morphology it is important to be able to normalize words into their base forms to better support for example search engines and linguistic studies. Main difficulties in Lemmatization arise from encountering previously unseen words during inference time as well as disambiguating ambiguous surface forms which can be inflected variants of several different base forms depending on the context.\n\n\n<span class=\"description-source\">Source: [Universal Lemmatizer: A Sequence to Sequence Model for Lemmatizing Universal Dependencies Treebanks ](https://arxiv.org/abs/1902.00972)</span>",
        "type": "Task",
        "probable_wikipedia": "Lemmatisation",
        "score": "6.205808",
        "probable_description": " Lemmatisation (or lemmatization) in linguistics is the process of grouping together the inflected forms of a word so they can be analysed as a single item, identified by the word's lemma, or dictionary form.  In computational linguistics, lemmatisation is the algorithmic process of determining the lemma of a word based on its intended meaning. Unlike stemming, lemmatisation depends on correctly identifying the intended part of speech and meaning of a word in a sentence, as well as within the larger context surrounding that sentence, such as neighboring sentences or even an entire document. As a result, developing efficient lemmatisation algorithms is an open area of research. "
    },
    {
        "id": "TASK_lexical-analysis",
        "name": "Lexical Analysis",
        "description": "Lexical analysis is the process of converting a sequence of characters into a sequence of tokens (strings with an assigned and thus identified meaning). (Source: Adapted from Wikipedia)",
        "type": "Task",
        "probable_wikipedia": "Lexical analysis",
        "score": "12.091372",
        "probable_description": " In computer science, lexical analysis, lexing or tokenization is the process of converting a sequence of characters (such as in a computer program or web page) into a sequence of tokens (strings with an assigned and thus identified meaning). A program that performs lexical analysis may be termed a \"lexer\", \"tokenizer\", or \"scanner\", though \"scanner\" is also a term for the first stage of a lexer. A lexer is generally combined with a parser, which together analyze the syntax of programming languages, web pages, and so forth. "
    },
    {
        "id": "TASK_lexical-simplification",
        "name": "Lexical Simplification",
        "description": "The goal of **Lexical Simplification** is to replace complex words (typically words that are used less often in language and are therefore less familiar to readers) with their simpler synonyms, without infringing the grammaticality and changing the meaning of the text.\n\n\n<span class=\"description-source\">Source: [Adversarial Propagation and Zero-Shot Cross-Lingual Transfer of Word Vector Specialization ](https://arxiv.org/abs/1809.04163)</span>",
        "type": "Task",
        "probable_wikipedia": "Lexical simplification",
        "score": "11.263526",
        "probable_description": " Lexical simplification is a sub-task of text simplification. It can be defined as any lexical substitution task that reduce text complexity.     "
    },
    {
        "id": "TASK_lightfield",
        "name": "Lightfield",
        "description": "Tasks related to the light-field imagery",
        "type": "Task",
        "probable_wikipedia": "Light field",
        "score": "1.1023496",
        "probable_description": " The light field is a vector function that describes the amount of light flowing in every direction through every point in space. The space of all possible light rays is given by the five-dimensional plenoptic function, and the magnitude of each ray is given by the radiance. Michael Faraday was the first to propose (in an 1846 lecture entitled \"Thoughts on Ray Vibrations\") that light should be interpreted as a field, much like the magnetic fields on which he had been working for several years. The phrase \"light field\" was coined by Andrey Gershun in a classic paper on the radiometric properties of light in three-dimensional space (1936). "
    },
    {
        "id": "TASK_line-detection",
        "name": "Line Detection",
        "description": "",
        "type": "Task",
        "probable_wikipedia": "Line detection",
        "score": "11.809154",
        "probable_description": " In image processing, line detection is an algorithm that takes a collection of n edge points and finds all the lines on which these edge points lie. The most popular line detectors are the Hough transform and convolution based techniques. "
    },
    {
        "id": "TASK_lipreading",
        "name": "Lipreading",
        "description": "",
        "type": "Task",
        "probable_wikipedia": "Lip reading",
        "score": "8.806586",
        "probable_description": " Lip reading, also known as lipreading or speechreading, is a technique of understanding speech by visually interpreting the movements of the lips, face and tongue when normal sound is not available. It relies also on information provided by the context, knowledge of the language, and any residual hearing. Although ostensibly used by deaf and hard-of-hearing people, most people with normal hearing process some speech information from sight of the moving mouth. "
    },
    {
        "id": "TASK_lip-reading",
        "name": "Lip Reading",
        "description": "**Lip Reading** is a task to infer the speech content in a video by using only the visual information, especially the lip movements. It has many crucial applications in practice, such as assisting audio-based speech recognition, biometric authentication and aiding hearing-impaired people.\n\n\n<span class=\"description-source\">Source: [Mutual Information Maximization for Effective Lip Reading ](https://arxiv.org/abs/2003.06439)</span>",
        "type": "Task",
        "probable_wikipedia": "Lip reading",
        "score": "12.345089",
        "probable_description": " Lip reading, also known as lipreading or speechreading, is a technique of understanding speech by visually interpreting the movements of the lips, face and tongue when normal sound is not available. It relies also on information provided by the context, knowledge of the language, and any residual hearing. Although ostensibly used by deaf and hard-of-hearing people, most people with normal hearing process some speech information from sight of the moving mouth. "
    },
    {
        "id": "TASK_liver-segmentation",
        "name": "Liver Segmentation",
        "description": "",
        "type": "Task",
        "probable_wikipedia": "Liver segment",
        "score": "3.845181",
        "probable_description": " In the widely used Couinaud (or \"French\") system of anatomy of the liver, the functional lobes are further divided into a total of eight subsegments based on a transverse plane through the bifurcation of the main portal vein. The system is named after Claude Couinaud. "
    },
    {
        "id": "TASK_lung-nodule-classification",
        "name": "Lung Nodule Classification",
        "description": "",
        "type": "Task",
        "probable_wikipedia": "Lung nodule",
        "score": "2.9665844",
        "probable_description": " A lung nodule or pulmonary nodule is a relatively small focal density in the lung. A solitary pulmonary nodule (SPN) or coin lesion, is a mass in the lung smaller than 3 centimeters in diameter. There may also be multiple nodules.  One or more lung nodules can be an incidental finding found in up to 0.2% of chest X-rays and around 1% of CT scans.  The nodule most commonly represents a benign tumor such as a granuloma or hamartoma, but in around 20% of cases it represents a malignant cancer, especially in older adults and smokers. Conversely, 10 to 20% of patients with lung cancer are diagnosed in this way. If the patient has a history of smoking or the nodule is growing, the possibility of cancer may need to be excluded through further radiological studies and interventions, possibly including surgical resection. The prognosis depends on the underlying condition. "
    },
    {
        "id": "TASK_machine-translation",
        "name": "Machine Translation",
        "description": "Machine translation is the task of translating a sentence in a source language to a different target language.\r\n\r\nApproaches for machine translation can range from rule-based to statistical to neural-based. More recently, encoder-decoder attention-based architectures like BERT have attained major improvements in machine translation. \r\n\r\nOne of the most popular datasets used to benchmark machine translation systems is the WMT family of datasets. Some of the most commonly used evaluation metrics for machine translation systems include BLEU, METEOR, NIST, and others. \r\n\r\n<span style=\"color:grey; opacity: 0.6\">( Image credit: [Google seq2seq](https://github.com/google/seq2seq) )</span>",
        "type": "Task",
        "probable_wikipedia": "Machine translation",
        "score": "10.87469",
        "probable_description": " Machine translation, sometimes referred to by the abbreviation MT (not to be confused with computer-aided translation, machine-aided human translation (MAHT) or interactive translation) is a sub-field of computational linguistics that investigates the use of software to translate text or speech from one language to another.  On a basic level, MT performs simple substitution of words in one language for words in another, but that alone usually cannot produce a good translation of a text because recognition of whole phrases and their closest counterparts in the target language is needed. Solving this problem with corpus statistical, and neural techniques is a rapidly growing field that is leading to better translations, handling differences in linguistic typology, translation of idioms, and the isolation of anomalies.  Current machine translation software often allows for customization by domain or profession (such as weather reports), improving output by limiting the scope of allowable substitutions. This technique is particularly effective in domains where formal or formulaic language is used. It follows that machine translation of government and legal documents more readily produces usable output than conversation or less standardised text.  Improved output quality can also be achieved by human intervention: for example, some systems are able to translate more accurately if the user has unambiguously identified which words in the text are proper names. With the assistance of these techniques, MT has proven useful as a tool to assist human translators and, in a very limited number of cases, can even produce output that can be used as is (e.g.,"
    },
    {
        "id": "TASK_malware-analysis",
        "name": "Malware Analysis",
        "description": "",
        "type": "Task",
        "probable_wikipedia": "Malware analysis",
        "score": "11.478573",
        "probable_description": " Malware analysis is the study or process of determining the functionality, origin and potential impact of a given malware sample such as a virus, worm, trojan horse, rootkit, or backdoor. Malware or malicious software is any computer software intended to harm the host operating system or to steal sensitive data from users, organizations or companies. Malware may include software that gathers user information without permission. "
    },
    {
        "id": "TASK_manufacturing-quality-control",
        "name": "Manufacturing Quality Control",
        "description": "AI for Quality control in manufacturing processes.",
        "type": "Task",
        "probable_wikipedia": "Quality control",
        "score": "7.32609",
        "probable_description": " Quality control (QC) is a process by which entities review the quality of all factors involved in production. ISO 9000 defines quality control as \"A part of quality management focused on fulfilling quality requirements\".  This approach places on a emphasis on three aspects (enshrined in standards such as ISO 9001):   Inspection is a major component of quality control, where physical product is examined visually (or the end results of a service are analyzed). Product inspectors will be provided with lists and descriptions of unacceptable product defects such as cracks or surface blemishes for example.  The quality of the outputs is at risk if any of these three aspects is deficient in any way. "
    },
    {
        "id": "TASK_manufacturing-simulation",
        "name": "Manufacturing simulation",
        "description": "Simulation of manufacturing system for applying AI methods and big data analysis",
        "type": "Task",
        "probable_wikipedia": "Simulation in manufacturing systems",
        "score": "5.048738",
        "probable_description": " Simulation in manufacturing systems is the use of software to make computer models of manufacturing systems, so to analyze them and thereby obtain important information. It has been syndicated as the second most popular management science among manufacturing managers. However, its use has been limited due to the complexity of some software packages, and to the lack of preparation some users have in the fields of probability and statistics.  This technique represents a valuable tool used by engineers when evaluating the effect of capital investment in equipment and physical facilities like factory plants, warehouses, and distribution centers. Simulation can be used to predict the performance of an existing or planned system and to compare alternative solutions for a particular design problem. "
    },
    {
        "id": "TASK_mathematical-proofs",
        "name": "Mathematical Proofs",
        "description": "",
        "type": "Task",
        "probable_wikipedia": "Mathematical proof",
        "score": "10.206585",
        "probable_description": " A mathematical proof is an inferential argument for a mathematical statement. In the argument, other previously established statements, such as theorems, can be used. In principle, a proof can be traced back to self-evident or assumed statements, known as axioms, along with accepted rules of inference. Axioms may be treated as conditions that must be met before the statement applies. Proofs are examples of exhaustive deductive reasoning or inductive reasoning and are distinguished from empirical arguments or non-exhaustive inductive reasoning (or \"reasonable expectation\"). A proof must demonstrate that a statement is always true (occasionally by listing \"all\" possible cases and showing that it holds in each), rather than enumerate many confirmatory cases. An unproved proposition that is believed to be true is known as a conjecture.  Proofs employ logic but usually include some amount of natural language which usually admits some ambiguity. In fact, the vast majority of proofs in written mathematics can be considered as applications of rigorous informal logic. Purely formal proofs, written in symbolic language instead of natural language, are considered in proof theory. The distinction between formal and informal proofs has led to much examination of current and historical mathematical practice, quasi-empiricism in mathematics, and so-called folk mathematics (in both senses of that term). The philosophy of mathematics is concerned with the role of language and logic in proofs, and mathematics as a language. "
    },
    {
        "id": "TASK_matrix-completion",
        "name": "Matrix Completion",
        "description": "**Matrix Completion** is a method for recovering lost information. It originates from machine learning and usually deals with highly sparse matrices. Missing or unknown data is estimated using the low-rank matrix of the known data.\n\n\n<span class=\"description-source\">Source: [A Fast Matrix-Completion-Based Approach for Recommendation Systems ](https://arxiv.org/abs/1912.00600)</span>",
        "type": "Task",
        "probable_wikipedia": "Matrix completion",
        "score": "10.729774",
        "probable_description": " Matrix completion is the task of filling in the missing entries of a partially observed matrix. A wide range of datasets are naturally organized in matrix form. One example is the movie-ratings matrix, as appears in the Netflix problem: Given a ratings matrix in which each entry formula_1 represents the rating of movie formula_2 by customer formula_3 if customer formula_3 has watched movie formula_2 and is otherwise missing, we would like to predict the remaining entries in order to make good recommendations to customers on what to watch next. Another example is the term-document matrix: The frequencies of words used in a collection of documents can be represented as a matrix, where each entry corresponds to the number of times the associated term appears in the indicated document.  Without any restrictions on the number of degrees of freedom in the completed matrix this problem is underdetermined since the hidden entries could be assigned arbitrary values. Thus matrix completion often seeks to find the lowest rank matrix or, if the rank of the completed matrix is known, a matrix of rank formula_6 that matches the known entries. The illustration shows that a partially revealed rank-1 matrix (on the left) can be completed with zero-error (on the right) since all the rows with missing entries should be the same as the third row. In the case of the Netflix problem the ratings matrix is expected to be low-rank since user preferences can often be described by a few factors, such as the movie genre and time"
    },
    {
        "id": "TASK_matting",
        "name": "Matting",
        "description": "",
        "type": "Task",
        "probable_wikipedia": "Matte (filmmaking)",
        "score": "5.159588",
        "probable_description": " Mattes are used in photography and special effects filmmaking to combine two or more image elements into a single, final image. Usually, mattes are used to combine a foreground image (e.g. actors on a set or a spaceship) with a background image (e.g. a scenic vista or a starfield with planets). In this case, the matte is the background painting. In film and stage, mattes can be physically huge sections of painted canvas, portraying large scenic expanses of landscapes.  In film, the principle of a matte requires masking certain areas of the film emulsion to selectively control which areas are exposed. However, many complex special-effects scenes have included dozens of discrete image elements, requiring very complex use of mattes, and layering mattes on top of one another. For an example of a simple matte, we may wish to depict a group of actors in front of a store, with a massive city and sky visible above the store's roof. We would have two images\u2014the actors on the set, and the image of the city\u2014to combine onto a third. This would require two masks/mattes. One would mask everything above the store's roof, and the other would mask everything below it. By using these masks/mattes when copying these images onto the third, we can combine the images without creating ghostly double-exposures. In film, this is an example of a static matte, where the shape of the mask does not change from frame to frame. Other shots may require mattes that change, to mask the shapes of"
    },
    {
        "id": "TASK_medical-diagnosis",
        "name": "Medical Diagnosis",
        "description": "**Medical Diagnosis** is the process of identifying the disease a patient is affected by, based on the assessment of specific risk factors, signs, symptoms and results of exams.\r\n\r\n\r\n<span class=\"description-source\">Source: [A probabilistic network for the diagnosis of acute cardiopulmonary diseases ](https://arxiv.org/abs/1609.06864)</span>",
        "type": "Task",
        "probable_wikipedia": "Medical diagnosis",
        "score": "12.267117",
        "probable_description": " Medical diagnosis (abbreviated Dx or D) is the process of determining which disease or condition explains a person's symptoms and signs. It is most often referred to as diagnosis with the medical context being implicit. The information required for diagnosis is typically collected from a history and physical examination of the person seeking medical care. Often, one or more diagnostic procedures, such as medical tests, are also done during the process. Sometimes posthumous diagnosis is considered a kind of medical diagnosis.  Diagnosis is often challenging, because many signs and symptoms are nonspecific. For example, redness of the skin (erythema), by itself, is a sign of many disorders and thus does not tell the healthcare professional what is wrong. Thus differential diagnosis, in which several possible explanations are compared and contrasted, must be performed. This involves the correlation of various pieces of information followed by the recognition and differentiation of patterns. Occasionally the process is made easy by a sign or symptom (or a group of several) that is pathognomonic.  Diagnosis is a major component of the procedure of a doctor's visit. From the point of view of statistics, the diagnostic procedure involves classification tests. "
    },
    {
        "id": "TASK_medical-procedure",
        "name": "Medical Procedure",
        "description": "Predicting medical procedures performed during a hospital admission.",
        "type": "Task",
        "probable_wikipedia": "Medical procedure",
        "score": "10.280197",
        "probable_description": " A medical procedure is a course of action intended to achieve a result in the delivery of healthcare.  A medical procedure with the intention of determining, measuring, or diagnosing a patient condition or parameter is also called a medical test. Other common kinds of procedures are therapeutic (i.e.,          "
    },
    {
        "id": "TASK_memex-question-answering",
        "name": "Memex Question Answering",
        "description": "Question answering with real-world multi-modal personal collections, e.g., photo albums with visual, text, time and location information.",
        "type": "Task",
        "probable_wikipedia": "Memex",
        "score": "1.411273",
        "probable_description": " The memex (originally coined \"at random\", though sometimes said to be a portmanteau of \"memory\" and \"index\") is the name of the hypothetical proto-hypertext system that Vannevar Bush described in his 1945 \"The Atlantic Monthly\" article \"As We May Think\". Bush envisioned the memex as a device in which individuals would compress and store all of their books, records, and communications, \"mechanized so that it may be consulted with exceeding speed and flexibility\". The memex would provide an \"enlarged intimate supplement to one's memory\". The concept of the memex influenced the development of early hypertext systems (eventually leading to the creation of the World Wide Web) and personal knowledge base software. The hypothetical implementation depicted by Bush for the purpose of concrete illustration was based upon a document bookmark list of static microfilm pages and lacked a true hypertext system, where parts of pages would have internal structure beyond the common textual format. Early electronic hypertext systems were thus inspired by memex rather than modeled directly upon it. "
    },
    {
        "id": "TASK_meta-learning",
        "name": "Meta-Learning",
        "description": "Meta-learning is a methodology considered with \"learning to learn\" machine learning algorithms.\r\n\r\n<span style=\"color:grey; opacity: 0.6\">( Image credit: [Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks](https://arxiv.org/pdf/1703.03400v3.pdf) )</span>",
        "type": "Task",
        "probable_wikipedia": "Meta learning",
        "score": "7.894109",
        "probable_description": " Meta learning is a branch of metacognition concerned with learning about one's own learning and learning processes.  The term comes from the meta prefix's modern meaning of an abstract recursion, or \"X about X,\" similar to its use in metaknowledge, metamemory, and meta-emotion. "
    },
    {
        "id": "TASK_metamerism",
        "name": "Metamerism",
        "description": "",
        "type": "Task",
        "probable_wikipedia": "Metamerism (biology)",
        "score": "3.8285341",
        "probable_description": " In biology, metamerism is the phenomenon of having a linear series of body segments fundamentally similar in structure, though not all such structures are entirely alike in any single life form because some of them perform special functions. In animals, metameric segments are referred to as somites or metameres. In plants, they are referred to as metamers or, more concretely, phytomers. "
    },
    {
        "id": "TASK_misinformation",
        "name": "Misinformation",
        "description": "",
        "type": "Task",
        "probable_wikipedia": "Misinformation",
        "score": "10.742178",
        "probable_description": " Misinformation is false or inaccurate information. Examples of misinformation include false rumors, insults and pranks, while examples of more deliberate disinformation include malicious content such as hoaxes, spearphishing and propaganda. News parody or satire may also become misinformation if it is taken as serious by the unwary and spread as if it were true. The terms \"misinformation\" and \"disinformation\" have been associated with the neologism \"Fake News,\" defined by some scholars as \u201cfabricated information that mimics news media content in form but not in organizational process or intent.\u201d "
    },
    {
        "id": "TASK_mixed-reality",
        "name": "Mixed Reality",
        "description": "",
        "type": "Task",
        "probable_wikipedia": "Mixed reality",
        "score": "12.242859",
        "probable_description": " Mixed reality (MR) is the merging of real and virtual worlds to produce new environments and visualizations where physical and digital objects co-exist and interact in real time. Mixed reality takes place not only in the physical world or the virtual world, but is a hybrid of reality and virtual reality, encompassing both augmented reality and augmented virtuality via immersive technology. The first immersive mixed reality system, providing enveloping sight, sound, and touch was the Virtual Fixtures platform developed at the U.S. Air Force's Armstrong Laboratories in the early 1990s. In a study published in 1992, the Virtual Fixtures project at the U.S. Air Force demonstrated for the first time that human performance could be significantly amplified by the introduction of spatially registered virtual objects overlaid on top of a person's direct view of a real physical environment. "
    },
    {
        "id": "TASK_mobile-security",
        "name": "Mobile Security",
        "description": "",
        "type": "Task",
        "probable_wikipedia": "Mobile security",
        "score": "10.908879",
        "probable_description": " Mobile security, or more specifically mobile device security, has become increasingly important in mobile computing. Of particular concern is the security of personal and business information now stored on smartphones.  More and more users and businesses use smartphones to communicate, but also to plan and organize their users' work and also private life. Within companies, these technologies are causing profound changes in the organization of information systems and therefore they have become the source of new risks. Indeed, smartphones collect and compile an increasing amount of sensitive information to which access must be controlled to protect the privacy of the user and the intellectual property of the company.  All smartphones, as computers, are preferred targets of attacks. These attacks exploit weaknesses inherent in smartphones that can come from the communication mode\u2014like Short Message Service (SMS, aka text messaging), Multimedia Messaging Service (MMS), WiFi, Bluetooth and GSM, the \"de facto\" global standard for mobile communications. There are also exploits that target software vulnerabilities in the browser or operating system. And some malicious software relies on the weak knowledge of an average user.  Security countermeasures are being developed and applied to smartphones, from security in different layers of software to the dissemination of information to end users. There are good practices to be observed at all levels, from design to use, through the development of operating systems, software layers, and downloadable apps. "
    },
    {
        "id": "TASK_model-selection",
        "name": "Model Selection",
        "description": "Given a set of candidate models, the goal of **Model Selection** is to select the model that best approximates the observed data and captures its underlying regularities. Model Selection criteria are defined such that they strike a balance between the goodness of fit, and the generalizability or complexity of the models.\n\n\n<span class=\"description-source\">Source: [Kernel-based Information Criterion ](https://arxiv.org/abs/1408.5810)</span>",
        "type": "Task",
        "probable_wikipedia": "Model selection",
        "score": "11.811856",
        "probable_description": " Model selection is the task of selecting a statistical model from a set of candidate models, given data. In the simplest cases, a pre-existing set of data is considered. However, the task can also involve the design of experiments such that the data collected is well-suited to the problem of model selection. Given candidate models of similar predictive or explanatory power, the simplest model is most likely to be the best choice (Occam's razor).  Model selection may also refer to the problem of selecting a few representative models from a large set of computational models for the purpose of decision making or optimization under uncertainty. "
    },
    {
        "id": "TASK_montezumas-revenge",
        "name": "Montezuma's Revenge",
        "description": "Montezuma's Revenge is an ATARI 2600 Benchmark game that is known to be difficult to perform on for reinforcement learning algorithms. Solutions typically employ algorithms that incentivise environment exploration in different ways.\r\n\r\nFor the state-of-the art tables, please consult the parent Atari Games task.\r\n\r\n<span style=\"color:grey; opacity: 0.6\">( Image credit: [Q-map](https://github.com/fabiopardo/qmap) )</span>",
        "type": "Task",
        "probable_wikipedia": "Montezuma's Revenge (video game)",
        "score": "9.601747",
        "probable_description": " Montezuma's Revenge is a 1984 platform game for Atari 8-bit computers, Atari 2600, Atari 5200, Apple II, ColecoVision, Commodore 64, IBM PC, and ZX Spectrum (as Panama Joe). It was designed and programmed by Robert Jaeger and published by Parker Brothers. The game's title references a colloquial expression for diarrhea contracted while visiting Mexico.  The game was ported to the Sega Master System in 1988. "
    },
    {
        "id": "TASK_morphological-analysis",
        "name": "Morphological Analysis",
        "description": "**Morphological Analysis** is a central task in language processing that can take a word as input and detect the various morphological entities in the word and provide a morphological representation of it.\n\n\n<span class=\"description-source\">Source: [Towards Finite-State Morphology of Kurdish ](https://arxiv.org/abs/2005.10652)</span>",
        "type": "Task",
        "probable_wikipedia": "Morphological analysis",
        "score": "5.9386106",
        "probable_description": " Morphological analysis is the analysis of morphology in various fields:   "
    },
    {
        "id": "TASK_morphological-inflection",
        "name": "Morphological Inflection",
        "description": "**Morphological Inflection** is the task of generating a target (inflected form) word from a source word (base form), given a morphological attribute, e.g. number, tense, and person etc. It is useful for alleviating data sparsity issues in translating morphologically rich languages. The transformation from a base form to an inflected form usually includes concatenating the base form with a prefix or a suffix and substituting some characters. For example, the inflected form of a Finnish stem el\u00e4keik\u00e4 (retirement age) is el\u00e4keiitt\u00e4 when the case is abessive and the number is plural.\n\n\n<span class=\"description-source\">Source: [Tackling Sequence to Sequence Mapping Problems with Neural Networks ](https://arxiv.org/abs/1810.10802)</span>",
        "type": "Task",
        "probable_wikipedia": "Inflection",
        "score": "1.0797967",
        "probable_description": " In morphology, inflection (or inflexion) is a process of word formation, in which a word is modified to express different grammatical categories such as tense, case, voice, aspect, person, number, gender, and mood. The inflection of verbs is called \"conjugation\", and one can refer to the inflection of nouns, adjectives, adverbs, pronouns, determiners, participles, prepositions and postpositions, numerals, articles etc., as \"declension\".  An inflection expresses grammatical categories with affixation (such as prefix, suffix, infix, circumfix, and transfix), apophony (as Indo-European ablaut), or other modifications. For example, the Latin verb ', meaning \"I will lead\", includes the suffix ', expressing person (first), number (singular), and tense-mood (future indicative or present subjunctive). The use of this suffix is an inflection. In contrast, in the English clause \"I will lead\", the word \"lead\" is not inflected for any of person, number, or tense; it is simply the bare form of a verb.  The inflected form of a word often contains both one or more free morphemes (a unit of meaning which can stand by itself as a word), and one or more bound morphemes (a unit of meaning which cannot stand alone as a word). For example, the English word \"cars\" is a noun that is inflected for number, specifically to express the plural; the content morpheme \"car\" is unbound because it could stand alone as a word, while the suffix \"-s\" is bound because it cannot stand alone as a word. These two morphemes together form the inflected word \"cars\".  Words that are never"
    },
    {
        "id": "TASK_motion-compensation",
        "name": "Motion Compensation",
        "description": "",
        "type": "Task",
        "probable_wikipedia": "Motion compensation",
        "score": "12.550578",
        "probable_description": " Motion compensation is an algorithmic technique used to predict a frame in a video, given the previous and/or future frames by accounting for motion of the camera and/or objects in the video. It is employed in the encoding of video data for video compression, for example in the generation of MPEG-2 files. Motion compensation describes a picture in terms of the transformation of a reference picture to the current picture. The reference picture may be previous in time or even from the future. When images can be accurately synthesized from previously transmitted/stored images, the compression efficiency can be improved. "
    },
    {
        "id": "TASK_motion-detection",
        "name": "Motion Detection",
        "description": "**Motion Detection** is a process to detect the presence of any moving entity in an area of interest. Motion Detection is of great importance due to its application in various areas such as surveillance and security, smart homes, and health monitoring.\n\n\n<span class=\"description-source\">Source: [Different Approaches for Human Activity Recognition\u2013 A Survey ](https://arxiv.org/abs/1906.05074)</span>",
        "type": "Task",
        "probable_wikipedia": "Motion detection",
        "score": "12.036425",
        "probable_description": " Motion detection is the process of detecting a change in the position of an object relative to its surroundings or a change in the surroundings relative to an object. Motion detection can be achieved by either mechanical or electronic methods. When motion detection is accomplished by natural organisms, it is called motion perception. "
    },
    {
        "id": "TASK_motion-estimation",
        "name": "Motion Estimation",
        "description": "**Motion Estimation** is used to determine the block-wise or pixel-wise motion vectors between two frames.\n\n\n<span class=\"description-source\">Source: [MEMC-Net: Motion Estimation and Motion Compensation Driven Neural Network for Video Interpolation and Enhancement ](https://arxiv.org/abs/1810.08768)</span>",
        "type": "Task",
        "probable_wikipedia": "Motion estimation",
        "score": "12.186046",
        "probable_description": " Motion estimation is the process of determining motion vectors that describe the transformation from one 2D image to another; usually from adjacent frames in a video sequence. It is an ill-posed problem as the motion is in three dimensions but the images are a projection of the 3D scene onto a 2D plane. The motion vectors may relate to the whole image (global motion estimation) or specific parts, such as rectangular blocks, arbitrary shaped patches or even per pixel. The motion vectors may be represented by a translational model or many other models that can approximate the motion of a real video camera, such as rotation and translation in all three dimensions and zoom. "
    },
    {
        "id": "TASK_motion-planning",
        "name": "Motion Planning",
        "description": "<span style=\"color:grey; opacity: 0.6\">( Image credit: [Motion Planning Among Dynamic, Decision-Making Agents with Deep Reinforcement Learning](https://arxiv.org/pdf/1805.01956v1.pdf) )</span>",
        "type": "Task",
        "probable_wikipedia": "Motion planning",
        "score": "10.17354",
        "probable_description": " Motion planning (also known as the navigation problem or the piano mover's problem) is a term used in robotics is to find a sequence of valid configurations that moves the robot from the source to destination.  For example, consider navigating a mobile robot inside a building to a distant waypoint. It should execute this task while avoiding walls and not falling down stairs. A motion planning algorithm would take a description of these tasks as input, and produce the speed and turning commands sent to the robot's wheels. Motion planning algorithms might address robots with a larger number of joints (e.g., industrial manipulators), more complex tasks (e.g. manipulation of objects), different constraints (e.g., a car that can only drive forward), and uncertainty (e.g. imperfect models of the environment or robot).  Motion planning has several robotics applications, such as autonomy, automation, and robot design in CAD software, as well as applications in other fields, such as animating digital characters, video game, artificial intelligence, architectural design, robotic surgery, and the study of biological molecules. "
    },
    {
        "id": "TASK_moving-object-detection",
        "name": "Moving Object Detection",
        "description": "",
        "type": "Task",
        "probable_wikipedia": "Moving object detection",
        "score": "11.7453575",
        "probable_description": " Moving object detection is a technique used in computer vision and image processing. Multiple consecutive frames from a video are compared by various methods to determine if any moving object is detected.  Moving objects detection has been used for wide range of applications like video surveillance, human activity analysis, road condition monitoring, airport safety, monitoring of protection along marine border and etc. "
    },
    {
        "id": "TASK_multi-armed-bandits",
        "name": "Multi-Armed Bandits",
        "description": "Multi-armed bandits refer to a task where a fixed amount of resources must be allocated between competing resources that maximizes expected gain. Typically these problems involve an exploration/exploitation trade-off.\r\n\r\n<span style=\"color:grey; opacity: 0.6\">( Image credit: [Microsoft Research](http://research.microsoft.com/en-us/projects/bandits/) )</span>",
        "type": "Task",
        "probable_wikipedia": "Multi-armed bandit",
        "score": "9.90057",
        "probable_description": " In probability theory, the multi-armed bandit problem (sometimes called the \"K\"-<ref name=\"doi10.1023/A:1013689704352\"></ref> or \"N\"-armed bandit problem) is a problem in which a fixed limited set of resources must be allocated between competing (alternative) choices in a way that maximizes their expected gain, when each choice's properties are only partially known at the time of allocation, and may become better understood as time passes or by allocating resources to the choice. This is a classic reinforcement learning problem that exemplifies the exploration-exploitation tradeoff dilemma. The name comes from imagining a gambler at a row of slot machines (sometimes known as \"one-armed bandits\"), who has to decide which machines to play, how many times to play each machine and in which order to play them, and whether to continue with the current machine or try a different machine. The multi-armed bandit problem also falls into the broad category of stochastic scheduling.  In the problem, each machine provides a random reward from a probability distribution specific to that machine. The objective of the gambler is to maximize the sum of rewards earned through a sequence of lever pulls. The crucial tradeoff the gambler faces at each trial is between \"exploitation\" of the machine that has the highest expected payoff and \"exploration\" to get more information about the expected payoffs of the other machines. The trade-off between exploration and exploitation is also faced in machine learning. In practice, multi-armed bandits have been used to model problems such as managing research projects in a large organization like a science"
    },
    {
        "id": "TASK_multi-class-classification",
        "name": "Multi-class Classification",
        "description": "",
        "type": "Task",
        "probable_wikipedia": "Multiclass classification",
        "score": "6.5286517",
        "probable_description": " \"Not to be confused with multi-label classification.\"  In machine learning, multiclass or multinomial classification is the problem of classifying instances into one of three or more classes. (Classifying instances into one of two classes is called binary classification.)  While some classification algorithms naturally permit the use of more than two classes, others are by nature binary algorithms; these can, however, be turned into multinomial classifiers by a variety of strategies.  Multiclass classification should not be confused with multi-label classification, where multiple labels are to be predicted for each instance. "
    },
    {
        "id": "TASK_multi-document-summarization",
        "name": "Multi-Document Summarization",
        "description": "**Multi-Document Summarization** is a process of representing a set of documents with a short piece of text by capturing the relevant information and filtering out the redundant information. Two prominent approaches to Multi-Document Summarization are extractive and abstractive summarization. Extractive summarization systems aim to extract salient snippets, sentences or passages from documents, while abstractive summarization systems aim to concisely paraphrase the content of the documents.\r\n\r\n\r\n<span class=\"description-source\">Source: [Multi-Document Summarization using Distributed Bag-of-Words Model ](https://arxiv.org/abs/1710.02745)</span>",
        "type": "Task",
        "probable_wikipedia": "Multi-document summarization",
        "score": "12.20347",
        "probable_description": " Multi-document summarization is an automatic procedure aimed at extraction of information from multiple texts written about the same topic. The resulting summary report allows individual users, such as professional information consumers, to quickly familiarize themselves with information contained in a large cluster of documents. In such a way, multi-document summarization systems are complementing the news aggregators performing the next step down the road of coping with information overload. "
    },
    {
        "id": "TASK_multi-label-classification",
        "name": "Multi-Label Classification",
        "description": "**Multi-Label Classification** is the supervised learning problem where an instance may be associated with multiple labels. This is an extension of single-label classification (i.e., multi-class, or binary) where each instance is only associated with a single class label.\r\n\r\n\r\n<span class=\"description-source\">Source: [Deep Learning for Multi-label Classification ](https://arxiv.org/abs/1502.05988)</span>",
        "type": "Task",
        "probable_wikipedia": "Multi-label classification",
        "score": "12.043975",
        "probable_description": " In machine learning, multi-label classification and the strongly related problem of multi-output classification are variants of the classification problem where multiple labels may be assigned to each instance. Multi-label classification is a generalization of multiclass classification, which is the single-label problem of categorizing instances into precisely one of more than two classes; in the multi-label problem there is no constraint on how many of the classes the instance can be assigned to.  Formally, multi-label classification is the problem of finding a model that maps inputs x to binary vectors y (assigning a value of 0 or 1 for each element (label) in y). "
    },
    {
        "id": "TASK_multi-modal-image-segmentation",
        "name": "Multi-modal image segmentation",
        "description": "",
        "type": "Task",
        "probable_wikipedia": "Image segmentation",
        "score": "0.2300828",
        "probable_description": " In computer vision, image segmentation is the process of partitioning a digital image into multiple segments (sets of pixels, also known as super-pixels). The goal of segmentation is to simplify and/or change the representation of an image into something that is more meaningful and easier to analyze. Image segmentation is typically used to locate objects and boundaries (lines, curves, etc.) in images. More precisely, image segmentation is the process of assigning a label to every pixel in an image such that pixels with the same label share certain characteristics.  The result of image segmentation is a set of segments that collectively cover the entire image, or a set of contours extracted from the image (see edge detection). Each of the pixels in a region are similar with respect to some characteristic or computed property, such as color, intensity, or texture. Adjacent regions are significantly different with respect to the same characteristic(s). When applied to a stack of images, typical in medical imaging, the resulting contours after image segmentation can be used to create 3D reconstructions with the help of interpolation algorithms like Marching cubes. "
    },
    {
        "id": "TASK_multimodal-sentiment-analysis",
        "name": "Multimodal Sentiment Analysis",
        "description": "Multimodal sentiment analysis is the task of performing sentiment analysis with multiple data sources - e.g. a camera feed of someone's face and their recorded speech.\r\n\r\n<span style=\"color:grey; opacity: 0.6\">( Image credit: [ICON: Interactive Conversational Memory Network\r\nfor Multimodal Emotion Detection](https://www.aclweb.org/anthology/D18-1280.pdf) )</span>",
        "type": "Task",
        "probable_wikipedia": "Multimodal sentiment analysis",
        "score": "12.119442",
        "probable_description": " Multimodal sentiment analysis is a new dimension of the traditional text-based sentiment analysis, which goes beyond the analysis of texts, and includes other modalities such as audio and visual data. It can be bimodal, which includes different combinations of two modalities, or trimodal, which incorporates three modalities. With the extensive amount of social media data available online in different forms such as videos and images, the conventional text-based sentiment analysis has evolved into more complex models of multimodal sentiment analysis, which can be applied in the development of virtual assistants, analysis of YouTube movie reviews, analysis of news videos, and emotion recognition (sometimes known as emotion detection) such as depression monitoring, among others.  Similar to the traditional sentiment analysis, one of the most basic task in multimodal sentiment analysis is sentiment classification, which classifies different sentiments into categories such as positive, negative, or neutral. The complexity of analyzing text, audio, and visual features to perform such a task requires the application of different fusion techniques, such as feature-level, decision-level, and hybrid fusion. The performance of these fusion techniques and the classification algorithms applied, are influenced by the type of textual, audio, and visual features employed in the analysis. "
    },
    {
        "id": "TASK_multi-object-tracking",
        "name": "Multi-Object Tracking",
        "description": "Multiple Object Tracking is the problem of automatically identifying multiple objects in a video and representing them as a set of trajectories with high accuracy.",
        "type": "Task",
        "probable_wikipedia": "Multiple object tracking",
        "score": "5.4153438",
        "probable_description": " Multiple object tracking, or MOT, is a versatile experimental paradigm developed by Zenon Pylyshyn for studying sustained visual attention in a dynamic environment in 1988. It was first developed in order to support visual indexing theory (FINST theory). MOT was then commonly used an experimental technique used in order to study how our visual system tracks multiple moving objects. Dozens or perhaps hundreds of modified MOT experiments have been conducted as a continuous attention-demanding task to further understanding human\u2019s visual and cognitive function. "
    },
    {
        "id": "TASK_multiple-instance-learning",
        "name": "Multiple Instance Learning",
        "description": "**Multiple Instance Learning** is a type of weakly supervised learning algorithm where training data is arranged in bags, where each bag contains a set of instances $X=\\\\{x_1,x_2, \\ldots,x_M\\\\}$, and there is one single label $Y$ per bag, $Y\\in\\\\{0, 1\\\\}$ in the case of a binary classification problem. It is assumed that individual labels $y_1, y_2,\\ldots, y_M$ exist for the instances within a bag, but they are unknown during training. In the standard Multiple Instance assumption, a bag is considered negative if all its instances are negative. On the other hand, a bag is positive, if at least one instance in the bag is positive.\n\n\n<span class=\"description-source\">Source: [Monte-Carlo Sampling applied to Multiple Instance Learning for Histological Image Classification ](https://arxiv.org/abs/1812.11560)</span>",
        "type": "Task",
        "probable_wikipedia": "Multiple instance learning",
        "score": "12.751665",
        "probable_description": " In machine learning, multiple-instance learning (MIL) is a type of supervised learning. Instead of receiving a set of instances which are individually labeled, the learner receives a set of labeled \"bags\", each containing many instances. In the simple case of multiple-instance binary classification, a bag may be labeled negative if all the instances in it are negative. On the other hand, a bag is labeled positive if there is at least one instance in it which is positive. From a collection of labeled bags, the learner tries to either (i) induce a concept that will label individual instances correctly or (ii) learn how to label bags without inducing the concept.  Convenient and simple example for MIL was given in. Imagine several people, and each of them has a key chain that contains few keys. Some of these people are able to enter a certain room, and some aren\u2019t. The task is then to predict whether a certain key or a certain key chain can get you into that room. To solve this problem we need to find the exact key that is common for all the \u201cpositive\u201d key chains. If we can correctly identify this key, we can also correctly classify an entire key chain - positive if it contains the required key, or negative if it doesn\u2019t. "
    },
    {
        "id": "TASK_multiple-object-tracking",
        "name": "Multiple Object Tracking",
        "description": "**Multiple Object Tracking** is the problem of automatically identifying multiple objects in a video and representing them as a set of trajectories with high accuracy.\n\n\n<span class=\"description-source\">Source: [SOT for MOT ](https://arxiv.org/abs/1712.01059)</span>",
        "type": "Task",
        "probable_wikipedia": "Multiple object tracking",
        "score": "11.215519",
        "probable_description": " Multiple object tracking, or MOT, is a versatile experimental paradigm developed by Zenon Pylyshyn for studying sustained visual attention in a dynamic environment in 1988. It was first developed in order to support visual indexing theory (FINST theory). MOT was then commonly used an experimental technique used in order to study how our visual system tracks multiple moving objects. Dozens or perhaps hundreds of modified MOT experiments have been conducted as a continuous attention-demanding task to further understanding human\u2019s visual and cognitive function. "
    },
    {
        "id": "TASK_multiple-sequence-alignment",
        "name": "Multiple Sequence Alignment",
        "description": "",
        "type": "Task",
        "probable_wikipedia": "Multiple sequence alignment",
        "score": "12.233733",
        "probable_description": " A multiple sequence alignment (MSA) is a sequence alignment of three or more biological sequences, generally protein, DNA, or RNA. In many cases, the input set of query sequences are assumed to have an evolutionary relationship by which they share a linkage and are descended from a common ancestor. From the resulting MSA, sequence homology can be inferred and phylogenetic analysis can be conducted to assess the sequences' shared evolutionary origins. Visual depictions of the alignment as in the image at right illustrate mutation events such as point mutations (single amino acid or nucleotide changes) that appear as differing characters in a single alignment column, and insertion or deletion mutations (indels or gaps) that appear as hyphens in one or more of the sequences in the alignment. Multiple sequence alignment is often used to assess sequence conservation of protein domains, tertiary and secondary structures, and even individual amino acids or nucleotides.  Multiple sequence alignment also refers to the process of aligning such a sequence set. Because three or more sequences of biologically relevant length can be difficult and are almost always time-consuming to align by hand, computational algorithms are used to produce and analyze the alignments. MSAs require more sophisticated methodologies than pairwise alignment because they are more computationally complex. Most multiple sequence alignment programs use heuristic methods rather than global optimization because identifying the optimal alignment between more than a few sequences of moderate length is prohibitively computationally expensive. "
    },
    {
        "id": "TASK_multi-task-learning",
        "name": "Multi-Task Learning",
        "description": "Multi-task learning aims to learn multiple different tasks simultaneously while maximizing\r\nperformance on one or all of the tasks.\r\n\r\n<span style=\"color:grey; opacity: 0.6\">( Image credit: [Cross-stitch Networks for Multi-task Learning](https://arxiv.org/pdf/1604.03539v1.pdf) )</span>",
        "type": "Task",
        "probable_wikipedia": "Multi-task learning",
        "score": "12.459562",
        "probable_description": " Multi-task learning (MTL) is a subfield of machine learning in which multiple learning tasks are solved at the same time, while exploiting commonalities and differences across tasks. This can result in improved learning efficiency and prediction accuracy for the task-specific models, when compared to training the models separately. Early versions of MTL were called \"hints\"  In a widely cited 1997 paper, Rich Caruana gave the following characterization:Multitask Learning is an approach to inductive transfer that improves generalization by using the domain information contained in the training signals of related tasks as an inductive bias. It does this by learning tasks in parallel while using a shared representation; what is learned for each task can help other tasks be learned better.  In the classification context, MTL aims to improve the performance of multiple classification tasks by learning them jointly. One example is a spam-filter, which can be treated as distinct but related classification tasks across different users. To make this more concrete, consider that different people have different distributions of features which distinguish spam emails from legitimate ones, for example an English speaker may find that all emails in Russian are spam, not so for Russian speakers. Yet there is a definite commonality in this classification task across users, for example one common feature might be text related to money transfer. Solving each user's spam classification problem jointly via MTL can let the solutions inform each other and improve performance. Further examples of settings for MTL include multiclass classification and multi-label classification.  Multi-task"
    },
    {
        "id": "TASK_music-genre-classification",
        "name": "Music Genre Classification",
        "description": "",
        "type": "Task",
        "probable_wikipedia": "Music genre",
        "score": "1.0742961",
        "probable_description": " A music genre is a conventional category that identifies some pieces of music as belonging to a shared tradition or set of conventions. It is to be distinguished from \"musical form\" and \"musical style\", although in practice these terms are sometimes used interchangeably. Recently, academics have argued that categorizing music by genre is inaccurate and outdated.  Music can be divided into different genres in many different ways. The artistic nature of music means that these classifications are often subjective and controversial, and some genres may overlap. There are even varying academic definitions of the term \"genre \"itself. In his book \"Form in Tonal Music\", Douglass M. Green distinguishes between genre and form. He lists madrigal, motet, canzona, ricercar, and dance as examples of genres from the Renaissance period. To further clarify the meaning of \"genre\", Green writes, \"Beethoven's Op. 61 and Mendelssohn's Op. 64 are identical in genre \u2013 both are violin concertos \u2013 but different in form. However, Mozart's Rondo for Piano, K. 511, and the \"Agnus Dei\" from his Mass, K. 317 are quite different in genre but happen to be similar in form.\" Some, like Peter van der Merwe, treat the terms \"genre\" and \"style\" as the same, saying that \"genre\" should be defined as pieces of music that share a certain style or \"basic musical language.\" Others, such as Allan F. Moore, state that \"genre\" and \"style\" are two separate terms, and that secondary characteristics such as subject matter can also differentiate between genres. A music genre or subgenre may"
    },
    {
        "id": "TASK_music-information-retrieval",
        "name": "Music Information Retrieval",
        "description": "",
        "type": "Task",
        "probable_wikipedia": "Music information retrieval",
        "score": "11.757836",
        "probable_description": " Music information retrieval (MIR) is the interdisciplinary science of retrieving information from music. MIR is a small but growing field of research with many real-world applications. Those involved in MIR may have a background in musicology, psychoacoustics, psychology, academic music study, signal processing, informatics, machine learning, optical music recognition, computational intelligence or some combination of these. "
    },
    {
        "id": "TASK_music-transcription",
        "name": "Music Transcription",
        "description": "Music transcription is the task of converting an acoustic musical signal into some form of music notation.\r\n\r\n<span style=\"color:grey; opacity: 0.6\">( Image credit: [ISMIR 2015 Tutorial - Automatic Music Transcription](http://c4dm.eecs.qmul.ac.uk/ismir15-amt-tutorial/AMT_tutorial_ISMIR_2015.pdf) )</span>",
        "type": "Task",
        "probable_wikipedia": "Transcription (music)",
        "score": "8.054258",
        "probable_description": " In music, transcription can mean notating a piece or a sound which was previously unnotated, as, for example, an improvised jazz solo. When a musician is tasked with creating sheet music from a recording and they write down the notes that make up the piece in music notation, it is said that they created a \"musical transcription\" of that recording. Transcription may also mean rewriting a piece of music, either solo or ensemble, for another instrument or other instruments than which it was originally intended. The Beethoven Symphonies by Franz Liszt are a good example. Transcription in this sense is sometimes called \"arrangement\", although strictly speaking transcriptions are faithful adaptations, whereas arrangements change significant aspects of the original piece. Further examples of music transcription include ethnomusicological notation of oral traditions of folk music, such as Bela Bartok's and Ralph Vaughan Williams' collections of the national folk music of Hungary and England respectively. The French composer Olivier Messiaen transcribed birdsong in the wild, and incorporated it into many of his compositions, for example his \"Catalogue d'oiseaux\" for solo piano. Transcription of this nature involves scale degree recognition and harmonic analysis, both of which the transcriber will need relative or perfect pitch to perform.  In popular music and rock, there are two forms of transcription. Individual performers copy a note-for-note guitar solo or other melodic line. As well, music publishers transcribe entire recordings of guitar solos and bass lines and sell the sheet music in bound books. Music publishers also publish PVG (piano/vocal/guitar) transcriptions of popular"
    },
    {
        "id": "TASK_named-entity-recognition-ner",
        "name": "Named Entity Recognition",
        "description": "Named entity recognition (NER) is the task of tagging entities in text with their corresponding type.\r\nApproaches typically use BIO notation, which differentiates the beginning (B) and the inside (I) of entities.\r\nO is used for non-entity tokens.\r\n\r\nExample:\r\n\r\n| Mark | Watney | visited | Mars |\r\n| --- | ---| --- | --- |\r\n| B-PER | I-PER | O | B-LOC |\r\n\r\n<span style=\"color:grey; opacity: 0.6\">( Image credit: [Zalando](https://research.zalando.com/welcome/mission/research-projects/flair-nlp/) )</span>",
        "type": "Task",
        "probable_wikipedia": "Named-entity recognition",
        "score": "11.133222",
        "probable_description": " Named-entity recognition (NER) (also known as entity identification, entity chunking and entity extraction) is a subtask of information extraction that seeks to locate and classify named entity mentions in unstructured text into pre-defined categories such as the person names, organizations, locations, medical codes, time expressions, quantities, monetary values, percentages, etc.  Most research on NER systems has been structured as taking an unannotated block of text, such as this one:  And producing an annotated block of text that highlights the names of entities:  In this example, a person name consisting of one token, a two-token company name and a temporal expression have been detected and classified.  State-of-the-art NER systems for English produce near-human performance. For example, the best system entering MUC-7 scored 93.39% of F-measure while human annotators scored 97.60% and 96.95%. "
    },
    {
        "id": "TASK_native-language-identification",
        "name": "Native Language Identification",
        "description": "Native Language Identification (NLI) is the task of determining an author's native language (L1) based only on their writings in a second language (L2).",
        "type": "Task",
        "probable_wikipedia": "Native-language identification",
        "score": "8.968303",
        "probable_description": " Native-language identification (NLI) is the task of determining an author's native language (L1) based only on their writings in a second language (L2). NLI works through identifying language-usage patterns that are common to specific L1 groups and then applying this knowledge to predict the native language of previously unseen texts. This is motivated in part by applications in second-language acquisition, language teaching and forensic linguistics, amongst others. "
    },
    {
        "id": "TASK_natural-language-understanding",
        "name": "Natural Language Understanding",
        "description": "**Natural Language Understanding** is an important field of Natural Language Processing which contains various tasks such as text classification, natural language inference and story comprehension. Applications enabled by natural language understanding range from question answering  to automated reasoning.\r\n\r\n\r\n\r\n<span class=\"description-source\">Source: [Find a Reasonable Ending for Stories: Does Logic Relation Help the Story Cloze Test? ](https://arxiv.org/abs/1812.05411)</span>",
        "type": "Task",
        "probable_wikipedia": "Natural-language understanding",
        "score": "10.710206",
        "probable_description": " Natural-language understanding (NLU) or natural-language interpretation (NLI) is a subtopic of natural-language processing in artificial intelligence that deals with machine reading comprehension. Natural-language understanding is considered an AI-hard problem.  There is considerable commercial interest in the field because of its application to automated reasoning, machine translation, question answering, news-gathering, text categorization, voice-activation, archiving, and large-scale content analysis.  NLU is the post-processing of text, after the use of NLP algorithms (identifying parts-of-speech, etc.), that utilizes context from recognition devices (automatic speech recognition [ASR], vision recognition, last conversation, misrecognized words from ASR, personalized profiles, microphone proximity etc.), in all of its forms, to discern meaning of fragmented and run-on sentences to execute an intent from typically voice commands. NLU has an ontology around the particular product vertical that is used to figure out the probability of some intent. An NLU has a defined list of known intents that derives the message payload from designated contextual information recognition sources. The NLU will provide back multiple message outputs to separate services (software) or resources (hardware) from a single derived intent (response to voice command initiator with visual sentence (shown or spoken) and transformed voice command message too different output messages to be consumed for M2M communications and actions). "
    },
    {
        "id": "DATASET_ner",
        "name": "NER",
        "full_name": null,
        "url": null,
        "type": "Dataset",
        "probable_wikipedia": "Ner",
        "score": "4.8479953",
        "probable_description": " The Ner is a river in central Poland approximately long, with sources to the south-east of \u0141odz. It is one of the right tributaries of the Warta River, and the largest river in \u0141odz.   "
    },
    {
        "id": "TASK_nethack",
        "name": "NetHack",
        "description": "Mean in-game score over 1000 episodes with random seeds not seen during training. See https://arxiv.org/abs/2006.13760 (Section 2.4 Evaluation Protocol) for details.",
        "type": "Task",
        "probable_wikipedia": "NetHack",
        "score": "8.350383",
        "probable_description": " NetHack is a single-player roguelike video game originally released in 1987 with ASCII graphics. It is a descendant of an earlier game called \"Hack\" (1982), which is a clone of \"Rogue\" (1980). Comparing it with \"Rogue\", \"Engadget\"s Justin Olivetti wrote that it took its exploration aspect and \"made it far richer with an encyclopedia of objects, a larger vocabulary, a wealth of pop culture mentions, and a puzzler's attitude.\" In 2000, \"Salon\" described it as \"one of the finest gaming experiences the computing world has to offer\".  The player chooses a character race and class for the mission of retrieving the Amulet of Yendor in a randomly generated dungeon. "
    },
    {
        "id": "TASK_network-congestion-control",
        "name": "Network Congestion Control",
        "description": "",
        "type": "Task",
        "probable_wikipedia": "TCP congestion control",
        "score": "2.0845902",
        "probable_description": " Transmission Control Protocol (TCP) uses a network congestion-avoidance algorithm that includes various aspects of an additive increase/multiplicative decrease (AIMD) scheme, with other schemes such as slow start and congestion window to achieve congestion avoidance. The TCP congestion-avoidance algorithm is the primary basis for congestion control in the Internet. Per the end-to-end principle, congestion control is largely a function of internet hosts, not the network itself. There are several variations and versions of the algorithm implemented in protocol stacks of operating systems of computers that connect to the Internet. "
    },
    {
        "id": "METHOD_neural-architecture-search",
        "name": "Neural Architecture Search",
        "full_name": "Neural Architecture Search",
        "description": "Neural Architecture Search (NAS) learns a modular architecture which can be transferred from a small dataset to a large dataset. The method does this by reducing the problem of learning best convolutional architectures to the problem of learning a small convolutional cell. The cell can then be stacked in series to handle larger images and more complex datasets.\nNote that this refers to the original method referred to as NAS - there is also a broader category of methods called \"neural architecture search\".",
        "category": [
            "Neural Architecture Search"
        ],
        "proposed_in": {
            "paper_id": "learning-transferable-architectures-for",
            "s2_paper_id": "d0611891b9e8a7c5731146097b6f201578f47b2f"
        },
        "type": "Method",
        "probable_wikipedia": "Neural architecture search",
        "score": "11.825397",
        "probable_description": " Neural architecture search (NAS) is a technique for automating the design of artificial neural networks (ANN), a widely used model in the field of machine learning. NAS has been used to design networks that are on par or outperform hand-designed architectures. Methods for NAS can be categorized according to the search space, search strategy and performance estimation strategy used:   NAS is closely related to hyperparameter optimization and is a subfield of automated machine learning (AutoML). "
    },
    {
        "id": "TASK_nmr-j-coupling",
        "name": "NMR J-coupling",
        "description": "https://github.com/larsbratholm/champs_kaggle",
        "type": "Task",
        "probable_wikipedia": "J-coupling",
        "score": "6.5979257",
        "probable_description": " In nuclear chemistry and nuclear physics, Scalar or \"J\"-couplings (also called \"indirect dipole\u2013dipole coupling\") are mediated through chemical bonds connecting two spins. It is an indirect interaction between two nuclear spins which arises from hyperfine interactions between the nuclei and local electrons. In NMR spectroscopy \"J\"-coupling contains information about relative bond distances and angles. Most importantly, \"J\"-coupling provides information on the connectivity of chemical bonds. It is responsible for the often complex splitting of resonance lines in the NMR spectra of fairly simple molecules. "
    },
    {
        "id": "TASK_node-clustering",
        "name": "Node Clustering",
        "description": "",
        "type": "Task",
        "probable_wikipedia": "Computer cluster",
        "score": "1.2710892",
        "probable_description": " A computer cluster is a set of loosely or tightly connected computers that work together so that, in many respects, they can be viewed as a single system. Unlike grid computers, computer clusters have each node set to perform the same task, controlled and scheduled by software.  The components of a cluster are usually connected to each other through fast local area networks, with each \"node\" (computer used as a server) running its own instance of an operating system. In most circumstances, all of the nodes use the same hardware and the same operating system, although in some setups (e.g. using Open Source Cluster Application Resources (OSCAR)), different operating systems can be used on each computer, or different hardware.  Clusters are usually deployed to improve performance and availability over that of a single computer, while typically being much more cost-effective than single computers of comparable speed or availability.  Computer clusters emerged as a result of convergence of a number of computing trends including the availability of low-cost microprocessors, high-speed networks, and software for high-performance distributed computing. They have a wide range of applicability and deployment, ranging from small business clusters with a handful of nodes to some of the fastest supercomputers in the world such as IBM's Sequoia. Prior to the advent of clusters, single unit fault tolerant mainframes with modular redundancy were employed; but the lower upfront cost of clusters, and increased speed of network fabric has favoured the adoption of clusters. In contrast to high-reliability mainframes clusters are cheaper to"
    },
    {
        "id": "TASK_non-intrusive-load-monitoring",
        "name": "Non-Intrusive Load Monitoring",
        "description": "",
        "type": "Task",
        "probable_wikipedia": "Nonintrusive load monitoring",
        "score": "8.054913",
        "probable_description": " Nonintrusive load monitoring (NILM), or nonintrusive appliance load monitoring (NIALM), is a process for analyzing changes in the voltage and current going into a house and deducing what appliances are used in the house as well as their individual energy consumption. Electric meters with NILM technology are used by utility companies to survey the specific uses of electric power in different homes. NILM is considered a low-cost alternative to attaching individual monitors on each appliance. It does, however, present privacy concerns. "
    },
    {
        "id": "TASK_novel-view-synthesis",
        "name": "Novel View Synthesis",
        "description": "Synthesize a target image with an arbitrary target camera pose from given source images and their camera poses.\r\n\r\n<span style=\"color:grey; opacity: 0.6\">( Image credit: [Multi-view to Novel view: Synthesizing novel views with Self-Learned Confidence](https://github.com/shaohua0116/Multiview2Novelview) )</span>",
        "type": "Task",
        "probable_wikipedia": "View synthesis",
        "score": "3.1982086",
        "probable_description": " Currently a study branch of Computer Science Research, aims to create new views of a specific subject starting from a number of pictures taken from given point of views.  Vision Research and Artificial Intelligence fields are involved in the definition of suitable approaches to the problem. See Computer Vision  Example problem 1: Given a number of images of a specific subject, taken from specific points with specific camera setting and orientations, try to build a synthetic image as taken from a virtual camera placed in a different point and with given settings.  Example problem 2: Two people interact through their computers, using a webcam. Try to render corrected images, as if taken from a virtual webcam positioned behind the application window. This would solve the long-standing Eye contact problem which is experienced in this environment. A double illusion is perceived by the users: each of them looks at each other's face, but neither of them get the proper feeling of it.  An example application of View Synthesis is Free view point television.    "
    },
    {
        "id": "TASK_numerical-integration",
        "name": "Numerical Integration",
        "description": "Numerical integration is the task to calculate the numerical value of a definite integral or the numerical solution of differential equations.",
        "type": "Task",
        "probable_wikipedia": "Numerical integration",
        "score": "10.761539",
        "probable_description": " In analysis, numerical integration comprises a broad family of algorithms for calculating the numerical value of a definite integral, and by extension, the term is also sometimes used to describe the numerical solution of differential equations. This article focuses on calculation of definite integrals. The term numerical quadrature (often abbreviated to \"quadrature\") is more or less a synonym for \"numerical integration\", especially as applied to one-dimensional integrals. Some authors refer to numerical integration over more than one dimension as cubature; others take \"quadrature\" to include higher-dimensional integration.  The basic problem in numerical integration is to compute an approximate solution to a definite integral  to a given degree of accuracy. If is a smooth function integrated over a small number of dimensions, and the domain of integration is bounded, there are many methods for approximating the integral to the desired precision. "
    },
    {
        "id": "TASK_object-counting",
        "name": "Object Counting",
        "description": "The goal of **Object Counting** task is to count the number of object instances in a single image or video sequence. It has many real-world applications such as traffic flow monitoring, crowdedness estimation, and product counting.\n\n\n<span class=\"description-source\">Source: [Learning to Count Objects with Few Exemplar Annotations ](https://arxiv.org/abs/1905.07898)</span>",
        "type": "Task",
        "probable_wikipedia": "Counting",
        "score": "2.2956834",
        "probable_description": " Counting is the process of determining the number of elements of a finite set of objects. The traditional way of counting consists of continually increasing a (mental or spoken) counter by a unit for every element of the set, in some order, while marking (or displacing) those elements to avoid visiting the same element more than once, until no unmarked elements are left; if the counter was set to one after the first object, the value after visiting the final object gives the desired number of elements. The related term \"enumeration\" refers to uniquely identifying the elements of a finite (combinatorial) set or infinite set by assigning a number to each element.  Counting sometimes involves numbers other than one; for example, when counting money, counting out change, \"counting by twos\" (2, 4, 6, 8, 10, 12,\u00a0...), or \"counting by fives\" (5, 10, 15, 20, 25,\u00a0...).  There is archaeological evidence suggesting that humans have been counting for at least 50,000\u00a0years. Counting was primarily used by ancient cultures to keep track of social and economic data such as number of group members, prey animals, property, or debts (i.e., accountancy). Notched bones were also found in the Border Caves in South Africa that may suggest that the concept of counting was known to humans as far back as 44,000 BCE. The development of counting led to the development of mathematical notation, numeral systems, and writing. "
    },
    {
        "id": "TASK_object-detection",
        "name": "Object Detection",
        "description": "Object detection is the task of detecting instances of objects of a certain class within an image. The state-of-the-art methods can be categorized into two main types: one-stage methods and two stage-methods. One-stage methods prioritize inference speed, and example models include YOLO, SSD and RetinaNet. Two-stage methods prioritize detection accuracy, and example models include Faster R-CNN, Mask R-CNN and Cascade R-CNN.\r\n\r\nThe most popular benchmark is the MSCOCO dataset. Models are typically evaluated according to a Mean Average Precision metric.\r\n\r\n<span style=\"color:grey; opacity: 0.6\">( Image credit: [Detectron](https://github.com/facebookresearch/detectron) )</span>",
        "type": "Task",
        "probable_wikipedia": "Object detection",
        "score": "12.311762",
        "probable_description": " Object detection is a computer technology related to computer vision and image processing that deals with detecting instances of semantic objects of a certain class (such as humans, buildings, or cars) in digital images and videos. Well-researched domains of object detection include face detection and pedestrian detection. Object detection has applications in many areas of computer vision, including image retrieval and video surveillance. "
    },
    {
        "id": "TASK_odd-one-out",
        "name": "Odd One Out",
        "description": "",
        "type": "Task",
        "probable_wikipedia": "Odd One Out",
        "score": "11.118582",
        "probable_description": " Odd One Out is a British game show based on the American version entitled \"Knockout\". It aired on BBC1 from 16 April 1982 to 19 April 1985 and was hosted by Paul Daniels. "
    },
    {
        "id": "TASK_one-class-classifier",
        "name": "One-class classifier",
        "description": "",
        "type": "Task",
        "probable_wikipedia": "One-class classification",
        "score": "0.1769622",
        "probable_description": " In machine learning, one-class classification (OCC), also known as unary classification or class-modelling, tries to \"identify\" objects of a specific class amongst all objects, by primarily learning from a training set containing only the objects of that class, although there exist variants of one-class classifiers where counter-examples are used to further refine the classification boundary. This is different from and more difficult than the traditional classification problem, which tries to \"distinguish between\" two or more classes with the training set containing objects from all the classes. An example is the classification of the operational status of a nuclear plant as 'normal': In this scenario, there are few, if any, examples of catastrophic system states; only the statistics of normal operation are known.  While many of the above approaches focus on the case of removing a small number of outliers or anomalies, one can also learn the other extreme, where the single class covers a small coherent subset of the data, using an information bottleneck approach. "
    },
    {
        "id": "TASK_one-shot-learning",
        "name": "One-Shot Learning",
        "description": "One-shot learning is the task of learning information about object categories from a single training example.\r\n\r\n<span style=\"color:grey; opacity: 0.6\">( Image credit: [Siamese Neural Networks for One-shot Image Recognition](https://www.cs.cmu.edu/~rsalakhu/papers/oneshot1.pdf) )</span>",
        "type": "Task",
        "probable_wikipedia": "One-shot learning",
        "score": "12.491278",
        "probable_description": " One-shot learning is an object categorization problem, found mostly in computer vision. Whereas most machine learning based object categorization algorithms require training on hundreds or thousands of samples/images and very large datasets, one-shot learning aims to learn information about object categories from one, or only a few, training samples/images.  The primary focus of this article will be on the solution to this problem presented by Fei-Fei Li, R. Fergus and P. Perona in IEEE Transactions on Pattern Analysis and Machine Intelligence, Vol28(4), 2006, which uses a generative object category model and variational Bayesian framework for representation and learning of visual object categories from a handful of training examples. Another paper, presented at the International Conference on Computer Vision and Pattern Recognition (CVPR) 2000 by Erik Miller, Nicholas Matsakis, and Paul Viola will also be discussed. "
    },
    {
        "id": "TASK_online-learning",
        "name": "online learning",
        "description": "",
        "type": "Task",
        "probable_wikipedia": "Online learning in higher education",
        "score": "4.504922",
        "probable_description": " Online learning involves courses offered by postsecondary institutions that are 100% virtual, excluding massively open online courses (MOOCs). Online learning, or virtual classes offered over the internet, is contrasted with traditional courses taken in a brick-and-mortar school building. It is the newest development in distance education that began in the mid-1990s with the spread of the internet. Learner experience is typically asynchronous, but may also incorporate synchronous elements. The vast majority of institutions utilize a Learning Management System for the administration of online courses. As theories of distance education evolve, digital technologies to support learning and pedagogy continue to transform as well. "
    },
    {
        "id": "TASK_open-information-extraction",
        "name": "Open Information Extraction",
        "description": "In natural language processing, open information extraction is the task of generating a structured, machine-readable representation of the information in text, usually in the form of triples or n-ary propositions (Source: Wikipedia).",
        "type": "Task",
        "probable_wikipedia": "Open information extraction",
        "score": "12.4812975",
        "probable_description": " In natural language processing, open information extraction (OIE) is the task of generating a structured, machine-readable representation of the information in text, usually in the form of triples or n-ary propositions. "
    },
    {
        "id": "TASK_optical-character-recognition",
        "name": "Optical Character Recognition",
        "description": "Optical character recognition or optical character reader (OCR) is the electronic or mechanical conversion of images of typed, handwritten or printed text into machine-encoded text, whether from a scanned document, a photo of a document, a scene-photo (for example the text on signs and billboards in a landscape photo, license plates in cars...) or from subtitle text superimposed on an image (for example: from a television broadcast)",
        "type": "Task",
        "probable_wikipedia": "Optical character recognition",
        "score": "11.666104",
        "probable_description": " Optical character recognition or optical character reader (OCR) is the mechanical or electronic conversion of images of typed, handwritten or printed text into machine-encoded text, whether from a scanned document, a photo of a document, a scene-photo (for example the text on signs and billboards in a landscape photo) or from subtitle text superimposed on an image (for example from a television broadcast).  Widely used as a form of information entry from printed paper data records \u2013 whether passport documents, invoices, bank statements, computerized receipts, business cards, mail, printouts of static-data, or any suitable documentation \u2013 it is a common method of digitizing printed texts so that they can be electronically edited, searched, stored more compactly, displayed on-line, and used in machine processes such as cognitive computing, machine translation, (extracted) text-to-speech, key data and text mining. OCR is a field of research in pattern recognition, artificial intelligence and computer vision.  Early versions needed to be trained with images of each character, and worked on one font at a time. Advanced systems capable of producing a high degree of recognition accuracy for most fonts are now common, and with support for a variety of digital image file format inputs. Some systems are capable of reproducing formatted output that closely approximates the original page including images, columns, and other non-textual components. "
    },
    {
        "id": "TASK_outlier-detection",
        "name": "Outlier Detection",
        "description": "**Outlier Detection** is a task of identifying a subset of a given data set which are considered anomalous in that they are unusual from other instances. It is one of the core data mining tasks and is central to many applications. In the security field, it can be used to identify potentially threatening users, in the manufacturing field it can be used to identify parts that are likely to fail.\n\n\n<span class=\"description-source\">Source: [Coverage-based Outlier Explanation ](https://arxiv.org/abs/1911.02617)</span>",
        "type": "Task",
        "probable_wikipedia": "Anomaly detection",
        "score": "6.691439",
        "probable_description": " In data mining, anomaly detection (also outlier detection) is the identification of rare items, events or observations which raise suspicions by differing significantly from the majority of the data. Typically the anomalous items will translate to some kind of problem such as bank fraud, a structural defect, medical problems or errors in a text. Anomalies are also referred to as outliers, novelties, noise, deviations and exceptions.  In particular, in the context of abuse and network intrusion detection, the interesting objects are often not \"rare\" objects, but unexpected \"bursts\" in activity. This pattern does not adhere to the common statistical definition of an outlier as a rare object, and many outlier detection methods (in particular unsupervised methods) will fail on such data, unless it has been aggregated appropriately. Instead, a cluster analysis algorithm may be able to detect the micro clusters formed by these patterns.  Three broad categories of anomaly detection techniques exist. Unsupervised anomaly detection techniques detect anomalies in an unlabeled test data set under the assumption that the majority of the instances in the data set are normal by looking for instances that seem to fit least to the remainder of the data set. Supervised anomaly detection techniques require a data set that has been labeled as \"normal\" and \"abnormal\" and involves training a classifier (the key difference to many other statistical classification problems is the inherent unbalanced nature of outlier detection). Semi-supervised anomaly detection techniques construct a model representing normal behavior from a given \"normal\" training data set, and then test"
    },
    {
        "id": "TASK_part-of-speech-tagging",
        "name": "Part-Of-Speech Tagging",
        "description": "Part-of-speech tagging (POS tagging) is the task of tagging a word in a text with its part of speech.\nA part of speech is a category of words with similar grammatical properties. Common English\nparts of speech are noun, verb, adjective, adverb, pronoun, preposition, conjunction, etc.\n\nExample: \n\n| Vinken | , | 61 | years | old |\n| --- | ---| --- | --- | --- |\n| NNP | , | CD | NNS | JJ |",
        "type": "Task",
        "probable_wikipedia": "Part-of-speech tagging",
        "score": "11.994588",
        "probable_description": " In corpus linguistics, part-of-speech tagging (POS tagging or PoS tagging or POST), also called grammatical tagging or word-category disambiguation, is the process of marking up a word in a text (corpus) as corresponding to a particular part of speech, based on both its definition and its context\u2014i.e., its relationship with adjacent and related words in a phrase, sentence, or paragraph. A simplified form of this is commonly taught to school-age children, in the identification of words as nouns, verbs, adjectives, adverbs, etc.  Once performed by hand, POS tagging is now done in the context of computational linguistics, using algorithms which associate discrete terms, as well as hidden parts of speech, in accordance with a set of descriptive tags. POS-tagging algorithms fall into two distinctive groups: rule-based and stochastic. E. Brill's tagger, one of the first and most widely used English POS-taggers, employs rule-based algorithms. "
    },
    {
        "id": "TASK_pedestrian-detection",
        "name": "Pedestrian Detection",
        "description": "Pedestrian detection is the task of detecting pedestrians from a camera.\r\n\r\nFurther state-of-the-art results (e.g. on the KITTI dataset) can be found at [3D Object Detection](https://paperswithcode.com/task/object-detection).\r\n\r\n<span style=\"color:grey; opacity: 0.6\">( Image credit: [High-level Semantic Feature Detection: A New Perspective for Pedestrian Detection](https://github.com/liuwei16/CSP) )</span>",
        "type": "Task",
        "probable_wikipedia": "Pedestrian detection",
        "score": "11.399063",
        "probable_description": " Pedestrian detection is an essential and significant task in any intelligent video surveillance system, as it provides the fundamental information for semantic understanding of the video footages. It has an obvious extension to automotive applications due to the potential for improving safety systems. Many car manufacturers (e.g. Volvo, Ford, GM, Nissan) offer this as an ADAS option in 2017.  "
    },
    {
        "id": "TASK_photometric-redshift-estimation",
        "name": "Photometric Redshift Estimation",
        "description": "",
        "type": "Task",
        "probable_wikipedia": "Photometric redshift",
        "score": "9.744487",
        "probable_description": " A photometric redshift is an estimate for the recession velocity of an astronomical object, such as a galaxy or quasar, without measuring its spectrum. The technique uses photometry (that is, the brightness of the object viewed through various standard filters, each of which lets through a relatively broad passband of colours, such as red light, green light, or blue light) to determine the redshift, and hence, through Hubble's law, the distance, of the observed object. The technique relies upon the spectrum of radiation being emitted by the object having strong features that can be detected by the relatively crude filters.  The technique was developed in the 1960s, but was largely replaced in the 1970s and 1980s by spectroscopic redshifts, using spectroscopy to observe the frequency (or wavelength) of characteristic spectral lines, and measure the shift of these lines from their laboratory positions. The photometric redshift technique has come back into mainstream use since 2000, as a result of large sky surveys conducted in the late 1990s and 2000s which have detected a large number of faint high-redshift objects, and telescope time limitations mean that only a small fraction of these can be observed by spectroscopy.  Photometric redshifts were originally determined by calculating the expected observed data from a known emission spectrum at a range of redshifts. In recent years, Bayesian statistical methods and artificial neural networks have been used to estimate redshifts from photometric data.  As photometric filters are sensitive to a range of wavelengths, and the technique relies on making many"
    },
    {
        "id": "TASK_photoplethysmography-ppg",
        "name": "Photoplethysmography (PPG)",
        "description": "**Photoplethysmography (PPG)** is a non-invasive light-based method that has been used since the 1930s for monitoring cardiovascular activity.\n\n\n<span class=\"description-source\">Source: [Non-contact transmittance photoplethysmographic imaging (PPGI) for long-distance cardiovascular monitoring ](https://arxiv.org/abs/1503.06775)</span>",
        "type": "Task",
        "probable_wikipedia": "Photoplethysmogram",
        "score": "4.9013753",
        "probable_description": " A photoplethysmogram (PPG) is an optically obtained plethysmogram that can be used to detect blood volume changes in the microvascular bed of tissue. A PPG is often obtained by using a pulse oximeter which illuminates the skin and measures changes in light absorption. A conventional pulse oximeter monitors the perfusion of blood to the dermis and subcutaneous tissue of the skin.  With each cardiac cycle the heart pumps blood to the periphery. Even though this pressure pulse is somewhat damped by the time it reaches the skin, it is enough to distend the arteries and arterioles in the subcutaneous tissue. If the pulse oximeter is attached without compressing the skin, a pressure pulse can also be seen from the venous plexus, as a small secondary peak.  The change in volume caused by the pressure pulse is detected by illuminating the skin with the light from a light-emitting diode (LED) and then measuring the amount of light either transmitted or reflected to a photodiode. Each cardiac cycle appears as a peak, as seen in the figure. Because blood flow to the skin can be modulated by multiple other physiological systems, the PPG can also be used to monitor breathing, hypovolemia, and other circulatory conditions. Additionally, the shape of the PPG waveform differs from subject to subject, and varies with the location and manner in which the pulse oximeter is attached. "
    },
    {
        "id": "TASK_photo-retouching",
        "name": "Photo Retouching",
        "description": "",
        "type": "Task",
        "probable_wikipedia": "Image editing",
        "score": "4.104677",
        "probable_description": " Image editing encompasses the processes of altering images, whether they are digital photographs, traditional photo-chemical photographs, or illustrations. Traditional analog image editing is known as photo retouching, using tools such as an airbrush to modify photographs, or editing illustrations with any traditional art medium. Graphic software programs, which can be broadly grouped into vector graphics editors, raster graphics editors, and 3D modelers, are the primary tools with which a user may manipulate, enhance, and transform images. Many image editing programs are also used to render or create computer art from scratch. "
    },
    {
        "id": "TASK_pico",
        "name": "PICO",
        "description": "",
        "type": "Task",
        "probable_wikipedia": "Pico-",
        "score": "7.272381",
        "probable_description": " Pico- (symbol p) is a unit prefix in the metric system denoting one trillionth, a factor of 10 ().  Derived from the Spanish , meaning \"peak\", \"beak\", \"bit\", this was one of the original 12 prefixes defined in 1960 when the International System of Units was established.  The radius of atoms range from 25 picometers (hydrogen) to 260 picometers (caesium). One picolight-year is about nine kilometers (six miles).   "
    },
    {
        "id": "TASK_point-cloud-classification",
        "name": "Point Cloud Classification",
        "description": "Point Cloud Classification is a task involving the classification of unordered 3D point sets (point clouds).",
        "type": "Task",
        "probable_wikipedia": "Point cloud",
        "score": "3.8866293",
        "probable_description": " A point cloud is a set of data points in space. Point clouds are generally produced by 3D scanners, which measure a large number of points on the external surfaces of objects around them. As the output of 3D scanning processes, point clouds are used for many purposes, including to create 3D CAD models for manufactured parts, for metrology and quality inspection, and for a multitude of visualization, animation, rendering and mass customization applications. "
    },
    {
        "id": "TASK_point-cloud-registration",
        "name": "Point Cloud Registration",
        "description": "**Point Cloud Registration** is a fundamental problem in 3D computer vision and photogrammetry. Given several sets of points in different coordinate systems, the aim of registration is to find the transformation that best aligns all of them into a common coordinate system. Point Cloud Registration plays a significant role in many vision applications such as 3D model reconstruction, cultural heritage management, landslide monitoring and solar energy analysis.\n\n\n<span class=\"description-source\">Source: [Iterative Global Similarity Points : A robust coarse-to-fine integration solution for pairwise 3D point cloud registration ](https://arxiv.org/abs/1808.03899)</span>",
        "type": "Task",
        "probable_wikipedia": "Point cloud",
        "score": "0.82992786",
        "probable_description": " A point cloud is a set of data points in space. Point clouds are generally produced by 3D scanners, which measure a large number of points on the external surfaces of objects around them. As the output of 3D scanning processes, point clouds are used for many purposes, including to create 3D CAD models for manufactured parts, for metrology and quality inspection, and for a multitude of visualization, animation, rendering and mass customization applications. "
    },
    {
        "id": "TASK_point-clouds",
        "name": "Point Clouds",
        "description": "",
        "type": "Task",
        "probable_wikipedia": "Point cloud",
        "score": "10.86602",
        "probable_description": " A point cloud is a set of data points in space. Point clouds are generally produced by 3D scanners, which measure a large number of points on the external surfaces of objects around them. As the output of 3D scanning processes, point clouds are used for many purposes, including to create 3D CAD models for manufactured parts, for metrology and quality inspection, and for a multitude of visualization, animation, rendering and mass customization applications. "
    },
    {
        "id": "TASK_point-processes",
        "name": "Point Processes",
        "description": "",
        "type": "Task",
        "probable_wikipedia": "Point process",
        "score": "8.655258",
        "probable_description": " In statistics and probability theory, a point process or point field is a collection of mathematical points randomly located on some underlying mathematical space such as the real line, the Cartesian plane, or more abstract spaces. Point processes can be used as mathematical models of phenomena or objects representable as points in some type of space.  There are different mathematical interpretations of a point process, such as a random counting measure or a random set. Some authors regard a point process and stochastic process as two different objects such that a point process is a random object that arises from or is associated with a stochastic process, though it has been remarked that the difference between point processes and stochastic processes is not clear. Others consider a point process as a stochastic process, where the process is indexed by sets of the underlying space on which it is defined, such as the real line or formula_1-dimensional Euclidean space. Other stochastic processes such as renewal and counting processes are studied in the theory of point processes. Sometimes the term \"point process\" is not preferred, as historically the word \"process\" denoted an evolution of some system in time, so point process is also called a random point field.  Point processes are well studied objects in probability theory and the subject of powerful tools in statistics for modeling and analyzing spatial data, which is of interest in such diverse disciplines as forestry, plant ecology, epidemiology, geography, seismology, materials science, astronomy, telecommunications, computational neuroscience, economics and others."
    },
    {
        "id": "TASK_populist-attitude",
        "name": "Populist attitude",
        "description": "",
        "type": "Task",
        "probable_wikipedia": "Populism",
        "score": "6.189552",
        "probable_description": " Populism refers to a range of political stances that emphasise the idea of \"the people\" and often juxtapose this group against \"the elite\". The term developed in the 19th century and has been applied to various politicians, parties, and movements since that time, although has rarely been chosen as a self-description. Within political science and other social sciences, several different definitions of populism have been employed, with some scholars proposing that the term be rejected altogether.  A common framework for interpreting populism is known as the ideational approach: this defines \"populism\" as an ideology which presents \"the people\" as a morally good force and contrasts them against \"the elite\", who are portrayed as corrupt and self-serving. Populists differ in how \"the people\" are defined, but it can be based along class, ethnic, or national lines. Populists typically present \"the elite\" as comprising the political, economic, cultural, and media establishment, depicted as a homogeneous entity and accused of placing their own interests, and often the interests of other groups\u2014such as large corporations, foreign countries, or immigrants\u2014above the interests of \"the people\". Populist parties and social movements are often led by charismatic or dominant figures who present themselves as the \"voice of the people\". According to the ideational approach, populism is often combined with other ideologies, such as nationalism, liberalism, or socialism. Thus, populists can be found at different locations along the left\u2013right political spectrum and there exists both left-wing populism and right-wing populism.  Other scholars of the social sciences have defined the term \"populism\""
    },
    {
        "id": "TASK_portfolio-optimization",
        "name": "Portfolio Optimization",
        "description": "",
        "type": "Task",
        "probable_wikipedia": "Portfolio optimization",
        "score": "12.6384735",
        "probable_description": " Portfolio optimization is the process of selecting the best portfolio (asset distribution), out of the set of all portfolios being considered, according to some objective. The objective typically maximizes factors such as expected return, and minimizes costs like financial risk. Factors being considered may range from tangible (such as assets, liabilities, earnings or other fundamentals) to intangible (such as selective divestment). "
    },
    {
        "id": "TASK_pos",
        "name": "POS",
        "description": "",
        "type": "Task",
        "probable_wikipedia": "Packet over SONET/SDH",
        "score": "8.035148",
        "probable_description": " Packet over SONET/SDH, abbreviated POS, is a communications protocol for transmitting packets in the form of the Point to Point Protocol (PPP) over SDH or SONET, which are both standard protocols for communicating digital information using lasers or light emitting diodes (LEDs) over optical fibre at high line rates. POS is defined by RFC 2615 as PPP over SONET/SDH. PPP is the Point to Point Protocol that was designed as a standard method of communicating over point-to-point links. Since SONET/SDH uses point-to-point circuits, PPP is well suited for use over these links. Scrambling is performed during insertion of the PPP packets into the SONET/SDH frame to solve various security attacks including denial-of-service attacks and the imitation of SONET/SDH alarms. This modification was justified as cost-effective because the scrambling algorithm was already used by the standard used to transport ATM cells over SONET/SDH. However, scrambling can optionally be disabled to allow a node to be compatible with another node that uses the now obsoleted RFC 1619 version of Packet over SONET/SDH which lacks the scrambler. "
    },
    {
        "id": "TASK_pose-estimation",
        "name": "Pose Estimation",
        "description": "Pose Estimation is a general problem in Computer Vision where the goal is to detect the position and orientation of a person or an object. Usually, this is done by predicting the location of specific keypoints like hands, head, elbows, etc. in case of Human Pose Estimation.\r\n\r\nA common benchmark for this task is [MPII Human Pose](https://paperswithcode.com/sota/pose-estimation-on-mpii-human-pose)\r\n\r\n<span style=\"color:grey; opacity: 0.6\">( Image credit: [Real-time 2D Multi-Person Pose Estimation on CPU: Lightweight OpenPose](https://github.com/Daniil-Osokin/lightweight-human-pose-estimation.pytorch) )</span>",
        "type": "Task",
        "probable_wikipedia": "Pose (computer vision)",
        "score": "0.34395042",
        "probable_description": " In computer vision and robotics, a typical task is to identify specific objects in an image and to determine each object's position and orientation relative to some coordinate system. This information can then be used, for example, to allow a robot to manipulate an object or to avoid moving into the object. The combination of \"position\" and \"orientation\" is referred to as the pose of an object, even though this concept is sometimes used only to describe the orientation. \"Exterior orientation\" and \"translation\" are also used as synonyms of pose.  The image data from which the pose of an object is determined can be either a single image, a stereo image pair, or an image sequence where, typically, the camera is moving with a known velocity. The objects which are considered can be rather general, including a living being or body parts, e.g., a head or hands. The methods which are used for determining the pose of an object, however, are usually specific for a class of objects and cannot generally be expected to work well for other types of objects.  The pose can be described by means of a rotation and translation transformation which brings the object from a reference pose to the observed pose. This rotation transformation can be represented in different ways, e.g., as a rotation matrix or a quaternion. "
    },
    {
        "id": "TASK_prediction-intervals",
        "name": "Prediction Intervals",
        "description": "A prediction interval is an estimate of an interval in which a future observation will fall, with a certain probability, given what has already been observed. Prediction intervals are often used in regression analysis.",
        "type": "Task",
        "probable_wikipedia": "Prediction interval",
        "score": "11.00454",
        "probable_description": " In statistical inference, specifically predictive inference, a prediction interval is an estimate of an interval in which a future observation will fall, with a certain probability, given what has already been observed. Prediction intervals are often used in regression analysis.  Prediction intervals are used in both frequentist statistics and Bayesian statistics: a prediction interval bears the same relationship to a future observation that a frequentist confidence interval or Bayesian credible interval bears to an unobservable population parameter: prediction intervals predict the distribution of individual future points, whereas confidence intervals and credible intervals of parameters predict the distribution of estimates of the true population mean or other quantity of interest that cannot be observed. "
    },
    {
        "id": "TASK_probabilistic-programming",
        "name": "Probabilistic Programming",
        "description": "Probabilistic programming languages are designed to describe probabilistic models and then perform inference in those models. PPLs are closely related to graphical models and Bayesian networks, but are more expressive and flexible.\r\n\r\n<span style=\"color:grey; opacity: 0.6\">( Image credit: [Michael Betancourt](https://betanalpha.github.io/writing/) )</span>",
        "type": "Task",
        "probable_wikipedia": "Probabilistic programming",
        "score": "12.548242",
        "probable_description": " Probabilistic programming (PP) is a programming paradigm in which probabilistic models are specified and inference for these models is performed automatically. It represents an attempt to unify probabilistic modeling and traditional general purpose programming in order to make the former easier and more widely applicable. It can be used to create systems that help make decisions in the face of uncertainty.  Programming languages used for probabilistic programming are referred to as \"Probabilistic programming languages\" (PPLs). "
    },
    {
        "id": "TASK_program-synthesis",
        "name": "Program Synthesis",
        "description": "",
        "type": "Task",
        "probable_wikipedia": "Program synthesis",
        "score": "11.823307",
        "probable_description": " In computer science, program synthesis is the task to automatically construct a program that satisfies a given high-level specification. In contrast to other automatic programming techniques, the specifications are usually non-algorithmic statements of an appropriate logical calculus. Often, program synthesis employs techniques from formal verification. "
    },
    {
        "id": "TASK_protein-folding",
        "name": "Protein Folding",
        "description": "",
        "type": "Task",
        "probable_wikipedia": "Protein folding",
        "score": "11.559556",
        "probable_description": " Protein folding is the physical process by which a protein chain acquires its native 3-dimensional structure, a conformation that is usually biologically functional, in an expeditious and reproducible manner. It is the physical process by which a polypeptide folds into its characteristic and functional three-dimensional structure from random coil. Each protein exists as an unfolded polypeptide or random coil when translated from a sequence of mRNA to a linear chain of amino acids. This polypeptide lacks any stable (long-lasting) three-dimensional structure (the left hand side of the first figure). As the polypeptide chain is being synthesized by a ribosome, the linear chain begins to fold into its three-dimensional structure. Folding begins to occur even during translation of the polypeptide chain. Amino acids interact with each other to produce a well-defined three-dimensional structure, the folded protein (the right hand side of the figure), known as the native state. The resulting three-dimensional structure is determined by the amino acid sequence or primary structure (Anfinsen's dogma).  The correct three-dimensional structure is essential to function, although some parts of functional proteins may remain unfolded, so that protein dynamics is important. Failure to fold into native structure generally produces inactive proteins, but in some instances misfolded proteins have modified or toxic functionality. Several neurodegenerative and other diseases are believed to result from the accumulation of amyloid fibrils formed by misfolded proteins. Many allergies are caused by incorrect folding of some proteins, because the immune system does not produce antibodies for certain protein structures.  Denaturation of proteins is a"
    },
    {
        "id": "TASK_protein-function-prediction",
        "name": "Protein Function Prediction",
        "description": "",
        "type": "Task",
        "probable_wikipedia": "Protein function prediction",
        "score": "12.287173",
        "probable_description": " Protein function prediction methods are techniques that bioinformatics researchers use to assign biological or biochemical roles to proteins. These proteins are usually ones that are poorly studied or predicted based on genomic sequence data. These predictions are often driven by data-intensive computational procedures. Information may come from nucleic acid sequence homology, gene expression profiles, protein domain structures, text mining of publications, phylogenetic profiles, phenotypic profiles, and protein-protein interaction. Protein function is a broad term: the roles of proteins range from catalysis of biochemical reactions to transport to signal transduction, and a single protein may play a role in multiple processes or cellular pathways.  Generally, function can be thought of as, \"anything that happens to or through a protein\". The Gene Ontology Consortium provides a useful classification of functions, based on a dictionary of well-defined terms divided into three main categories of \"molecular function, biological process\" and \"cellular component\". Researchers can query this database with a protein name or accession number to retrieve associated Gene Ontology (GO) terms or annotations based on computational or experimental evidence.  While techniques such as microarray analysis, RNA interference, and the yeast two-hybrid system can be used to experimentally demonstrate the function of a protein, advances in sequencing technologies have made the rate at which proteins can be experimentally characterized much slower than the rate at which new sequences become available. Thus, the annotation of new sequences is mostly by \"prediction\" through computational methods, as these types of annotation can often be done quickly and for many genes or"
    },
    {
        "id": "TASK_protein-structure-prediction",
        "name": "Protein Structure Prediction",
        "description": "",
        "type": "Task",
        "probable_wikipedia": "Protein structure prediction",
        "score": "12.24573",
        "probable_description": " Protein structure prediction is the inference of the three-dimensional structure of a protein from its amino acid sequence\u2014that is, the prediction of its folding and its secondary and tertiary structure from its primary structure. Structure prediction is fundamentally different from the inverse problem of protein design. Protein structure prediction is one of the most important goals pursued by bioinformatics and theoretical chemistry; it is highly important in medicine (for example, in drug design) and biotechnology (for example, in the design of novel enzymes). Every two years, the performance of current methods is assessed in the CASP experiment (Critical Assessment of Techniques for Protein Structure Prediction). A continuous evaluation of protein structure prediction web servers is performed by the community project CAMEO3D. "
    },
    {
        "id": "METHOD_q-learning",
        "name": "Q-Learning",
        "full_name": "Q-Learning",
        "description": "Q-Learning is an off-policy temporal difference control algorithm:\n$$Q\\left(S_{t}, A_{t}\\right) \\leftarrow Q\\left(S_{t}, A_{t}\\right) + \\alpha\\left[R_{t+1} + \\gamma\\max_{a}Q\\left(S_{t+1}, a\\right) - Q\\left(S_{t}, A_{t}\\right)\\right] $$\nThe learned action-value function $Q$ directly approximates $q_{*}$, the optimal action-value function, independent of the policy being followed.\nSource: Sutton and Barto, Reinforcement Learning, 2nd Edition",
        "paper": null,
        "category": [
            "Off-Policy TD Control"
        ],
        "type": "Method",
        "probable_wikipedia": "Q-learning",
        "score": "12.356554",
        "probable_description": " \"Q\"-learning is a model-free reinforcement learning algorithm. The goal of Q-learning is to learn a policy, which tells an agent what action to take under what circumstances. It does not require a model (hence the connotation \"model-free\") of the environment, and it can handle problems with stochastic transitions and rewards, without requiring adaptations.  For any finite Markov decision process (FMDP), \"Q\"-learning finds a policy that is optimal in the sense that it maximizes the expected value of the total reward over any and all successive steps, starting from the current state. \"Q\"-learning can identify an optimal action-selection policy for any given FMDP, given infinite exploration time and a partly-random policy. \"Q\" names the function that returns the reward used to provide the reinforcement and can be said to stand for the \"quality\" of an action taken in a given state. "
    },
    {
        "id": "TASK_quantization",
        "name": "Quantization",
        "description": "**Quantization** is a promising technique to reduce the computation cost of neural network training, which can replace high-cost floating-point numbers (e.g., float32) with low-cost fixed-point numbers (e.g., int8/int16).\n\n\n<span class=\"description-source\">Source: [Adaptive Precision Training: Quantify Back Propagation in Neural Networks with Fixed-point Numbers ](https://arxiv.org/abs/1911.00361)</span>",
        "type": "Task",
        "probable_wikipedia": "Quantization",
        "score": "8.422352",
        "probable_description": " Quantization is the process of constraining an input from a continuous or otherwise large set of values (such as the real numbers) to a discrete set (such as the integers). The terms \"quantization\" and \"discretization\" are often denotatively synonymous but not always connotatively interchangeable.     "
    },
    {
        "id": "TASK_quantum-state-tomography",
        "name": "Quantum State Tomography",
        "description": "",
        "type": "Task",
        "probable_wikipedia": "Quantum tomography",
        "score": "10.899851",
        "probable_description": " Quantum tomography or quantum state tomography is the process of reconstructing the quantum state (density matrix) for a source of quantum systems by measurements on the systems coming from the source. The source may be any device or system which prepares quantum states either consistently into quantum pure states or otherwise into general mixed states. To be able to uniquely identify the state, the measurements must be tomographically complete. That is, the measured operators must form an operator basis on the Hilbert space of the system, providing all the information about the state. Such a set of observations is sometimes called a quorum.  In quantum process tomography on the other hand, known quantum states are used to probe a quantum process to find out how the process can be described. Similarly, quantum measurement tomography works to find out what measurement is being performed.  The general principle behind quantum state tomography is that by repeatedly performing many different measurements on quantum systems described by identical density matrices, frequency counts can be used to infer probabilities, and these probabilities are combined with Born's rule to determine a density matrix which fits the best with the observations.  This can be easily understood by making a classical analogy. Consider a harmonic oscillator (e.g. a pendulum). The position and momentum of the oscillator at any given point can be measured and therefore the motion can be completely described by the phase space. This is shown in figure 1. By performing this measurement for a large number of"
    },
    {
        "id": "TASK_question-answering",
        "name": "Question Answering",
        "description": "Question Answering is the task of answering questions (typically reading comprehension questions), but abstaining when presented with a question that cannot be answered based on the provided context. \r\n\r\nQuestion answering can be segmented into domain-specific tasks like community question answering and knowledge-base question answering. Popular benchmark datasets for evaluation question answering systems include [SQuAD](/dataset/squad), [HotPotQA](/dataset/hotpotqa), [bAbI](/dataset/babi-1), [TriviaQA](/dataset/triviaqa), [WikiQA](/dataset/wikiqa), and many others. Models for question answering are typically evaluated on metrics like EM and F1. Some recent top performing models are T5 and XLNet.\r\n\r\n( Image credit: [SQuAD](https://rajpurkar.github.io/mlx/qa-and-squad/) )",
        "type": "Task",
        "probable_wikipedia": "Question answering",
        "score": "11.015567",
        "probable_description": " Question answering (QA) is a computer science discipline within the fields of information retrieval and natural language processing (NLP), which is concerned with building systems that automatically answer questions posed by humans in a natural language. "
    },
    {
        "id": "TASK_reading-comprehension",
        "name": "Reading Comprehension",
        "description": "Most current question answering datasets frame the task as reading comprehension where the question is about a paragraph or document and the answer often is a span in the document. \r\n\r\nSome specific tasks of reading comprehension include multi-modal machine reading comprehension and textual machine reading comprehension, among others. In the literature, machine reading comprehension can be divide into four categories: **cloze style**, **multiple choice**, **span prediction**, and **free-form answer**.  Read more about each category [here](https://paperswithcode.com/paper/a-survey-on-machine-reading-comprehension-1).\r\n\r\nBenchmark datasets used for testing a model's reading comprehension abilities include [MovieQA](/dataset/movieqa), [ReCoRD](dataset/record), and [RACE](/dataset/race), among others.\r\n\r\nThe Machine Reading group at UCL also provides an [overview of reading comprehension tasks](https://uclnlp.github.io/ai4exams/data.html).\r\n\r\nFigure source: [A Survey on Machine Reading Comprehension: Tasks, Evaluation Metrics and Benchmark Datasets](https://arxiv.org/pdf/2006.11880.pdf)",
        "type": "Task",
        "probable_wikipedia": "Reading comprehension",
        "score": "11.449017",
        "probable_description": " Reading comprehension is the ability to process text, understand its meaning, and to integrate with what the reader already knows. Fundamental skills required in efficient reading comprehension are knowing meaning of words, ability to understand meaning of a word from discourse context, ability to follow organization of passage and to identify antecedents and references in it, ability to draw inferences from a passage about its contents, ability to identify the main thought of a passage, ability to answer questions answered in a passage, ability to recognize the literary devices or propositional structures used in a passage and determine its tone, to understand the situational mood (agents, objects, temporal and spatial reference points, casual and intentional inflections, etc.) conveyed for assertions, questioning, commanding, refraining etc. and finally ability to determine writer's purpose, intent and point of view, and draw inferences about the writer (discourse-semantics).  Ability to comprehend text is influenced by reader's skills and their ability to process information. If word recognition is difficult, students use too much of their processing capacity to read individual words, which interferes with their ability to comprehend what is read. There are many reading strategies to improve reading comprehension and inferences, including improving one's vocabulary, critical text analysis (intertextuality, actual events vs. narration of events, etc.) and practicing deep reading. "
    },
    {
        "id": "TASK_real-time-strategy-games",
        "name": "Real-Time Strategy Games",
        "description": "Real-Time Strategy (RTS) tasks involve training an agent to play video games with continuous gameplay and high-level macro-strategic goals such as map control, economic superiority and more.\r\n\r\n<span style=\"color:grey; opacity: 0.6\">( Image credit: [Multi-platform Version of StarCraft: Brood War in a Docker Container](https://github.com/Games-and-Simulations/sc-docker) )</span>",
        "type": "Task",
        "probable_wikipedia": "Real-time strategy",
        "score": "9.20797",
        "probable_description": " Real-time strategy (RTS) is a sub-genre of strategy video games in which the game does not progress incrementally in turns. This is distinguished from turn-based strategy (TBS), in which all players take turns when playing.  In an RTS, the participants position and maneuver units and structures under their control to secure areas of the map and/or destroy their opponents' assets. In a typical RTS, it is possible to create additional units and structures during the course of a game. This is generally limited by a requirement to expend accumulated resources. These resources are in turn garnered by controlling special points on the map and/or possessing certain types of units and structures devoted to this purpose. More specifically, the typical game of the RTS genre features resource gathering, base building, in-game technological development and indirect control of units. The term \"real-time strategy\" was coined by Brett Sperry to market \"Dune II\" in the early 1990s.  The tasks a player must perform to succeed at an RTS can be very demanding, and complex user interfaces have evolved to cope with the challenge. Some features have been borrowed from desktop environments; for example, the technique of \"clicking and dragging\" to select all units under a given area. Though some game genres share conceptual and gameplay similarities with the RTS template, recognized genres are generally not subsumed as RTS games. For instance, city-building games, construction and management simulations, and games of the real-time tactics variety are generally not considered to be \"real-time strategy\". "
    },
    {
        "id": "TASK_recommendation-systems",
        "name": "Recommendation Systems",
        "description": "The recommendation systems task is to produce a list of recommendations for a user.\r\n\r\n<span style=\"color:grey; opacity: 0.6\">( Image credit: [CuMF_SGD](https://arxiv.org/pdf/1610.05838v3.pdf) )</span>",
        "type": "Task",
        "probable_wikipedia": "Recommender system",
        "score": "7.467472",
        "probable_description": " A recommender system or a recommendation system (sometimes replacing \"system\" with a synonym such as platform or engine) is a subclass of information filtering system that seeks to predict the \"rating\" or \"preference\" a user would give to an item. They are primarily used in commercial applications.  Recommender systems are utilized in a variety of areas, and are most commonly recognized as playlist generators for video and music services like Netflix, YouTube and Spotify, product recommenders for services such as Amazon, or content recommenders for social media platforms such as Facebook and Twitter. These systems can operate using a single input, like music, or multiple inputs within and across platforms like news, books, and search queries. There are also popular recommender systems for specific topics like restaurants and online dating. Recommender systems have been developed to explore research articles and experts, collaborators, financial services, and life insurance. "
    },
    {
        "id": "TASK_record-linking",
        "name": "Record linking",
        "description": "The task of finding records in a data set that refer to the same entity across different data sources",
        "type": "Task",
        "probable_wikipedia": "Record linkage",
        "score": "1.8189683",
        "probable_description": " Record linkage (RL) is the task of finding records in a data set that refer to the same entity across different data sources (e.g., data files, books, websites, and databases). Record linkage is necessary when joining data sets based on entities that may or may not share a common identifier (e.g., database key, URI, National identification number), which may be due to differences in record shape, storage location, or curator style or preference. A data set that has undergone RL-oriented reconciliation may be referred to as being \"cross-linked\". Record linkage is called data linkage in many jurisdictions, but is the same process. "
    },
    {
        "id": "TASK_rectification",
        "name": "Rectification",
        "description": "",
        "type": "Task",
        "probable_wikipedia": "Rectification (law)",
        "score": "7.91198",
        "probable_description": " Rectification is a remedy whereby a court orders a change in a written document to reflect what it ought to have said in the first place. It is an equitable remedy, which means the circumstances where it can be applied are limited.  In the United States, this remedy is commonly referred to as reformation. "
    },
    {
        "id": "TASK_referring-expression-generation",
        "name": "Referring expression generation",
        "description": "Generate referring expressions",
        "type": "Task",
        "probable_wikipedia": "Referring expression generation",
        "score": "12.344862",
        "probable_description": " Referring expression generation (REG) is the subtask of natural language generation (NLG) that received most scholarly attention. While NLG is concerned with the conversion of non-linguistic information into natural language, REG focuses only on the creation of referring expressions (noun phrases) that identify specific entities called \"targets\".  This task can be split into two sections. The \"content selection\" part determines which set of properties distinguish the intended target and the \"linguistic realization\" part defines how these properties are translated into natural language. A variety of algorithms have been developed in the NLG community to generate different types of referring expressions. "
    },
    {
        "id": "TASK_relation-extraction",
        "name": "Relation Extraction",
        "description": "**Relation Extraction** is the task of predicting attributes and relations for entities in a sentence. For example, given a sentence \u201cBarack Obama was born in Honolulu, Hawaii.\u201d, a relation classifier aims at predicting the relation of \u201cbornInCity\u201d. Relation Extraction is the key component for building relation knowledge graphs, and it is of crucial significance to natural language processing applications such as structured search, sentiment analysis, question answering, and summarization.\r\n\r\n\r\n<span class=\"description-source\">Source: [Deep Residual Learning for Weakly-Supervised Relation Extraction ](https://arxiv.org/abs/1707.08866)</span>",
        "type": "Task",
        "probable_wikipedia": "Relationship extraction",
        "score": "6.1753826",
        "probable_description": " A relationship extraction task requires the detection and classification of semantic relationship mentions within a set of artifacts, typically from text or XML documents. The task is very similar to that of information extraction (IE), but IE additionally requires the removal of repeated relations (disambiguation) and generally refers to the extraction of many different relationships. "
    },
    {
        "id": "TASK_representation-learning",
        "name": "Representation Learning",
        "description": "Representation learning is concerned with training machine learning algorithms to learn useful representations, e.g. those that are interpretable, have latent features, or can be used for transfer learning.\r\n\r\nDeep neural networks can be considered representation learning models that typically encode information which is projected into a different subspace. These representations are then usually passed on to a linear classifier to, for instance, train a classifier. \r\n\r\nRepresentation learning can be divided into:\r\n\r\n-  **Supervised representation learning**: learning representations on task A using annotated data and used to solve task B\r\n- **Unsupervised representation learning**: learning representations on a task in an unsupervised way (label-free data). These are then used to address downstream tasks and reducing the need for annotated data when learning news tasks. Powerful models like [GPT](/method/gpt) and [BERT](/method/bert) leverage unsupervised representation learning to tackle language tasks.  \r\n\r\nMore recently, [self-supervised learning (SSL)](/task/self-supervised-learning) is one of the main drivers behind unsupervised representation learning in fields like computer vision and NLP. \r\n\r\nHere are some additional readings to go deeper on the task:\r\n\r\n- [Representation Learning: A Review and New Perspectives](/paper/representation-learning-a-review-and-new) - Bengio et al. (2012)\r\n- [A Few Words on Representation Learning](https://sthalles.github.io/a-few-words-on-representation-learning/) - Thalles Silva\r\n\r\n<span style=\"color:grey; opacity: 0.6\">( Image credit: [Visualizing and Understanding Convolutional Networks](https://arxiv.org/pdf/1311.2901.pdf) )</span>",
        "type": "Task",
        "probable_wikipedia": "Feature learning",
        "score": "6.283696",
        "probable_description": " In machine learning, feature learning or representation learning is a set of techniques that allows a system to automatically discover the representations needed for feature detection or classification from raw data. This replaces manual feature engineering and allows a machine to both learn the features and use them to perform a specific task.  Feature learning is motivated by the fact that machine learning tasks such as classification often require input that is mathematically and computationally convenient to process. However, real-world data such as images, video, and sensor data has not yielded to attempts to algorithmically define specific features. An alternative is to discover such features or representations through examination, without relying on explicit algorithms.  Feature learning can be either supervised or unsupervised. "
    },
    {
        "id": "TASK_respiratory-failure",
        "name": "Respiratory Failure",
        "description": "Continuous prediction of onset of respiratory failure in the next 12h given the patient is not in failure now.",
        "type": "Task",
        "probable_wikipedia": "Respiratory failure",
        "score": "10.287612",
        "probable_description": " Respiratory failure results from inadequate gas exchange by the respiratory system, meaning that the arterial oxygen, carbon dioxide or both cannot be kept at normal levels. A drop in the oxygen carried in blood is known as hypoxemia; a rise in arterial carbon dioxide levels is called hypercapnia. Respiratory failure is classified as either Type 1 or Type 2, based on whether there is a high carbon dioxide level. The definition of respiratory failure in clinical trials usually includes increased respiratory rate, abnormal blood gases (hypoxemia, hypercapnia, or both), and evidence of increased work of breathing. Respiratory failure causes an altered mental status due to ischemia in the brain.  The normal partial pressure reference values are: oxygen PaO more than , and carbon dioxide PaCO lesser than . "
    },
    {
        "id": "TASK_robot-navigation",
        "name": "Robot Navigation",
        "description": "The fundamental objective of mobile **Robot Navigation** is to arrive at a goal position without collision. The mobile robot is supposed to be aware of obstacles and move freely in different working scenarios.\n\n\n<span class=\"description-source\">Source: [Learning to Navigate from Simulation via Spatial and Semantic Information Synthesis with Noise Model Embedding ](https://arxiv.org/abs/1910.05758)</span>",
        "type": "Task",
        "probable_wikipedia": "Robot navigation",
        "score": "10.463942",
        "probable_description": " For any mobile device, the ability to navigate in its environment is important. Avoiding dangerous situations such as collisions and unsafe conditions (temperature, radiation, exposure to weather, etc.) comes first, but if the robot has a purpose that relates to specific places in the robot environment, it must find those places. This article will present an overview of the skill of navigation and try to identify the basic blocks of a robot navigation system, types of navigation systems, and closer look at its related building components.  Robot navigation means the robot's ability to determine its own position in its frame of reference and then to plan a path towards some goal location. In order to navigate in its environment, the robot or any other mobility device requires representation, i.e. a map of the environment and the ability to \"interpret\" that representation.  Navigation can be defined as the combination of the three fundamental competences: Some robot navigation systems are capable of simultaneous localization and mapping based on 3D reconstructions of their surroundings.  Robot localization denotes the robot's ability to establish its own position and orientation within the frame of reference. Path planning is effectively an extension of localisation, in that it requires the determination of the robot's current position and a position of a goal location, both within the same frame of reference or coordinates. Map building can be in the shape of a metric map or any notation describing locations in the robot frame of reference. "
    },
    {
        "id": "TASK_robust-design",
        "name": "Robust Design",
        "description": "",
        "type": "Task",
        "probable_wikipedia": "Robust parameter design",
        "score": "1.0601786",
        "probable_description": " A robust parameter design, introduced by Genichi Taguchi, is an experimental design used to exploit the interaction between control and uncontrollable noise variables by robustification -- finding the settings of the control factors that minimize response variation from uncontrollable factors. Control variables are variables of which the experimenter has full control. Noise variables lie on the other side of the spectrum, and while these variables may be easily controlled in an experimental setting, outside of the experimental world they are very hard, if not impossible, to control. Robust parameter designs use a naming convention similar to that of FFDs. A 2 is a 2-level design where \"m1\" is the number of control factors, \"m2\" is the number of noise factors, \"p1\" is the level of fractionation for control factors, and \"p2\" is the level of fractionation for noise factors.  Consider an RPD cake-baking example from Montgomery (2005), where an experimenter wants to improve the quality of cake. While the cake manufacturer can control the amount of flour, amount of sugar, amount of baking powder, and coloring content of the cake, other factors are uncontrollable, such as oven temperature and bake time. The manufacturer can print instructions for a bake time of 20 minutes but in the real world has no control over consumer baking habits. Variations in the quality of the cake can arise from baking at 325\u00b0 instead of 350\u00b0 or from leaving the cake in the oven for a slightly too short or too long period of time. Robust parameter designs seek"
    },
    {
        "id": "TASK_saliency-detection",
        "name": "Saliency Detection",
        "description": "**Saliency Detection** is a preprocessing step in computer vision which aims at finding salient objects in an image.\n\n\n<span class=\"description-source\">Source: [An Unsupervised Game-Theoretic Approach to Saliency Detection ](https://arxiv.org/abs/1708.02476)</span>",
        "type": "Task",
        "probable_wikipedia": "Salience (neuroscience)",
        "score": "0.42681873",
        "probable_description": " The salience (also called saliency) of an item \u2013 be it an object, a person, a pixel, etc. \u2013 is the state or quality by which it stands out from its neighbors. Saliency detection is considered to be a key attentional mechanism that facilitates learning and survival by enabling organisms to focus their limited perceptual and cognitive resources on the most pertinent subset of the available sensory data.  Saliency typically arises from contrasts between items and their neighborhood, such as a red dot surrounded by white dots, a flickering message indicator of an answering machine, or a loud noise in an otherwise quiet environment. Saliency detection is often studied in the context of the visual system, but similar mechanisms operate in other sensory systems. What is salient can be influenced by training: for example, for human subjects particular letters can become salient by training.  When attention deployment is driven by salient stimuli, it is considered to be bottom-up, memory-free, and reactive. Conversely, attention can also be guided by top-down, memory-dependent, or anticipatory mechanisms, such as when looking ahead of moving objects or sideways before crossing streets. Humans and other animals have difficulty paying attention to more than one item simultaneously, so they are faced with the challenge of continuously integrating and prioritizing different bottom-up and top-down influences. "
    },
    {
        "id": "TASK_scene-text-recognition",
        "name": "Scene Text Recognition",
        "description": "See [Scene Text Detection](https://paperswithcode.com/task/scene-text-detection) for leaderboards in this task.",
        "type": "Task",
        "probable_wikipedia": "Scene text",
        "score": "0.3593535",
        "probable_description": " Scene text is text that appears in an image captured by a camera in an outdoor environment. The detection and recognition of scene text from camera captured images are computer vision tasks which became important after smart phones with good cameras became ubiquitous. The text in scene images varies in shape, font, colour and position. The recognition of scene text is further complicated sometimes by non-uniform illumination and focus. To improve scene text recognition, the International Conference on Document Analysis and Recognition (ICDAR) conducts a robust reading competition once in two years. The competition was held in 2003, 2005 and during every ICDAR conference. International association for pattern recognition (IAPR) has created a list of datasets as Reading systems. "
    },
    {
        "id": "TASK_selection-bias",
        "name": "Selection bias",
        "description": "",
        "type": "Task",
        "probable_wikipedia": "Selection bias",
        "score": "12.095869",
        "probable_description": " Selection bias is the bias introduced by the selection of individuals, groups or data for analysis in such a way that proper randomization is not achieved, thereby ensuring that the sample obtained is not representative of the population intended to be analyzed. It is sometimes referred to as the selection effect. The phrase \"selection bias\" most often refers to the distortion of a statistical analysis, resulting from the method of collecting samples. If the selection bias is not taken into account, then some conclusions of the study may be false. "
    },
    {
        "id": "TASK_self-driving-cars",
        "name": "Self-Driving Cars",
        "description": "Self-driving cars : the task of making a car that can drive itself without human guidance.\r\n\r\n<span style=\"color:grey; opacity: 0.6\">( Image credit: [Learning a Driving Simulator](https://github.com/commaai/research) )</span>",
        "type": "Task",
        "probable_wikipedia": "Self-driving car",
        "score": "10.202951",
        "probable_description": " A self-driving car, also known as an autonomous car, driverless car, or robotic car, is a vehicle that is capable of sensing its environment and moving safely with little or no human input.  Self-driving cars combine a variety of sensors to perceive their surroundings, such as radar, lidar, sonar, GPS, odometry and inertial measurement units. Advanced control systems interpret sensory information to identify appropriate navigation paths, as well as obstacles and relevant signage.  Long distance trucks are seen as being in the forefront of adopting and implementing the technology. "
    },
    {
        "id": "TASK_semantic-parsing",
        "name": "Semantic Parsing",
        "description": "**Semantic Parsing** is the task of transducing natural language utterances into formal meaning representations. The target meaning representations can be defined according to a wide variety of formalisms. This include linguistically-motivated semantic representations that are designed to capture the meaning of any sentence such as \u03bb-calculus or the abstract meaning representations. Alternatively, for more task-driven approaches to Semantic Parsing, it is common for meaning representations to represent executable programs such as SQL queries, robotic commands, smart phone instructions, and even general-purpose programming languages like Python and Java.\n\n\n<span class=\"description-source\">Source: [Tranx: A Transition-based Neural Abstract Syntax Parser for Semantic Parsing and Code Generation ](https://arxiv.org/abs/1810.02720)</span>",
        "type": "Task",
        "probable_wikipedia": "Semantic parsing",
        "score": "12.598307",
        "probable_description": " Semantic parsing is the task of converting a natural language utterance to a logical form: a machine-understandable representation of its meaning. Semantic parsing can thus be understood as extracting the precise meaning of an utterance. Applications of semantic parsing include machine translation, question answering and code generation. The phrase was first used in the 1970's by Yorick Wilks as the basis for machine translation programs working with only semantic representations. "
    },
    {
        "id": "TASK_semantic-role-labeling",
        "name": "Semantic Role Labeling",
        "description": "Semantic role labeling aims to model the predicate-argument structure of a sentence\r\nand is often described as answering \"Who did what to whom\". BIO notation is typically\r\nused for semantic role labeling.\r\n\r\nExample:\r\n\r\n| Housing | starts | are | expected | to | quicken | a | bit | from | August\u2019s | pace | \r\n| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | \r\n| B-ARG1 | I-ARG1 | O |  O  |  O  |   V  | B-ARG2 | I-ARG2 | B-ARG3 | I-ARG3 | I-ARG3 |",
        "type": "Task",
        "probable_wikipedia": "Semantic role labeling",
        "score": "12.52003",
        "probable_description": " In natural language processing, semantic role labeling (also called shallow semantic parsing) is the process that assigns labels to words or phrases in a sentence that indicate their semantic role in the sentence, such as that of an agent, goal, or result.  It consists of the detection of the semantic arguments associated with the predicate or verb of a sentence and their classification into their specific roles. For example, given a sentence like \"Mary sold the book to John\", the task would be to recognize the verb \"to sell\" as representing the predicate, \"Mary\" as representing the seller (agent), \"the book\" as representing the goods (theme), and \"John\" as representing the recipient. This is an important step towards making sense of the meaning of a sentence. A semantic analysis of this sort is at a lower-level of abstraction than a syntax tree, i.e. it has more categories, thus groups fewer clauses in each category. For instance, \"the book belongs to me\" would need two labels such as \"possessed\" and \"possessor\" whereas \"the book was sold to John\" would need two other labels such as \"goal\" (or \"theme\") and \"receiver\" (or \"recipient\") even though these two clauses would be very similar as far as \"subject\" and \"object\" functions are concerned. "
    },
    {
        "id": "TASK_semantic-similarity",
        "name": "Semantic Similarity",
        "description": "The main objective **Semantic Similarity** is to measure the distance between the semantic meanings of a pair of words, phrases, sentences, or documents. For example, the word \u201ccar\u201d is more similar to \u201cbus\u201d than it is to \u201ccat\u201d. The two main approaches to measuring Semantic Similarity are knowledge-based approaches and corpus-based, distributional methods.\n\n\n<span class=\"description-source\">Source: [Visual and Semantic Knowledge Transfer for Large Scale Semi-supervised Object Detection ](https://arxiv.org/abs/1801.03145)</span>",
        "type": "Task",
        "probable_wikipedia": "Semantic similarity",
        "score": "11.4077835",
        "probable_description": " Semantic similarity is a metric defined over a set of documents or terms, where the idea of distance between them is based on the likeness of their meaning or semantic content as opposed to similarity which can be estimated regarding their syntactical representation (e.g. their string format). These are mathematical tools used to estimate the strength of the semantic relationship between units of language, concepts or instances, through a numerical description obtained according to the comparison of information supporting their meaning or describing their nature. The term semantic similarity is often confused with semantic relatedness. Semantic relatedness includes any relation between two terms, while semantic similarity only includes \"is a\" relations. For example, \"car\" is similar to \"bus\", but is also related to \"road\" and \"driving\".  Computationally, semantic similarity can be estimated by defining a topological similarity, by using ontologies to define the distance between terms/concepts. For example, a naive metric for the comparison of concepts ordered in a partially ordered set and represented as nodes of a directed acyclic graph (e.g., a taxonomy), would be the shortest-path linking the two concept nodes. Based on text analyses, semantic relatedness between units of language (e.g., words, sentences) can also be estimated using statistical means such as a vector space model to correlate words and textual contexts from a suitable text corpus. "
    },
    {
        "id": "TASK_sentence-completion",
        "name": "Sentence Completion",
        "description": "",
        "type": "Task",
        "probable_wikipedia": "Sentence completion tests",
        "score": "2.0283425",
        "probable_description": " Sentence completion tests are a class of semi-structured projective techniques. Sentence completion tests typically provide respondents with beginnings of sentences, referred to as \"stems\", and respondents then complete the sentences in ways that are meaningful to them. The responses are believed to provide indications of attitudes, beliefs, motivations, or other mental states. Therefore, sentence completion technique, with such advantage, promotes the respondents to disclose their concealed feelings. Notwithstanding, there is debate over whether or not sentence completion tests elicit responses from conscious thought rather than unconscious states. This debate would affect whether sentence completion tests can be strictly categorized as projective tests.  A sentence completion test form may be relatively short, such as those used to assess responses to advertisements, or much longer, such as those used to assess personality. A long sentence completion test is the Forer Sentence Completion Test, which has 100 stems. The tests are usually administered in booklet form where respondents complete the stems by writing words on paper.  The structures of sentence completion tests vary according to the length and relative generality and wording of the sentence stems. Structured tests have longer stems that lead respondents to more specific types of responses; less structured tests provide shorter stems, which produce a wider variety of responses. "
    },
    {
        "id": "TASK_sentence-embedding",
        "name": "Sentence Embedding",
        "description": "",
        "type": "Task",
        "probable_wikipedia": "Sentence embedding",
        "score": "9.121254",
        "probable_description": " Sentence embedding is the collective name for a set of techniques in natural language processing (NLP) where sentences are mapped to vectors of real numbers "
    },
    {
        "id": "TASK_sentence-embeddings",
        "name": "Sentence Embeddings",
        "description": "",
        "type": "Task",
        "probable_wikipedia": "Sentence embedding",
        "score": "7.7476263",
        "probable_description": " Sentence embedding is the collective name for a set of techniques in natural language processing (NLP) where sentences are mapped to vectors of real numbers "
    },
    {
        "id": "TASK_sentence-segmentation",
        "name": "Sentence segmentation",
        "description": "",
        "type": "Task",
        "probable_wikipedia": "Sentence boundary disambiguation",
        "score": "5.4292154",
        "probable_description": " Sentence boundary disambiguation (SBD), also known as sentence breaking, sentence boundary detection, and sentence segmentation, is the problem in natural language processing of deciding where sentences begin and end. Natural language processing tools often require their input to be divided into sentences; however, sentence boundary identification can be challenging due to the potential ambiguity of punctuation marks. In written English, a period may indicate the end of a sentence, or may denote an abbreviation, a decimal point, an ellipsis, or an email address, among other possibilities. About 47% of the periods in the Wall Street Journal corpus denote abbreviations. Question marks and exclamation marks can be similarly ambiguous due to use in emoticons, computer code, and slang.  Languages like Japanese and Chinese have unambiguous sentence-ending markers. "
    },
    {
        "id": "TASK_senter",
        "name": "SENTER",
        "description": "",
        "type": "Task",
        "probable_wikipedia": "Sender",
        "score": "9.106832",
        "probable_description": " A sender was a special type of circuit in 20th-century electromechanical telephone exchanges which registered the telephone numbers dialed by the subscriber, and then transmitted that information to another exchange. In some American exchange designs, for example, the No. 1 Crossbar switch there were both originating senders and terminating senders. The corresponding device in the British director telephone system was called a \"director\" and, in other contexts, \"register\". "
    },
    {
        "id": "TASK_sentiment-analysis",
        "name": "Sentiment Analysis",
        "description": "Sentiment analysis is the task of classifying the polarity of a given text. For instance, a text-based tweet can be categorized into either \"positive\", \"negative\", or \"neutral\". Given the text and accompanying labels, a model can be trained to predict the correct sentiment. \r\n\r\nSentiment analysis techniques can be categorized into machine learning approaches, lexicon-based approaches, and even hybrid methods. Some subcategories of research in sentiment analysis include: multimodal sentiment analysis, aspect-based sentiment analysis, fine-grained opinion analysis, language specific sentiment analysis.\r\n\r\nMore recently, deep learning techniques, such as RoBERTa and T5, are used to train high-performing sentiment classifiers that are evaluated using metrics like F1, recall, and precision. To evaluate sentiment analysis systems, benchmark datasets like SST, GLUE, and IMDB movie reviews are used.\r\n\r\nFurther readings:\r\n\r\n- [Sentiment Analysis Based on Deep Learning: A Comparative Study](https://paperswithcode.com/paper/sentiment-analysis-based-on-deep-learning-a)",
        "type": "Task",
        "probable_wikipedia": "Sentiment analysis",
        "score": "12.151243",
        "probable_description": " Sentiment analysis (also known as opinion mining or emotion AI) refers to the use of natural language processing, text analysis, computational linguistics, and biometrics to systematically identify, extract, quantify, and study affective states and subjective information. Sentiment analysis is widely applied to voice of the customer materials such as reviews and survey responses, online and social media, and healthcare materials for applications that range from marketing to customer service to clinical medicine. "
    },
    {
        "id": "TASK_sequential-pattern-mining",
        "name": "Sequential Pattern Mining",
        "description": "**Sequential Pattern Mining** is the process that discovers relevant patterns between data examples where the values are delivered in a sequence.\n\n\n<span class=\"description-source\">Source: [Big Data Analytics for Large Scale Wireless Networks: Challenges and Opportunities ](https://arxiv.org/abs/1909.08069)</span>",
        "type": "Task",
        "probable_wikipedia": "Sequential pattern mining",
        "score": "11.538522",
        "probable_description": " Sequential pattern mining is a topic of data mining concerned with finding statistically relevant patterns between data examples where the values are delivered in a sequence. It is usually presumed that the values are discrete, and thus time series mining is closely related, but usually considered a different activity. Sequential pattern mining is a special case of structured data mining.  There are several key traditional computational problems addressed within this field. These include building efficient databases and indexes for sequence information, extracting the frequently occurring patterns, comparing sequences for similarity, and recovering missing sequence members. In general, sequence mining problems can be classified as \"string mining\" which is typically based on string processing algorithms and \"itemset mining\" which is typically based on association rule learning. \"Local process models\" extend sequential pattern mining to more complex patterns that can include (exclusive) choices, loops, and concurrency constructs in addition to the sequential ordering construct. "
    },
    {
        "id": "TASK_sketch",
        "name": "Sketch",
        "description": "",
        "type": "Task",
        "probable_wikipedia": "Sketch (drawing)",
        "score": "9.516221",
        "probable_description": " A sketch (ultimately from Greek \u03c3\u03c7\u03b5\u03b4\u03b9\u03bf\u03c2 \u2013 \"schedios\", \"done extempore\") is a rapidly executed freehand drawing that is not usually intended as a finished work. A sketch may serve a number of purposes: it might record something that the artist sees, it might record or develop an idea for later use or it might be used as a quick way of graphically demonstrating an image, idea or principle.  Sketches can be made in any drawing medium. The term is most often applied to graphic work executed in a dry medium such as silverpoint, graphite, pencil, charcoal or pastel. It may also apply to drawings executed in pen and ink, digital input such as a digital pen, ballpoint pen, marker pen, water colour and oil paint. The latter two are generally referred to as \"water colour sketches\" and \"oil sketches\". A sculptor might model three-dimensional sketches in clay, plasticine or wax. "
    },
    {
        "id": "TASK_sketch-recognition",
        "name": "Sketch Recognition",
        "description": "",
        "type": "Task",
        "probable_wikipedia": "Sketch recognition",
        "score": "11.102568",
        "probable_description": " Sketch recognition is the automated recognition of hand-drawn diagrams by a computer. Research in sketch recognition lies at the crossroads of Artificial Intelligence and Human Computer Interaction. Recognition algorithms usually are gesture-based, appearance-based, geometry-based, or a combination thereof.    "
    },
    {
        "id": "TASK_skills-assessment",
        "name": "Skills Assessment",
        "description": "",
        "type": "Task",
        "probable_wikipedia": "Skill assessment",
        "score": "8.965711",
        "probable_description": " Competence assessment is a process in which evidence is gathered by the assessor and evaluated against agreed criteria in order to make a judgement of competence. Skill assessment is the comparison of actual performance of a skill with the specified standard for performance of that skill under the circumstances specified by the standard, and evaluation of whether the performance meets or exceed the requirements. Assessment of a skill should comply with the four principles of validity, reliability, fairness and flexibility.  Formative assessment provides feedback for remedial work and coaching, while summative assessment checks whether the competence has been achieved at the end of training. Assessment of combinations of skills and their foundational knowledge may provide greater efficiency, and in some cases competence in one skill my imply competence in other skills. The thoroughness rewired of assessment may depend on the consequences of occasional poor performance. "
    },
    {
        "id": "TASK_solitaire",
        "name": "Solitaire",
        "description": "A family of single-player games using one or more standard decks of playing cards.",
        "type": "Task",
        "probable_wikipedia": "Solitaire",
        "score": "11.277091",
        "probable_description": " Solitaire is any tabletop game which one can play by oneself. The term \"solitaire\" is also used for single-player games of concentration and skill using a set layout of tiles, pegs or stones rather than cards. These games include peg solitaire and mahjong solitaire. Most solitaire games function as a puzzle which, due to a different starting position, may (or may not) be solved in a different fashion each time.    "
    },
    {
        "id": "TASK_spatial-interpolation",
        "name": "Spatial Interpolation",
        "description": "",
        "type": "Task",
        "probable_wikipedia": "Multivariate interpolation",
        "score": "2.241517",
        "probable_description": " In numerical analysis, multivariate interpolation or spatial interpolation is interpolation on functions of more than one variable.  The function to be interpolated is known at given points formula_1 and the interpolation problem consist of yielding values at arbitrary points formula_2.  Multivariate interpolation is particularly important in geostatistics, where it is used to create a digital elevation model from a set of points on the Earth's surface (for example, spot heights in a topographic survey or depths in a hydrographic survey). "
    },
    {
        "id": "TASK_speaker-identification",
        "name": "Speaker Identification",
        "description": "",
        "type": "Task",
        "probable_wikipedia": "Speaker recognition",
        "score": "3.9126196",
        "probable_description": " Speaker recognition is the identification of a person from characteristics of voices. It is used to answer the question \"Who is speaking?\" The term voice recognition can refer to \"speaker recognition\" or speech recognition. Speaker verification (also called speaker authentication) contrasts with identification, and \"speaker recognition\" differs from \"speaker diarisation\" (recognizing when the same speaker is speaking).  Recognizing the speaker can simplify the task of translating speech in systems that have been trained on specific voices or it can be used to authenticate or verify the identity of a speaker as part of a security process. Speaker recognition has a history dating back some four decades and uses the acoustic features of speech that have been found to differ between individuals. These acoustic patterns reflect both anatomy and learned behavioral patterns. "
    },
    {
        "id": "TASK_speaker-recognition",
        "name": "Speaker Recognition",
        "description": "**Speaker Recognition** is the process of identifying or confirming the identity of a person given his speech segments.\n\n\n<span class=\"description-source\">Source: [Margin Matters: Towards More Discriminative Deep Neural Network Embeddings for Speaker Recognition ](https://arxiv.org/abs/1906.07317)</span>",
        "type": "Task",
        "probable_wikipedia": "Speaker recognition",
        "score": "12.068894",
        "probable_description": " Speaker recognition is the identification of a person from characteristics of voices. It is used to answer the question \"Who is speaking?\" The term voice recognition can refer to \"speaker recognition\" or speech recognition. Speaker verification (also called speaker authentication) contrasts with identification, and \"speaker recognition\" differs from \"speaker diarisation\" (recognizing when the same speaker is speaking).  Recognizing the speaker can simplify the task of translating speech in systems that have been trained on specific voices or it can be used to authenticate or verify the identity of a speaker as part of a security process. Speaker recognition has a history dating back some four decades and uses the acoustic features of speech that have been found to differ between individuals. These acoustic patterns reflect both anatomy and learned behavioral patterns. "
    },
    {
        "id": "TASK_speaker-verification",
        "name": "Speaker Verification",
        "description": "Speaker verification is the verifying the identity of a person from characteristics of the voice.\r\n\r\n<span style=\"color:grey; opacity: 0.6\">( Image credit: [Contrastive-Predictive-Coding-PyTorch\r\n](https://github.com/jefflai108/Contrastive-Predictive-Coding-PyTorch) )</span>",
        "type": "Task",
        "probable_wikipedia": "Speaker recognition",
        "score": "2.937149",
        "probable_description": " Speaker recognition is the identification of a person from characteristics of voices. It is used to answer the question \"Who is speaking?\" The term voice recognition can refer to \"speaker recognition\" or speech recognition. Speaker verification (also called speaker authentication) contrasts with identification, and \"speaker recognition\" differs from \"speaker diarisation\" (recognizing when the same speaker is speaking).  Recognizing the speaker can simplify the task of translating speech in systems that have been trained on specific voices or it can be used to authenticate or verify the identity of a speaker as part of a security process. Speaker recognition has a history dating back some four decades and uses the acoustic features of speech that have been found to differ between individuals. These acoustic patterns reflect both anatomy and learned behavioral patterns. "
    },
    {
        "id": "TASK_speech-dereverberation",
        "name": "Speech Dereverberation",
        "description": "Removing reverberation from audio signals",
        "type": "Task",
        "probable_wikipedia": "Dereverberation",
        "score": "4.311048",
        "probable_description": " Dereverberation is the process by which the effects of reverberation are removed from sound, after such reverberant sound has been picked up by microphones. Dereverberation is a subtopic of acoustic digital signal processing and is most commonly applied to speech but also has relevance in some aspects of music processing. Dereverberation of audio (speech or music) is a corresponding function to blind deconvolution of images, although the techniques used are usually very different. Reverberation itself is caused by sound reflections in a room (or other enclosed space) and is quantified by the room reverberation time and the direct-to-reverberant ratio. The effect of dereverberation is to increase the direct-to-reverberant ratio so that the sound is perceived as closer and clearer.  A main application of dereverberation is in hands-free phones and desktop conferencing terminals because, in these cases, the microphones are not close to the source of sound \u2013 the talker\u2019s mouth \u2013 but at arm\u2019s length or further distance. As well as telecommunications, dereverberation is importantly applied in automatic speech recognition because speech recognizers are usually error-prone in reverberant scenarios.  Dereverberation became established as a topic of scientific research in the years 2000 to 2005., although a few notable early articles exist. The first scientific text book on the topic was published in 2010. A global scientific study sponsored by the IEEE Technical Committee for Audio and Acoustic Signal Processing took place in 2014.  Three different approaches can be followed to perform dereverberation. In the first approach, reverberation is cancelled by exploiting a"
    },
    {
        "id": "TASK_speech-enhancement",
        "name": "Speech Enhancement",
        "description": "Speech enhancement is the task of taking a noisy speech input and producing an enhanced speech output.\r\n\r\n<span style=\"color:grey; opacity: 0.6\">( Image credit: [A Fully Convolutional Neural Network For Speech Enhancement](https://arxiv.org/pdf/1609.07132v1.pdf) )</span>",
        "type": "Task",
        "probable_wikipedia": "Speech enhancement",
        "score": "11.2120695",
        "probable_description": " Speech enhancement aims to improve speech quality by using various algorithms. The objective of enhancement is improvement in intelligibility and/or overall perceptual quality of degraded speech signal using audio signal processing techniques.  Enhancing of speech degraded by noise, or noise reduction, is the most important field of speech enhancement, and used for many applications such as mobile phones, VoIP, teleconferencing systems, speech recognition, and hearing aids "
    },
    {
        "id": "TASK_speech-recognition",
        "name": "Speech Recognition",
        "description": "Speech recognition is the task of recognising speech within audio and converting it into text.\r\n\r\n<span style=\"color:grey; opacity: 0.6\">( Image credit: [SpecAugment](https://arxiv.org/pdf/1904.08779v2.pdf) )</span>",
        "type": "Task",
        "probable_wikipedia": "Speech recognition",
        "score": "11.044818",
        "probable_description": " Speech recognition is a interdisciplinary subfield of computational linguistics that develops methodologies and technologies that enables the recognition and translation of spoken language into text by computers. It is also known as automatic speech recognition (ASR), computer speech recognition or speech to text (STT). It incorporates knowledge and research in the linguistics, computer science, and electrical engineering fields.  Some speech recognition systems require \"training\" (also called \"enrollment\") where an individual speaker reads text or isolated vocabulary into the system. The system analyzes the person's specific voice and uses it to fine-tune the recognition of that person's speech, resulting in increased accuracy. Systems that do not use training are called \"speaker independent\" systems. Systems that use training are called \"speaker dependent\".  Speech recognition applications include voice user interfaces such as voice dialing (e.g. \"call home\"), call routing (e.g. \"I would like to make a collect call\"), domotic appliance control, search (e.g. find a podcast where particular words were spoken), simple data entry (e.g., entering a credit card number), preparation of structured documents (e.g. a radiology report), determining speaker characteristics, speech-to-text processing (e.g., word processors or emails), and aircraft (usually termed direct voice input).  The term \"voice recognition\" or \"speaker identification\" refers to identifying the speaker, rather than what they are saying. Recognizing the speaker can simplify the task of translating speech in systems that have been trained on a specific person's voice or it can be used to authenticate or verify the identity of a speaker as part of a security process. "
    },
    {
        "id": "TASK_speech-synthesis",
        "name": "Speech Synthesis",
        "description": "Speech synthesis is the task of generating speech from some other modality like text, lip movements etc. \r\n\r\nPlease note that the leaderboards here are not really comparable between studies - as they use mean opinion score as a metric and collect different samples from Amazon Mechnical Turk.\r\n\r\n<span style=\"color:grey; opacity: 0.6\">( Image credit: [WaveNet: A generative model for raw audio](https://deepmind.com/blog/article/wavenet-generative-model-raw-audio) )</span>",
        "type": "Task",
        "probable_wikipedia": "Speech synthesis",
        "score": "10.865473",
        "probable_description": " Speech synthesis is the artificial production of human speech. A computer system used for this purpose is called a speech computer or speech synthesizer, and can be implemented in software or hardware products. A text-to-speech (TTS) system converts normal language text into speech; other systems render symbolic linguistic representations like phonetic transcriptions into speech.  Synthesized speech can be created by concatenating pieces of recorded speech that are stored in a database. Systems differ in the size of the stored speech units; a system that stores phones or diphones provides the largest output range, but may lack clarity. For specific usage domains, the storage of entire words or sentences allows for high-quality output. Alternatively, a synthesizer can incorporate a model of the vocal tract and other human voice characteristics to create a completely \"synthetic\" voice output.  The quality of a speech synthesizer is judged by its similarity to the human voice and by its ability to be understood clearly. An intelligible text-to-speech program allows people with visual impairments or reading disabilities to listen to written words on a home computer. Many computer operating systems have included speech synthesizers since the early 1990s.  A text-to-speech system (or \"engine\") is composed of two parts: a front-end and a back-end. The front-end has two major tasks. First, it converts raw text containing symbols like numbers and abbreviations into the equivalent of written-out words. This process is often called \"text normalization\", \"pre-processing\", or \"tokenization\". The front-end then assigns phonetic transcriptions to each word, and divides and marks"
    },
    {
        "id": "TASK_spike-sorting",
        "name": "Spike Sorting",
        "description": "Spike sorting is a class of techniques used in the analysis of electrophysiological data. Spike sorting algorithms use the shape(s) of waveforms collected with one or more electrodes in the brain to distinguish the activity of one or more neurons from background electrical noise.",
        "type": "Task",
        "probable_wikipedia": "Spike sorting",
        "score": "12.278262",
        "probable_description": " Spike sorting is a class of techniques used in the analysis of electrophysiological data. Spike sorting algorithms use the shape(s) of waveforms collected with one or more electrodes in the brain to distinguish the activity of one or more neurons from background electrical noise.  Neurons produce action potentials that are referred to as 'spikes' in laboratory jargon. Frequently this term is used for electrical signals recorded in the vicinity of individual neurons with a microelectrode (exception: 'spikes' in EEG recordings). In these recordings action potentials appear as sharp spikes (deviations from the baseline). These extracellular electrodes pick up all the components constituting the field at the point of its contact. This includes the component due to the synaptic currents and the action potentials. The synaptic currents have slower time course and the spikes have faster time course. They are thus easily separated by filtering: highpass for spikes and low pass for the synaptic mechanisms. The component of the field due to the synaptic mechanism is referred to as the local field potential (LFP). Spike sorting refers to the process of assigning spikes to different neurons. The background to this is that the exact time course of a spike event as recorded by the electrode depends on the size and shape of the neuron, the position of the recording electrode relative to the neuron, etc. These electrodes, positioned outside of the cells in the tissue, however, often 'see' the spikes generated by several neurons in their vicinity. Since the spike shapes are unique and quite"
    },
    {
        "id": "TASK_sports-analytics",
        "name": "Sports Analytics",
        "description": "",
        "type": "Task",
        "probable_wikipedia": "Sports analytics",
        "score": "12.164774",
        "probable_description": " Sports analytics are a collection of relevant, historical, statistics that when properly applied can provide a competitive advantage to a team or individual. Through the collection and analyzation of these data, sports analytics inform players, coaches and other staff in order to facilitate decision making both during and prior to sporting events. The term \"sports analytics\" was popularized in mainstream sports culture following the release of the 2011 film, \"Moneyball\", in which Oakland Athletics General Manager Billy Beane (played by Brad Pitt) relies heavily on the use of analytics to build a competitive team on a minimal budget.  There are two key aspects of sports analytics \u2014 on-field and off-field analytics. On-field analytics deals with improving the on-field performance of teams and players. It digs deep into aspects such as game tactics and player fitness. Off-field analytics deals with the business side of sports. Off-field analytics focuses on helping a sport organisation or body surface patterns and insights through data that would help increase ticket and merchandise sales, improve fan engagement, etc. Off-field analytics essentially uses data to help rightsholders take decisions that would lead to higher growth and increased profitability.  As technology has advanced over the last number of years data collection has become more in-depth and can be conducted with relative ease. Advancements in data collection have allowed for sports analytics to grow as well, leading to the development of advanced statistics as well sport specific technologies that allow for things like game simulations to be conducted by teams prior to"
    },
    {
        "id": "TASK_starcraft",
        "name": "Starcraft",
        "description": "Starcraft I is a RTS game; the task is to train an agent to play the game.\r\n\r\n<span style=\"color:grey; opacity: 0.6\">( Image credit: [Macro Action Selection with Deep Reinforcement Learning in StarCraft](https://arxiv.org/pdf/1812.00336v3.pdf) )</span>",
        "type": "Task",
        "probable_wikipedia": "StarCraft",
        "score": "9.744864",
        "probable_description": " StarCraft is a military science fiction media franchise, created by Chris Metzen and James Phinney and owned by Blizzard Entertainment. The series, set in the beginning of the 26th century, centers on a galactic struggle for dominance among four species\u2014the adaptable and mobile Terrans, the ever-evolving insectoid Zerg, the powerfully enigmatic Protoss, and the \"god-like\" Xel'Naga creator race\u2014in a distant part of the Milky Way galaxy known as the Koprulu Sector. The series debuted with the video game \"StarCraft\" in 1998. It has grown to include a number of other games as well as eight novelizations, two \"Amazing Stories\" articles, a , and other licensed merchandise such as collectible statues and toys.  Blizzard Entertainment began planning \"StarCraft\" in 1995, with a development team led by Metzen and Phinney. The game debuted at E3 1996, and used a modified \"\" game engine. \"StarCraft\" also marked the creation of Blizzard Entertainment's film department; the game introduced high quality cinematics integral to the storyline of the series. Most of the original development team for \"StarCraft\" returned to work on the game's expansion pack, \"\"; the game's development began shortly after \"StarCraft\" was released. In 2001, \"\" began development under Nihilistic Software. Unlike the previous real-time strategy games in the series, \"Ghost\" was to be a stealth-action game. After three years of development, work on the game was postponed in 2004. Development of \"\" began in 2003; the game was announced in May 2007 and was released in July 2010. The \"StarCraft II\" franchise continued with the \"\""
    },
    {
        "id": "TASK_steering-control",
        "name": "Steering Control",
        "description": "",
        "type": "Task",
        "probable_wikipedia": "Steering",
        "score": "1.8727518",
        "probable_description": " Steering is the collection of components, linkages, etc. which allows any vehicle (car, motorcycle, bicycle) to follow the desired course. An exception is the case of rail transport by which rail tracks combined together with railroad switches (and also known as 'points' in British English) provide the steering function. The primary purpose of the steering system is to allow the driver to guide the vehicle. "
    },
    {
        "id": "TASK_steganographics",
        "name": "Steganographics",
        "description": "",
        "type": "Task",
        "probable_wikipedia": "Steganography",
        "score": "8.560683",
        "probable_description": " Steganography ( ) is the practice of concealing a file, message, image, or video within another file, message, image, or video. The word \"steganography\" combines the Greek words \"steganos\" (), meaning \"covered, concealed, or protected\", and \"graphein\" () meaning \"writing\".  The first recorded use of the term was in 1499 by Johannes Trithemius in his \"Steganographia\", a treatise on cryptography and steganography, disguised as a book on magic. Generally, the hidden messages appear to be (or to be part of) something else: images, articles, shopping lists, or some other cover text. For example, the hidden message may be in invisible ink between the visible lines of a private letter. Some implementations of steganography that lack a shared secret are forms of security through obscurity, and key-dependent steganographic schemes adhere to Kerckhoffs's principle.  The advantage of steganography over cryptography alone is that the intended secret message does not attract attention to itself as an object of scrutiny. Plainly visible encrypted messages, no matter how unbreakable they are, arouse interest and may in themselves be incriminating in countries in which encryption is illegal.  Whereas cryptography is the practice of protecting the contents of a message alone, steganography is concerned both with concealing the fact that a secret message is being sent and its contents.  Steganography includes the concealment of information within computer files. In digital steganography, electronic communications may include steganographic coding inside of a transport layer, such as a document file, image file, program or protocol. Media files are ideal for steganographic"
    },
    {
        "id": "TASK_stochastic-block-model",
        "name": "Stochastic Block Model",
        "description": "",
        "type": "Task",
        "probable_wikipedia": "Stochastic block model",
        "score": "11.913127",
        "probable_description": " The stochastic block model is a generative model for random graphs. This model tends to produce graphs containing \"communities\", subsets characterized by being connected with one another with particular edge densities. For example, edges may be more common within communities than between communities. The stochastic block model is important in statistics, machine learning, and network science, where it serves as a useful benchmark for the task of recovering community structure in graph data. "
    },
    {
        "id": "TASK_stochastic-optimization",
        "name": "Stochastic Optimization",
        "description": "**Stochastic Optimization** is the task of optimizing certain objective functional by generating and using stochastic random variables. Usually the Stochastic Optimization is an iterative process of generating random variables that progressively finds out the minima or the maxima of the objective functional. Stochastic Optimization is usually applied in the non-convex functional spaces where the usual deterministic optimization such as linear or quadratic programming or their variants cannot be used.\n\n\n<span class=\"description-source\">Source: [ASOC: An Adaptive Parameter-free Stochastic Optimization Techinique for Continuous Variables ](https://arxiv.org/abs/1506.08004)</span>",
        "type": "Task",
        "probable_wikipedia": "Stochastic optimization",
        "score": "11.712424",
        "probable_description": " Stochastic optimization (SO) methods are optimization methods that generate and use random variables. For stochastic problems, the random variables appear in the formulation of the optimization problem itself, which involves random objective functions or random constraints. Stochastic optimization methods also include methods with random iterates. Some stochastic optimization methods use random iterates to solve stochastic problems, combining both meanings of stochastic optimization. Stochastic optimization methods generalize deterministic methods for deterministic problems. "
    },
    {
        "id": "TASK_stock-market-prediction",
        "name": "Stock Market Prediction",
        "description": "",
        "type": "Task",
        "probable_wikipedia": "Stock market prediction",
        "score": "11.382684",
        "probable_description": " Stock market prediction is the act of trying to determine the future value of a company stock or other financial instrument traded on an exchange. The successful prediction of a stock's future price could yield significant profit. The efficient-market hypothesis suggests that stock prices reflect all currently available information and any price changes that are not based on newly revealed information thus are inherently unpredictable. Others disagree and those with this viewpoint possess myriad methods and technologies which purportedly allow them to gain future price information. "
    },
    {
        "id": "TASK_stock-prediction",
        "name": "Stock Prediction",
        "description": "",
        "type": "Task",
        "probable_wikipedia": "Stock market prediction",
        "score": "5.1992116",
        "probable_description": " Stock market prediction is the act of trying to determine the future value of a company stock or other financial instrument traded on an exchange. The successful prediction of a stock's future price could yield significant profit. The efficient-market hypothesis suggests that stock prices reflect all currently available information and any price changes that are not based on newly revealed information thus are inherently unpredictable. Others disagree and those with this viewpoint possess myriad methods and technologies which purportedly allow them to gain future price information. "
    },
    {
        "id": "TASK_stock-price-prediction",
        "name": "Stock Price Prediction",
        "description": "",
        "type": "Task",
        "probable_wikipedia": "Stock market prediction",
        "score": "5.4739766",
        "probable_description": " Stock market prediction is the act of trying to determine the future value of a company stock or other financial instrument traded on an exchange. The successful prediction of a stock's future price could yield significant profit. The efficient-market hypothesis suggests that stock prices reflect all currently available information and any price changes that are not based on newly revealed information thus are inherently unpredictable. Others disagree and those with this viewpoint possess myriad methods and technologies which purportedly allow them to gain future price information. "
    },
    {
        "id": "TASK_structured-prediction",
        "name": "Structured Prediction",
        "description": "**Structured Prediction** is an area of machine learning focusing on representations of spaces with combinatorial structure, and algorithms for inference and parameter estimation over these structures. Core methods include both tractable exact approaches like dynamic programming and spanning tree algorithms as well as heuristic techniques such as linear programming relaxations and greedy search.\n\n\n<span class=\"description-source\">Source: [Torch-Struct: Deep Structured Prediction Library ](https://arxiv.org/abs/2002.00876)</span>",
        "type": "Task",
        "probable_wikipedia": "Structured prediction",
        "score": "12.385747",
        "probable_description": " Structured prediction or structured (output) learning is an umbrella term for supervised machine learning techniques that involves predicting structured objects, rather than scalar discrete or real values.  Similar to commonly used supervised learning techniques, structured prediction models are typically trained by means of observed data in which the true prediction value is used to adjust model parameters. Due to the complexity of the model and the interrelations of predicted variables the process of prediction using a trained model and of training itself is often computationally infeasible and approximate inference and learning methods are used. "
    },
    {
        "id": "TASK_surface-reconstruction",
        "name": "Surface Reconstruction",
        "description": "",
        "type": "Task",
        "probable_wikipedia": "Surface reconstruction",
        "score": "10.737207",
        "probable_description": " Surface reconstruction refers to the process by which atoms at the surface of a crystal assume a different structure than that of the bulk. Surface reconstructions are important in that they help in the understanding of surface chemistry for various materials, especially in the case where another material is adsorbed onto the surface. "
    },
    {
        "id": "TASK_survey-sampling",
        "name": "Survey Sampling",
        "description": "",
        "type": "Task",
        "probable_wikipedia": "Survey sampling",
        "score": "10.174927",
        "probable_description": " In statistics, survey sampling describes the process of selecting a sample of elements from a target population to conduct a survey. The term \"survey\" may refer to many different types or techniques of observation. In survey sampling it most often involves a questionnaire used to measure the characteristics and/or attitudes of people. Different ways of contacting members of a sample once they have been selected is the subject of survey data collection. The purpose of sampling is to reduce the cost and/or the amount of work that it would take to survey the entire target population. A survey that measures the entire target population is called a census. A sample refers to a group or section of a population from which information is to be obtained  Survey samples can be broadly divided into two types: probability samples and super samples. Probability-based samples implement a sampling plan with specified probabilities (perhaps adapted probabilities specified by an adaptive procedure). Probability-based sampling allows design-based inference about the target population. The inferences are based on a known objective probability distribution that was specified in the study protocol. Inferences from probability-based surveys may still suffer from many types of bias.  Surveys that are not based on probability sampling have greater difficulty measuring their bias or sampling error. Surveys based on non-probability samples often fail to represent the people in the target population.  In academic and government survey research, probability sampling is a standard procedure. In the United States, the Office of Management and Budget's \"List of Standards"
    },
    {
        "id": "TASK_survival-analysis",
        "name": "Survival Analysis",
        "description": "**Survival Analysis** is a branch of statistics focused on the study of time-to-event data, usually called survival times. This type of data appears in a wide range of applications such as failure times in mechanical systems, death times of patients in a clinical trial or duration of unemployment in a population. One of the main objectives of Survival Analysis is the estimation of the so-called survival function and the hazard function. If a random variable has density function $f$ and cumulative distribution function $F$, then its survival function $S$ is $1-F$, and its hazard $\u00ce\u00bb$ is $f/S$.\r\n\r\n\r\n<span class=\"description-source\">Source: [Gaussian Processes for Survival Analysis ](https://arxiv.org/abs/1611.00817)</span>\r\n\r\nImage: [Kvamme et al.](https://arxiv.org/pdf/1910.06724v1.pdf)",
        "type": "Task",
        "probable_wikipedia": "Survival analysis",
        "score": "12.641371",
        "probable_description": " Survival analysis is a branch of statistics for analyzing the expected duration of time until one or more events happen, such as death in biological organisms and failure in mechanical systems. This topic is called reliability theory or reliability analysis in engineering, duration analysis or duration modelling in economics, and event history analysis in sociology. Survival analysis attempts to answer questions such as: what is the proportion of a population which will survive past a certain time? Of those that survive, at what rate will they die or fail? Can multiple causes of death or failure be taken into account? How do particular circumstances or characteristics increase or decrease the probability of survival?  To answer such questions, it is necessary to define \"lifetime\". In the case of biological survival, death is unambiguous, but for mechanical reliability, failure may not be well-defined, for there may well be mechanical systems in which failure is partial, a matter of degree, or not otherwise localized in time. Even in biological problems, some events (for example, heart attack or other organ failure) may have the same ambiguity. The theory outlined below assumes well-defined events at specific times; other cases may be better treated by models which explicitly account for ambiguous events.  More generally, survival analysis involves the modelling of time to event data; in this context, death or failure is considered an \"event\" in the survival analysis literature \u2013 traditionally only a single event occurs for each subject, after which the organism or mechanism is dead or broken."
    },
    {
        "id": "TASK_tag",
        "name": "TAG",
        "description": "",
        "type": "Task",
        "probable_wikipedia": "Tag (2015 film)",
        "score": "4.986866",
        "probable_description": " Tag, known in Japan as , is a 2015 Japanese suspense action horror art film directed by Sion Sono and inspired by the title of the novel \"Riaru Onigokko\" by Yusuke Yamada. It was released in Japan on July 11, 2015. The film's theme song, \"Real Onigokko\", was written and performed for the movie by the rock band Glim Spanky. "
    },
    {
        "id": "TASK_template-matching",
        "name": "Template Matching",
        "description": "",
        "type": "Task",
        "probable_wikipedia": "Template matching",
        "score": "12.1989",
        "probable_description": " Template matching is a technique in digital image processing for finding small parts of an image which match a template image. It can be used in manufacturing as a part of quality control, a way to navigate a mobile robot, or as a way to detect edges in images.  The main challenges in the template matching task are: occlusion, detection of non-rigid transformations, illumination and background changes, background clutter and scale changes. "
    },
    {
        "id": "TASK_temporal-logic",
        "name": "Temporal Logic",
        "description": "Learning logic with respect to time and ordering of events.",
        "type": "Task",
        "probable_wikipedia": "Temporal logic",
        "score": "11.533715",
        "probable_description": " In logic, temporal logic is any system of rules and symbolism for representing, and reasoning about, propositions qualified in terms of time (for example, \"I am \"always\" hungry\", \"I will \"eventually\" be hungry\", or \"I will be hungry \"until\" I eat something\"). It is sometimes also used to refer to tense logic, a modal logic-based system of temporal logic introduced by Arthur Prior in the late 1950s, with important contributions by Hans Kamp. It has been further developed by computer scientists, notably Amir Pnueli, and logicians.  Temporal logic has found an important application in formal verification, where it is used to state requirements of hardware or software systems. For instance, one may wish to say that \"whenever\" a request is made, access to a resource is \"eventually\" granted, but it is \"never\" granted to two requestors simultaneously. Such a statement can conveniently be expressed in a temporal logic. "
    },
    {
        "id": "TASK_tensor-decomposition",
        "name": "Tensor Decomposition",
        "description": "",
        "type": "Task",
        "probable_wikipedia": "Tensor decomposition",
        "score": "10.169789",
        "probable_description": " In multilinear algebra, a tensor decomposition is any scheme for expressing a tensor as a sequence of elementary operations acting on other, often simpler tensors. Many tensor decompositions generalize some matrix decompositions.  The main tensor decompositions are:  "
    },
    {
        "id": "TASK_tensor-networks",
        "name": "Tensor Networks",
        "description": "",
        "type": "Task",
        "probable_wikipedia": "Tensor network theory",
        "score": "0.83671683",
        "probable_description": " Tensor network theory is a theory of brain function (particularly that of the cerebellum) that provides a mathematical model of the transformation of sensory space-time coordinates into motor coordinates and vice versa by cerebellar neuronal networks. The theory was developed by Andras Pellionisz and Rodolfo Llinas in the 1980s as a geometrization of brain function (especially of the central nervous system) using tensors. "
    },
    {
        "id": "TASK_term-extraction",
        "name": "Term Extraction",
        "description": "Term Extraction, or Automated Term Extraction (ATE), is about extraction domain-specific terms from natural language text. \r\nFor example, the sentence \u201cWe meta-analyzed mortality using random-effect models\u201d contains the domain-specific single-word terms \"meta-analyzed\", \"mortality\" and the multi-word term \"random-effect models\".",
        "type": "Task",
        "probable_wikipedia": "Terminology extraction",
        "score": "7.686356",
        "probable_description": " Terminology extraction (also known as term extraction, glossary extraction, term recognition, or terminology mining) is a subtask of information extraction. The goal of terminology extraction is to automatically extract relevant terms from a given corpus.  In the semantic web era, a growing number of communities and networked enterprises started to access and interoperate through the internet. Modeling these communities and their information needs is important for several web applications, like topic-driven web crawlers, web services, recommender systems, etc. The development of terminology extraction is also essential to the language industry.  One of the first steps to model the knowledge domain of a virtual community is to collect a vocabulary of domain-relevant terms, constituting the linguistic surface manifestation of domain concepts. Several methods to automatically extract technical terms from domain-specific document warehouses have been described in the literature.  Typically, approaches to automatic term extraction make use of linguistic processors (part of speech tagging, phrase chunking) to extract terminological candidates, i.e. syntactically plausible terminological noun phrases, NPs (e.g. compounds \"credit card\", adjective-NPs \"local tourist information office\", and prepositional-NPs \"board of directors\" - in English, the first two constructs are the most frequent). Terminological entries are then filtered from the candidate list using statistical and machine learning methods. Once filtered, because of their low ambiguity and high specificity, these terms are particularly useful for conceptualizing a knowledge domain or for supporting the creation of a domain ontology or a terminology base. Furthermore, terminology extraction is a very useful starting point for semantic similarity, knowledge management,"
    },
    {
        "id": "TASK_test-unseen",
        "name": "Test unseen",
        "description": "",
        "type": "Task",
        "probable_wikipedia": "Unseen examination",
        "score": "0.4671987",
        "probable_description": " In the UK, an unseen examination is an essay test in school or college, where the student does not know what questions are going to be asked. The student is required to answer questions based upon what they have learned over the course of their academic study.  "
    },
    {
        "id": "TASK_text-based-games",
        "name": "text-based games",
        "description": "Text-based games to evaluate the Reinforcement Learning Agents",
        "type": "Task",
        "probable_wikipedia": "Text-based game",
        "score": "10.785334",
        "probable_description": " A text game or text-based game is an electronic game that uses a text-based user interface, that is, the user interface employs a set of encodable characters such as ASCII instead of bitmap or vector graphics.  Text-based games have been well documented since at least the 1960s, when teleprinters were interlaced with mainframe computers as a form of input, where the output was printed on paper. With that, notable titles were developed for those computers using the teleprinter in the 1960s and 1970s, and numerous more have been developed for video terminals since at least the mid-1970s, having reached their peak popularity in that decade and the 1980s, and continued as early online games into the mid-1990s.  Although generally replaced in favor of video games that utilize non-textual graphics, text-based games continue to be written by independent developers. They have been the basis of instigating genres of video gaming, especially adventure and role-playing video games. "
    },
    {
        "id": "TASK_text-clustering",
        "name": "Text Clustering",
        "description": "Grouping a set of texts in such a way that objects in the same group (called a cluster) are more similar (in some sense) to each other than to those in other groups (clusters). (Source: Adapted from Wikipedia)",
        "type": "Task",
        "probable_wikipedia": "Document clustering",
        "score": "1.5503803",
        "probable_description": " Document clustering (or text clustering) is the application of cluster analysis to textual documents. It has applications in automatic document organization, topic extraction and fast information retrieval or filtering. "
    },
    {
        "id": "TASK_text-segmentation",
        "name": "Text Segmentation",
        "description": "Text segmentation deals with the correct division of a document into semantically coherent blocks.",
        "type": "Task",
        "probable_wikipedia": "Text segmentation",
        "score": "11.589946",
        "probable_description": " Text segmentation is the process of dividing written text into meaningful units, such as words, sentences, or topics. The term applies both to mental processes used by humans when reading text, and to artificial processes implemented in computers, which are the subject of natural language processing. The problem is non-trivial, because while some written languages have explicit word boundary markers, such as the word spaces of written English and the distinctive initial, medial and final letter shapes of Arabic, such signals are sometimes ambiguous and not present in all written languages.  Compare speech segmentation, the process of dividing speech into linguistically meaningful portions. "
    },
    {
        "id": "TASK_text-simplification",
        "name": "Text Simplification",
        "description": "**Text Simplification** is the task of reducing the complexity of the vocabulary and sentence structure of text while retaining its original meaning, with the goal of improving readability and understanding. Simplification has a variety of important societal applications, for example increasing accessibility for those with cognitive disabilities such as aphasia, dyslexia, and autism, or for non-native speakers and children with reading difficulties.\r\n\r\n\r\n<span class=\"description-source\">Source: [Multilingual Unsupervised Sentence Simplification](https://arxiv.org/abs/2005.00352)</span>",
        "type": "Task",
        "probable_wikipedia": "Text simplification",
        "score": "11.711062",
        "probable_description": " Text simplification is an operation used in natural language processing to modify, enhance, classify or otherwise process an existing corpus of human-readable text in such a way that the grammar and structure of the prose is greatly simplified, while the underlying meaning and information remains the same. Text simplification is an important area of research, because natural human languages ordinarily contain complex compound constructions that are not easily processed through automation. In terms of reducing language diversity, semantic compression can be employed to limit and simplify a set of words used in given texts. "
    },
    {
        "id": "TASK_text-summarization",
        "name": "Text Summarization",
        "description": "Shortening a set of data computationally, to create a summary that represents the most important or relevant information within the original content (Source: Wikipedia).",
        "type": "Task",
        "probable_wikipedia": "Automatic summarization",
        "score": "0.17240895",
        "probable_description": " Automatic summarization is the process of shortening a text document with software, in order to create a summary with the major points of the original document. Technologies that can make a coherent summary take into account variables such as length, writing style and syntax.  Automatic data summarization is part of machine learning and data mining. The main idea of summarization is to find a subset of data which contains the \"information\" of the entire set. Such techniques are widely used in industry today. Search engines are an example; others include summarization of documents, image collections and videos. Document summarization tries to create a representative summary or abstract of the entire document, by finding the most informative sentences, while in image summarization the system finds the most representative and important (i.e. salient) images. For surveillance videos, one might want to extract the important events from the uneventful context.  There are two general approaches to automatic summarization: extraction and abstraction. Extractive methods work by selecting a subset of existing words, phrases, or sentences in the original text to form the summary. In contrast, abstractive methods build an internal semantic representation and then use natural language generation techniques to create a summary that is closer to what a human might express. Such a summary might include verbal innovations. Research to date has focused primarily on extractive methods, which are appropriate for image collection summarization and video summarization. "
    },
    {
        "id": "TASK_text-to-speech-synthesis",
        "name": "Text-To-Speech Synthesis",
        "description": "Converting written text in natural language to speech.",
        "type": "Task",
        "probable_wikipedia": "Speech synthesis",
        "score": "3.8986783",
        "probable_description": " Speech synthesis is the artificial production of human speech. A computer system used for this purpose is called a speech computer or speech synthesizer, and can be implemented in software or hardware products. A text-to-speech (TTS) system converts normal language text into speech; other systems render symbolic linguistic representations like phonetic transcriptions into speech.  Synthesized speech can be created by concatenating pieces of recorded speech that are stored in a database. Systems differ in the size of the stored speech units; a system that stores phones or diphones provides the largest output range, but may lack clarity. For specific usage domains, the storage of entire words or sentences allows for high-quality output. Alternatively, a synthesizer can incorporate a model of the vocal tract and other human voice characteristics to create a completely \"synthetic\" voice output.  The quality of a speech synthesizer is judged by its similarity to the human voice and by its ability to be understood clearly. An intelligible text-to-speech program allows people with visual impairments or reading disabilities to listen to written words on a home computer. Many computer operating systems have included speech synthesizers since the early 1990s.  A text-to-speech system (or \"engine\") is composed of two parts: a front-end and a back-end. The front-end has two major tasks. First, it converts raw text containing symbols like numbers and abbreviations into the equivalent of written-out words. This process is often called \"text normalization\", \"pre-processing\", or \"tokenization\". The front-end then assigns phonetic transcriptions to each word, and divides and marks"
    },
    {
        "id": "TASK_texture-synthesis",
        "name": "Texture Synthesis",
        "description": "The fundamental goal of example-based **Texture Synthesis** is to generate a texture, usually larger than the input, that faithfully captures all the visual characteristics of the exemplar, yet is neither identical to it, nor exhibits obvious unnatural looking artifacts.\n\n\n<span class=\"description-source\">Source: [Non-Stationary Texture Synthesis by Adversarial Expansion ](https://arxiv.org/abs/1805.04487)</span>",
        "type": "Task",
        "probable_wikipedia": "Texture synthesis",
        "score": "11.241536",
        "probable_description": " Texture synthesis is the process of algorithmically constructing a large digital image from a small digital sample image by taking advantage of its structural content. It is an object of research in computer graphics and is used in many fields, amongst others digital image editing, 3D computer graphics and post-production of films.  Texture synthesis can be used to fill in holes in images (as in inpainting), create large non-repetitive background images and expand small pictures. "
    },
    {
        "id": "TASK_time-series",
        "name": "Time Series",
        "description": "Time series deals with sequential data where the data is indexed (ordered) by a time dimension.\r\n\r\n<span style=\"color:grey; opacity: 0.6\">( Image credit: [Autoregressive CNNs for Asynchronous Time Series](https://arxiv.org/pdf/1703.04122v4.pdf) )</span>",
        "type": "Task",
        "probable_wikipedia": "Time series",
        "score": "11.465518",
        "probable_description": " A time series is a series of data points indexed (or listed or graphed) in time order. Most commonly, a time series is a sequence taken at successive equally spaced points in time. Thus it is a sequence of discrete-time data. Examples of time series are heights of ocean tides, counts of sunspots, and the daily closing value of the Dow Jones Industrial Average.  Time series are very frequently plotted via line charts. Time series are used in statistics, signal processing, pattern recognition, econometrics, mathematical finance, weather forecasting, earthquake prediction, electroencephalography, control engineering, astronomy, communications engineering, and largely in any domain of applied science and engineering which involves temporal measurements.  Time series \"analysis comprises methods for analyzing time series data in order to extract meaningful statistics and other characteristics of the data. Time series \"forecasting is the use of a model to predict future values based on previously observed values. While regression analysis is often employed in such a way as to test theories that the current values of one or more independent time series affect the current value of another time series, this type of analysis of time series is not called \"time series analysis\", which focuses on comparing values of a single time series or multiple dependent time series at different points in time. Interrupted time series analysis is the analysis of interventions on a single time series.  Time series data have a natural temporal ordering. This makes time series analysis distinct from cross-sectional studies, in which there is no"
    },
    {
        "id": "TASK_tomographic-reconstructions",
        "name": "Tomographic Reconstructions",
        "description": "",
        "type": "Task",
        "probable_wikipedia": "Tomographic reconstruction",
        "score": "7.2315006",
        "probable_description": " Tomographic reconstruction is a type of multidimensional inverse problem where the challenge is to yield an estimate of a specific system from a finite number of projections. The mathematical basis for tomographic imaging was laid down by Johann Radon. A notable example of applications is the reconstruction of computed tomography (CT) where cross-sectional images of patients are obtained in non-invasive manner. Recent developments have seen the Radon transform and its inverse used for tasks related to realistic object insertion required for testing and evaluating computed tomography use in airport security.  This article applies in general to reconstruction methods for all kinds of tomography, but some of the terms and physical descriptions refer directly to the reconstruction of X-ray computed tomography. "
    },
    {
        "id": "TASK_tone-mapping",
        "name": "Tone Mapping",
        "description": "",
        "type": "Task",
        "probable_wikipedia": "Tone mapping",
        "score": "12.315266",
        "probable_description": " Tone mapping is a technique used in image processing and computer graphics to map one set of colors to another to approximate the appearance of high-dynamic-range images in a medium that has a more limited dynamic range. Print-outs, CRT or LCD monitors, and projectors all have a limited dynamic range that is inadequate to reproduce the full range of light intensities present in natural scenes. Tone mapping addresses the problem of strong contrast reduction from the scene radiance to the displayable range while preserving the image details and color appearance important to appreciate the original scene content. "
    },
    {
        "id": "TASK_topic-models",
        "name": "Topic Models",
        "description": "A topic model is a type of statistical model for discovering the abstract \"topics\" that occur in a collection of documents. Topic modeling is a frequently used text-mining tool for the discovery of hidden semantic structures in a text body.",
        "type": "Task",
        "probable_wikipedia": "Topic model",
        "score": "9.047914",
        "probable_description": " In machine learning and natural language processing, a topic model is a type of statistical model for discovering the abstract \"topics\" that occur in a collection of documents. Topic modeling is a frequently used text-mining tool for discovery of hidden semantic structures in a text body. Intuitively, given that a document is about a particular topic, one would expect particular words to appear in the document more or less frequently: \"dog\" and \"bone\" will appear more often in documents about dogs, \"cat\" and \"meow\" will appear in documents about cats, and \"the\" and \"is\" will appear equally in both. A document typically concerns multiple topics in different proportions; thus, in a document that is 10% about cats and 90% about dogs, there would probably be about 9 times more dog words than cat words. The \"topics\" produced by topic modeling techniques are clusters of similar words. A topic model captures this intuition in a mathematical framework, which allows examining a set of documents and discovering, based on the statistics of the words in each, what the topics might be and what each document's balance of topics is.  Topic models are also referred to as probabilistic topic models, which refers to statistical algorithms for discovering the latent semantic structures of an extensive text body. In the age of information, the amount of the written material we encounter each day is simply beyond our processing capacity. Topic models can help to organize and offer insights for us to understand large collections of unstructured text bodies. Originally"
    },
    {
        "id": "TASK_topological-data-analysis",
        "name": "Topological Data Analysis",
        "description": "",
        "type": "Task",
        "probable_wikipedia": "Topological data analysis",
        "score": "11.504563",
        "probable_description": " In applied mathematics, topological data analysis (TDA) is an approach to the analysis of datasets using techniques from topology. Extraction of information from datasets that are high-dimensional, incomplete and noisy is generally challenging. TDA provides a general framework to analyze such data in a manner that is insensitive to the particular metric chosen and provides dimensionality reduction and robustness to noise. Beyond this, it inherits functoriality, a fundamental concept of modern mathematics, from its topological nature, which allows it to adapt to new mathematical tools.  The initial motivation is to study the shape of data. TDA has combined algebraic topology and other tools from pure mathematics to allow mathematically rigorous study of \"shape\". The main tool is persistent homology, an adaptation of homology to point cloud data. Persistent homology has been applied to many types of data across many fields. Moreover, its mathematical foundation is also of theoretical importance. The unique features of TDA make it a promising bridge between topology and geometry. "
    },
    {
        "id": "TASK_traffic-classification",
        "name": "Traffic Classification",
        "description": "**Traffic Classification** is a task of categorizing traffic flows into application-aware classes such as chats, streaming, VoIP, etc. Classification can be used for several purposes including policy enforcement and control or QoS management.\n\n\n<span class=\"description-source\">Source: [Classification of Traffic Using Neural Networks by Rejecting: a Novel Approach in Classifying VPN Traffic ](https://arxiv.org/abs/2001.03665)</span>",
        "type": "Task",
        "probable_wikipedia": "Traffic classification",
        "score": "12.135732",
        "probable_description": " Traffic classification is an automated process which categorises computer network traffic according to various parameters (for example, based on port number or protocol) into a number of \"traffic classes\". Each resulting traffic class can be treated differently in order to differentiate the service implied for the data generator or consumer. "
    },
    {
        "id": "TASK_traffic-sign-detection",
        "name": "Traffic Sign Detection",
        "description": "",
        "type": "Task",
        "probable_wikipedia": "Traffic-sign recognition",
        "score": "1.3090073",
        "probable_description": " Traffic-sign recognition (TSR) is a technology by which a vehicle is able to recognize the traffic signs put on the road e.g. \"speed limit\" or \"children\" or \"turn ahead\". This is part of the features collectively called ADAS. The technology is being developed by a variety of automotive suppliers. It uses image processing techniques to detect the traffic signs. The detection methods can be generally divided into color based, shape based and learning based methods. "
    },
    {
        "id": "TASK_traffic-sign-recognition",
        "name": "Traffic Sign Recognition",
        "description": "Traffic sign recognition is the task of recognising traffic signs in an image or video.\r\n\r\n<span style=\"color:grey; opacity: 0.6\">( Image credit: [Novel Deep Learning Model for Traffic Sign Detection Using Capsule\r\nNetworks ](https://arxiv.org/pdf/1805.04424v1.pdf) )</span>",
        "type": "Task",
        "probable_wikipedia": "Traffic-sign recognition",
        "score": "10.760013",
        "probable_description": " Traffic-sign recognition (TSR) is a technology by which a vehicle is able to recognize the traffic signs put on the road e.g. \"speed limit\" or \"children\" or \"turn ahead\". This is part of the features collectively called ADAS. The technology is being developed by a variety of automotive suppliers. It uses image processing techniques to detect the traffic signs. The detection methods can be generally divided into color based, shape based and learning based methods. "
    },
    {
        "id": "TASK_transfer-learning",
        "name": "Transfer Learning",
        "description": "Transfer learning is a methodology where weights from a model trained on one task are taken and either used (a) to construct a fixed feature extractor, (b) as weight initialization and/or fine-tuning.\r\n\r\n<span style=\"color:grey; opacity: 0.6\">( Image credit: [Subodh Malgonde](https://medium.com/@subodh.malgonde/transfer-learning-using-tensorflow-52a4f6bcde3e) )</span>",
        "type": "Task",
        "probable_wikipedia": "Transfer learning",
        "score": "8.965257",
        "probable_description": " Transfer learning is a research problem in machine learning that focuses on storing knowledge gained while solving one problem and applying it to a different but related problem. For example, knowledge gained while learning to recognize cars could apply when trying to recognize trucks. This area of research bears some relation to the long history of psychological literature on transfer of learning, although formal ties between the two fields are limited. "
    },
    {
        "id": "TASK_translation",
        "name": "Translation",
        "description": "",
        "type": "Task",
        "probable_wikipedia": "Translation",
        "score": "8.792201",
        "probable_description": " Translation is the communication of the meaning of a source-language text by means of an equivalent target-language text. The English language draws a terminological distinction (not all languages do) between \"translating\" (a written text) and \"interpreting\" (oral or sign-language communication between users of different languages); under this distinction, translation can begin only after the appearance of writing within a language community.  A translator always risks inadvertently introducing source-language words, grammar, or syntax into the target-language rendering. On the other hand, such \"spill-overs\" have sometimes imported useful source-language calques and loanwords that have enriched target languages. Translators, including early translators of sacred texts, have helped shape the very languages into which they have translated.  Because of the laboriousness of the translation process, since the 1940s efforts have been made, with varying degrees of success, to automate translation or to mechanically aid the human translator. More recently, the rise of the Internet has fostered a world-wide market for translation services and has facilitated \"language localization\". "
    },
    {
        "id": "TASK_transliteration",
        "name": "Transliteration",
        "description": "**Transliteration** is a mechanism for converting a word in a source (foreign) language to a target language, and often adopts approaches from machine translation. In machine translation, the objective is to preserve the semantic meaning of the utterance as much as possible while following the syntactic structure in the target language. In Transliteration, the objective is to preserve the original pronunciation of the source word as much as possible while following the phonological structures of the target language.\n\nFor example, the city\u2019s name \u201cManchester\u201d has become well known by people of languages other than English. These new words are often named entities that are important in cross-lingual information retrieval, information extraction, machine translation, and often present out-of-vocabulary challenges to spoken language technologies such as automatic speech recognition, spoken keyword search, and text-to-speech.\n\n\n<span class=\"description-source\">Source: [Phonology-Augmented Statistical Framework for Machine Transliteration using Limited Linguistic Resources ](https://arxiv.org/abs/1810.03184)</span>",
        "type": "Task",
        "probable_wikipedia": "Transliteration",
        "score": "10.309484",
        "probable_description": " Transliteration is a type of conversion of a text from one script to another that involves swapping letters (thus \"trans-\" + \"liter-\") in predictable ways (such as \u03b1 \u2192 a, \u0434 \u2192 d, \u03c7 \u2192 ch, \u0576 \u2192 n or \u00e6 \u2192 ae).  For instance, for the Modern Greek term \"\", which is usually translated as \"Hellenic Republic\", the usual transliteration to Latin script is \"\", and the name for Russia in Cyrillic script, \"\", is usually transliterated as \"Rossiya\".  Transliteration is not primarily concerned with representing the sounds of the original but rather with representing the characters, ideally accurately and unambiguously. Thus, in the above example, \u03bb\u03bb is transliterated as 'll', but pronounced /l/; \u0394 is transliterated as 'D', but pronounced /\u00f0/; and \u03b7 is transliterated as 'e', though it is pronounced /i/ (exactly like \u03b9) and is not long.  Conversely, transcription notes the \"sounds\" but not necessarily the spelling. So \"\" could be transcribed as \"\", which does not specify which of the /i/ sounds are written as \u03b7 and which as \u03b9. "
    },
    {
        "id": "TASK_traveling-salesman-problem",
        "name": "Traveling Salesman Problem",
        "description": "",
        "type": "Task",
        "probable_wikipedia": "Travelling salesman problem",
        "score": "10.714427",
        "probable_description": " The travelling salesman problem (TSP) asks the following question: \"Given a list of cities and the distances between each pair of cities, what is the shortest possible route that visits each city and returns to the origin city?\" It is an NP-hard problem in combinatorial optimization, important in operations research and theoretical computer science.  The travelling purchaser problem and the vehicle routing problem are both generalizations of TSP.  In the theory of computational complexity, the decision version of the TSP (where, given a length \"L\", the task is to decide whether the graph has any tour shorter than \"L\") belongs to the class of NP-complete problems. Thus, it is possible that the worst-case running time for any algorithm for the TSP increases superpolynomially (but no more than exponentially) with the number of cities.  The problem was first formulated in 1930 and is one of the most intensively studied problems in optimization. It is used as a benchmark for many optimization methods. Even though the problem is computationally difficult, a large number of heuristics and exact algorithms are known, so that some instances with tens of thousands of cities can be solved completely and even problems with millions of cities can be approximated within a small fraction of 1%.  The TSP has several applications even in its purest formulation, such as planning, logistics, and the manufacture of microchips. Slightly modified, it appears as a sub-problem in many areas, such as DNA sequencing. In these applications, the concept \"city\" represents, for example, customers,"
    },
    {
        "id": "TASK_hypothesis-testing",
        "name": "Two-sample testing",
        "description": "In statistical hypothesis testing, a two-sample test is a test performed on the data of two random samples, each independently obtained from a different given population. The purpose of the test is to determine whether the difference between these two populations is statistically significant. The statistics used in two-sample tests can be used to solve many machine learning problems, such as domain adaptation, covariate shift and generative adversarial networks.",
        "type": "Task",
        "probable_wikipedia": "Two-sample hypothesis testing",
        "score": "12.364184",
        "probable_description": " In statistical hypothesis testing, a two-sample test is a test performed on the data of two random samples, each independently obtained from a different given population. The purpose of the test is to determine whether the difference between these two populations is statistically significant.  There are a large number of statistical tests that can be used in a two-sample test. Which one(s) are appropriate depend on a variety of factors, such as: "
    },
    {
        "id": "TASK_ultrasound",
        "name": "Ultrasound",
        "description": "",
        "type": "Task",
        "probable_wikipedia": "Ultrasound",
        "score": "10.096208",
        "probable_description": " Ultrasound is sound waves with frequencies higher than the upper audible limit of human hearing. Ultrasound is not different from \"normal\" (audible) sound in its physical properties, except that humans cannot hear it. This limit varies from person to person and is approximately 20 kilohertz (20,000 hertz) in healthy young adults. Ultrasound devices operate with frequencies from 20\u00a0kHz up to several gigahertz.  Ultrasound is used in many different fields. Ultrasonic devices are used to detect objects and measure distances. Ultrasound imaging or sonography is often used in medicine. In the nondestructive testing of products and structures, ultrasound is used to detect invisible flaws. Industrially, ultrasound is used for cleaning, mixing, and accelerating chemical processes. Animals such as bats and porpoises use ultrasound for locating prey and obstacles. Scientists are also studying ultrasound using graphene diaphragms as a method of communication. "
    },
    {
        "id": "TASK_unity",
        "name": "Unity",
        "description": "",
        "type": "Task",
        "probable_wikipedia": "Unity Church",
        "score": "3.5607488",
        "probable_description": " Unity, known informally as Unity Church, is a New Thought Christian organization that publishes the \"Daily Word\" devotional publication. It describes itself as a \"positive, practical Christianity\" which \"teach[es] the effective daily application of the principles of Truth taught and exemplified by Jesus Christ\" and promotes \"a way of life that leads to health, prosperity, happiness, and peace of mind.\" "
    },
    {
        "id": "TASK_variable-selection",
        "name": "Variable Selection",
        "description": "",
        "type": "Task",
        "probable_wikipedia": "Feature selection",
        "score": "6.165832",
        "probable_description": " In machine learning and statistics, feature selection, also known as variable selection, attribute selection or variable subset selection, is the process of selecting a subset of relevant features (variables, predictors) for use in model construction. Feature selection techniques are used for four reasons:  The central premise when using a feature selection technique is that the data contains some features that are either \"redundant\" or \"irrelevant\", and can thus be removed without incurring much loss of information. \"Redundant\" and \"irrelevant\" are two distinct notions, since one relevant feature may be redundant in the presence of another relevant feature with which it is strongly correlated.  Feature selection techniques should be distinguished from feature extraction. Feature extraction creates new features from functions of the original features, whereas feature selection returns a subset of the features. Feature selection techniques are often used in domains where there are many features and comparatively few samples (or data points). Archetypal cases for the application of feature selection include the analysis of written texts and DNA microarray data, where there are many thousands of features, and a few tens to hundreds of samples. "
    },
    {
        "id": "TASK_variational-monte-carlo",
        "name": "Variational Monte Carlo",
        "description": "Variational methods for quantum physics",
        "type": "Task",
        "probable_wikipedia": "Variational Monte Carlo",
        "score": "12.26541",
        "probable_description": " In computational physics, variational Monte Carlo (VMC) is a quantum Monte Carlo method that applies the variational method to approximate the ground state of a quantum system.  The basic building block is a generic wave function formula_1 depending on some parameters formula_2. The optimal values of the parameters formula_2 is then found upon minimizing the total energy of the system.  In particular, given the Hamiltonian formula_4, and denoting with formula_5 a many-body configuration, the expectation value of the energy can be written as :  formula_6  Following the Monte Carlo method for evaluating integrals, we can interpret formula_7 as a probability distribution function, sample it, and evaluate the energy expectation value formula_8 as the average of the so-called local energy formula_9. Once formula_8 is known for a given set of variational parameters formula_2, then optimization is performed in order to minimize the energy and obtain the best possible representation of the ground-state wave-function.  VMC is no different from any other variational method, except that the many-dimensional integrals are evaluated numerically. Monte Carlo integration is particularly crucial in this problem since the dimension of the many-body Hilbert space, comprising all the possible values of the configurations formula_5, typically grows exponentially with the size of the physical system. Other approaches to the numerical evaluation of the energy expectation values would therefore, in general, limit applications to much smaller systems than those analyzable thanks to the Monte Carlo approach.  The accuracy of the method then largely depends on the choice of the variational"
    },
    {
        "id": "TASK_vector-graphics",
        "name": "Vector Graphics",
        "description": "",
        "type": "Task",
        "probable_wikipedia": "Vector graphics",
        "score": "11.487503",
        "probable_description": " Vector graphics are computer graphics images that are defined in terms of 2D points, which are connected by lines and curves to form polygons and other shapes. Each of these points has a definite position on the \"x-\" and \"y-\"axis of the work plane and determines the direction of the path; further, each path may have various properties including values for stroke color, shape, curve, thickness, and fill. Vector graphics are commonly found today in the SVG, EPS, PDF or AI graphic file formats and are intrinsically different from the more common raster graphics file formats of JPEG, PNG, APNG, GIF, and MPEG4. "
    },
    {
        "id": "TASK_video-deinterlacing",
        "name": "Video Deinterlacing",
        "description": "",
        "type": "Task",
        "probable_wikipedia": "Deinterlacing",
        "score": "5.901138",
        "probable_description": " Deinterlacing is the process of converting interlaced video, such as common analog television signals or 1080i format HDTV signals, into a non-interlaced form.  An interlaced video frame consists of two sub-fields taken in sequence, each sequentially scanned at odd, and then even, lines of the image sensor. Analog television employed this technique because it allowed for less transmission bandwidth and further eliminated the perceived flicker that a similar frame rate would give using progressive scan. CRT-based displays were able to display interlaced video correctly due to their complete analogue nature. Newer displays are inherently digital, in that the display comprises discrete pixels. Consequently, the two fields need to be combined into a single frame, which leads to various visual defects. The deinterlacing process should try to minimize these.  Deinterlacing has been researched for decades and employs complex processing algorithms; however, consistent results have been very hard to achieve. "
    },
    {
        "id": "TASK_video-denoising",
        "name": "Video Denoising",
        "description": "",
        "type": "Task",
        "probable_wikipedia": "Video denoising",
        "score": "8.745925",
        "probable_description": " Video denoising is the process of removing noise from a video signal. Video denoising methods can be divided into:  It is done in two areas:  They are chroma and luminance, chroma noise is where one see color fluctuations and luminance is where one see light/dark fluctuations. Generally, the luminance noise looks more like film grain while chroma noise looks more unnatural or digital like.  Video denoising methods are designed and tuned for specific types of noise. Typical video noise types are following:  Different suppression methods are used to remove all these artifacts from video.   "
    },
    {
        "id": "TASK_video-description",
        "name": "Video Description",
        "description": "The goal of automatic **Video Description** is to tell a story about events happening in a video. While early Video Description methods produced captions for short clips that were manually segmented to contain a single event of interest, more recently dense video captioning has been proposed to both segment distinct events in time and describe them in a series of coherent sentences. This problem is a generalization of dense image region captioning and has many practical applications, such as generating textual summaries for the visually impaired, or detecting and describing important events in surveillance footage.\n\n\n<span class=\"description-source\">Source: [Joint Event Detection and Description in Continuous Video Streams ](https://arxiv.org/abs/1802.10250)</span>",
        "type": "Task",
        "probable_wikipedia": "Audio description",
        "score": "5.8601117",
        "probable_description": " Audio description, also referred to as a video description, described video, or more precisely called a visual description, is an additional narration track intended primarily for blind and visually impaired consumers of visual media (including television and film, dance, opera, and visual art). It consists of a narrator talking through the presentation, describing what is happening on the screen or stage during the natural pauses in the audio, and sometimes during dialogue if deemed necessary.  For the performing arts (theater, dance, opera), and media (television, movies and DVD), description is a form of audio-visual translation, using the natural pauses in dialogue or between critical sound elements to insert narrative that translates the visual image into a sense form that is accessible to millions of individuals who otherwise lack full access to television and film. Occasionally when there is dialogue that is in another language from the main one of the film and subtitled on screen, the subtitles are read in character by the describer.  In museums or visual art exhibitions, audio described tours (or universally designed tours that include description or the augmentation of existing recorded programs on audio- or videotape), are used to provide access to visitors who are blind or have low vision. Docents or tour guides can be trained to employ audio description in their presentations. Audio description of sporting events is now becoming more common, in particular at soccer stadiums.  Researchers are working to show how description, through its use of varied word choice, synonyms, metaphor and simile,"
    },
    {
        "id": "TASK_video-games",
        "name": "Video Games",
        "description": "",
        "type": "Task",
        "probable_wikipedia": "Video game",
        "score": "9.696234",
        "probable_description": " A video game is an electronic game that involves interaction with a user interface to generate visual feedback on a two- or three-dimensional video display device such as a TV screen, virtual reality headset or computer monitor. Since the 1980s, video games have become an increasingly important part of the entertainment industry, and whether they are also a form of art is a matter of dispute.  The electronic systems used to play video games are called platforms. Video games are developed and released for one or several platforms and may not be available on others. Specialized platforms such as arcade games, which present the game in a large, typically coin-operated chassis, were common in the 1980s in video arcades, but declined in popularity as other, more affordable platforms became available. These include dedicated devices such as video game consoles, as well as general-purpose computers like a laptop, desktop or handheld computing devices.  The input device used for games, the game controller, varies across platforms. Common controllers include gamepads, joysticks, mouse devices, keyboards, the touchscreens of mobile devices, or even a person's body, using a Kinect sensor. Players view the game on a display device such as a television or computer monitor or sometimes on virtual reality head-mounted display goggles. There are often game sound effects, music and voice actor lines which come from loudspeakers or headphones. Some games in the 2000s include haptic, vibration-creating effects, force feedback peripherals and virtual reality headsets.  In the 2010s, the commercial importance of the video game"
    },
    {
        "id": "TASK_video-inpainting",
        "name": "Video Inpainting",
        "description": "The goal of **Video Inpainting** is to fill in missing regions of a given video sequence with contents that are both spatially and temporally coherent. Video Inpainting, also known as video completion, has many real-world applications such as undesired object removal and video restoration.\r\n\r\n\r\n<span class=\"description-source\">Source: [Deep Flow-Guided Video Inpainting ](https://arxiv.org/abs/1905.02884)</span>",
        "type": "Task",
        "probable_wikipedia": "Inpainting",
        "score": "0.8808869",
        "probable_description": " Inpainting is the process of reconstructing lost or deteriorated parts of images and videos. In the museum world, in the case of a valuable painting, this task would be carried out by a skilled art conservator or art restorer. In the digital world, inpainting (also known as \"image interpolation\" or \"video interpolation\") refers to the application of sophisticated algorithms to replace lost or corrupted parts of the image data (mainly small regions or to remove small defects). "
    },
    {
        "id": "TASK_video-object-tracking",
        "name": "Video Object Tracking",
        "description": "",
        "type": "Task",
        "probable_wikipedia": "Video tracking",
        "score": "6.2015214",
        "probable_description": " Video tracking is the process of locating a moving object (or multiple objects) over time using a camera. It has a variety of uses, some of which are: human-computer interaction, security and surveillance, video communication and compression, augmented reality, traffic control, medical imaging and video editing. Video tracking can be a time consuming process due to the amount of data that is contained in video. Adding further to the complexity is the possible need to use object recognition techniques for tracking, a challenging problem in its own right. "
    },
    {
        "id": "TASK_visual-odometry",
        "name": "Visual Odometry",
        "description": "**Visual Odometry** is an important area of information fusion in which the central aim is to estimate the pose of a robot using data collected by visual sensors.\n\n\n<span class=\"description-source\">Source: [Bi-objective Optimization for Robust RGB-D Visual Odometry ](https://arxiv.org/abs/1411.7445)</span>",
        "type": "Task",
        "probable_wikipedia": "Visual odometry",
        "score": "12.52765",
        "probable_description": " In robotics and computer vision, visual odometry is the process of determining the position and orientation of a robot by analyzing the associated camera images. It has been used in a wide variety of robotic applications, such as on the Mars Exploration Rovers. "
    },
    {
        "id": "TASK_visual-reasoning",
        "name": "Visual Reasoning",
        "description": "Ability to understand  actions and reasoning  associated with any  visual images",
        "type": "Task",
        "probable_wikipedia": "Visual reasoning",
        "score": "10.917607",
        "probable_description": " Visual reasoning is the process of manipulating one's mental image of an object in order to reach a certain conclusion \u2013 for example, mentally constructing a piece of machinery to experiment with different mechanisms. In a frequently cited paper in the journal \"Science\" and a later book Eugene S. Ferguson, a mechanical engineer and historian of technology, claims that visual reasoning is a widely used tool used in creating technological artefacts. There is ample evidence that visual methods, particularly drawing, play a central role in creating artefacts. Ferguson's visual reasoning also has parallels in philosopher David Gooding's argument that experimental scientists work with a combination of action, instruments, objects and procedures as well as words. That is, with a significant non-verbal component.  Ferguson argues that non-verbal (largely visual) reasoning does not get much attention in areas like history of technology and philosophy of science because the people involved are verbal rather than visual thinkers.  Those who use visual reasoning, notably architects, designers, engineers, and certain mathematicians conceive and manipulate objects in \"the mind's eye\" before putting them on paper. Having done this the paper or computer versions (in CAD) can be manipulated by metaphorically \"building\" the object on paper (or computer) before building it physically.  Nicola Tesla claimed that the first alternating current motor he built ran perfectly because he had visualized and \"run\" models of it in his mind before building the prototype.   "
    },
    {
        "id": "DATASET_vist",
        "name": "VIST",
        "full_name": "Visual Storytelling",
        "url": "https://visionandlanguage.net/VIST/dataset.html",
        "proposed_in": {
            "paper_id": "visual-storytelling",
            "s2_paper_id": "927987a48c2a519bbc097d8b6c925b64a85b7d8e"
        },
        "abstract": "We introduce the first dataset for sequential vision-to-language, and explore how this data may be used for the task of visual storytelling. The first release of this dataset, SIND1 v.1, includes 81,743 unique photos in 20,211 sequences, aligned to both descriptive (caption) and story language. We establish several strong baselines for the storytelling task, and motivate an automatic metric to benchmark progress. Modelling concrete description as well as figurative and social language, as provided in this dataset and the storytelling task, has the potential to move artificial intelligence from basic understandings of typical visual scenes towards more and more human-like understanding of grounded event structure and subjective expression.",
        "type": "Dataset",
        "probable_wikipedia": "Visual narrative",
        "score": "6.989423",
        "probable_description": " A visual narrative (also visual storytelling) is a story told primarily through the use of visual media. The story may be told using still photography, illustration, or video, and can be enhanced with graphics, music, voice and other audio. "
    },
    {
        "id": "TASK_weather-forecasting",
        "name": "Weather Forecasting",
        "description": "**Weather Forecasting** is the prediction of future weather conditions such as precipitation, temperature, pressure and wind.\n\n\n<span class=\"description-source\">Source: [MetNet: A Neural Weather Model for Precipitation Forecasting ](https://arxiv.org/abs/2003.12140)</span>",
        "type": "Task",
        "probable_wikipedia": "Weather forecasting",
        "score": "12.189463",
        "probable_description": " Weather forecasting is the application of science and technology to predict the conditions of the atmosphere for a given location and time. People have attempted to predict the weather informally for millennia and formally since the 19th century. Weather forecasts are made by collecting quantitative data about the current state of the atmosphere at a given place and using meteorology to project how the atmosphere will change.  Once calculated by hand based mainly upon changes in barometric pressure, current weather conditions, and sky condition or cloud cover, weather forecasting now relies on computer-based models that take many atmospheric factors into account. Human input is still required to pick the best possible forecast model to base the forecast upon, which involves pattern recognition skills, teleconnections, knowledge of model performance, and knowledge of model biases. The inaccuracy of forecasting is due to the chaotic nature of the atmosphere, the massive computational power required to solve the equations that describe the atmosphere, the error involved in measuring the initial conditions, and an incomplete understanding of atmospheric processes. Hence, forecasts become less accurate as the difference between current time and the time for which the forecast is being made (the \"range\" of the forecast) increases. The use of ensembles and model consensus help narrow the error and pick the most likely outcome.  There are a variety of end uses to weather forecasts. Weather warnings are important forecasts because they are used to protect life and property. Forecasts based on temperature and precipitation are important to agriculture, and"
    },
    {
        "id": "TASK_window-detection",
        "name": "Window Detection",
        "description": "",
        "type": "Task",
        "probable_wikipedia": "Window detector",
        "score": "2.3002968",
        "probable_description": " A window detector circuit, also called window comparator circuit or dual edge limit detector circuits is used to determine whether an unknown input is between two precise reference threshold voltages. It employs two comparators to detect over-voltage or under-voltage.  Each single comparator detects the common input voltage against one of two reference voltages, normally upper and lower limits. Outputs behind a logic gate like AND detect the input as in range of the so-called \"window\" between upper and lower reference.  Window detectors are used in industrial alarms, level sensor and controls, digital computers and production-line testing. "
    },
    {
        "id": "TASK_word-alignment",
        "name": "Word Alignment",
        "description": "**Word Alignment** is the task of finding the correspondence between source and target words in a pair of sentences that are translations of each other.\n\n\n<span class=\"description-source\">Source: [Neural Network-based Word Alignment through Score Aggregation ](https://arxiv.org/abs/1606.09560)</span>",
        "type": "Task",
        "probable_wikipedia": "Bitext word alignment",
        "score": "6.6984982",
        "probable_description": " Bitext word alignment or simply word alignment is the natural language processing task of identifying translation relationships among the words (or more rarely multiword units) in a bitext, resulting in a bipartite graph between the two sides of the bitext, with an arc between two words if and only if they are translations of one another. Word alignment is typically done after sentence alignment has already identified pairs of sentences that are translations of one another.  Bitext word alignment is an important supporting task for most methods of statistical machine translation. The parameters of statistical machine translation models are typically estimated by observing word-aligned bitexts, and conversely automatic word alignment is typically done by choosing that alignment which best fits a statistical machine translation model. Circular application of these two ideas results in an instance of the expectation-maximization algorithm.  This approach to training is an instance of unsupervised learning, in that the system is not given examples of the kind of output desired, but is trying to find values for the unobserved model and alignments which best explain the observed bitext. Recent work has begun to explore supervised methods which rely on presenting the system with a (usually small) number of manually aligned sentences. In addition to the benefit of the additional information provided by supervision, these models are typically also able to more easily take advantage of combining many features of the data, such as context, syntactic structure, part-of-speech, or translation lexicon information, which are difficult to integrate into the generative statistical"
    },
    {
        "id": "TASK_word-embeddings",
        "name": "Word Embeddings",
        "description": "Word embedding is the collective name for a set of language modeling and feature learning techniques in natural language processing (NLP) where words or phrases from the vocabulary are mapped to vectors of real numbers.\r\n\r\nTechniques for learning word embeddings can include  Word2Vec, GloVe, and other neural network-based approaches that train on an NLP task such as language modeling or document classification.\r\n\r\n<span style=\"color:grey; opacity: 0.6\">( Image credit: [Dynamic Word Embedding for Evolving Semantic Discovery](https://arxiv.org/pdf/1703.00607v2.pdf) )</span>",
        "type": "Task",
        "probable_wikipedia": "Word embedding",
        "score": "12.096225",
        "probable_description": " Word embedding is the collective name for a set of language modeling and feature learning techniques in natural language processing (NLP) where words or phrases from the vocabulary are mapped to vectors of real numbers. Conceptually it involves a mathematical embedding from a space with many dimensions per word to a continuous vector space with a much lower dimension.  Methods to generate this mapping include neural networks, dimensionality reduction on the word co-occurrence matrix, probabilistic models, explainable knowledge base method, and explicit representation in terms of the context in which words appear.  Word and phrase embeddings, when used as the underlying input representation, have been shown to boost the performance in NLP tasks such as syntactic parsing and sentiment analysis. "
    },
    {
        "id": "TASK_word-sense-disambiguation",
        "name": "Word Sense Disambiguation",
        "description": "The task of Word Sense Disambiguation (WSD) consists of associating words in context with their most suitable entry in a pre-defined sense inventory. The de-facto sense inventory for English in WSD is [WordNet](https://wordnet.princeton.edu).\nFor example, given the word \u201cmouse\u201d and the following sentence:\n\n\u201cA mouse consists of an object held in one's hand, with one or more buttons.\u201d \n\nwe would assign \u201cmouse\u201d  with its electronic device sense ([the 4th sense in the WordNet sense inventory](http://wordnetweb.princeton.edu/perl/webwn?c=8&sub=Change&o2=&o0=1&o8=1&o1=1&o7=&o5=&o9=&o6=&o3=&o4=&i=-1&h=000000&s=mouse)).",
        "type": "Task",
        "probable_wikipedia": "Word-sense disambiguation",
        "score": "10.885596",
        "probable_description": " In computational linguistics, word-sense disambiguation (WSD) is an open problem concerned with identifying which sense of a word is used in a sentence. The solution to this problem impacts other computer-related writing, such as discourse, improving relevance of search engines, anaphora resolution, coherence, inference.  The human brain is quite proficient at word-sense disambiguation. That natural language is formed in a way that requires so much of it is a reflection of that neurologic reality. In other words, human language developed in a way that reflects (and also has helped to shape) the innate ability provided by the brain's neural networks. In computer science and the information technology that it enables, it has been a long-term challenge to develop the ability in computers to do natural language processing and machine learning.  A rich variety of techniques have been researched, from dictionary-based methods that use the knowledge encoded in lexical resources, to supervised machine learning methods in which a classifier is trained for each distinct word on a corpus of manually sense-annotated examples, to completely unsupervised methods that cluster occurrences of words, thereby inducing word senses. Among these, supervised learning approaches have been the most successful algorithms to date.  Accuracy of current algorithms is difficult to state without a host of caveats. In English, accuracy at the coarse-grained (homograph) level is routinely above 90%, with some methods on particular homographs achieving over 96%. On finer-grained sense distinctions, top accuracies from 59.1% to 69.0% have been reported in evaluation exercises (SemEval-2007, Senseval-2), where the baseline"
    },
    {
        "id": "TASK_word-sense-induction",
        "name": "Word Sense Induction",
        "description": "Word sense induction (WSI) is widely known as the \u201cunsupervised version\u201d of WSD. The problem states as: Given a target word (e.g., \u201ccold\u201d) and a collection of sentences (e.g., \u201cI caught a cold\u201d, \u201cThe weather is cold\u201d) that use the word, cluster the sentences according to their different senses/meanings. We do not need to know the sense/meaning of each cluster, but sentences inside a cluster should have used the target words with the same sense.\r\n\r\nDescription from [NLP Progress](http://nlpprogress.com/english/word_sense_disambiguation.html)",
        "type": "Task",
        "probable_wikipedia": "Word-sense induction",
        "score": "9.617561",
        "probable_description": " In computational linguistics, word-sense induction (WSI) or discrimination is an open problem of natural language processing, which concerns the automatic identification of the senses of a word (i.e. meanings). Given that the output of word-sense induction is a set of senses for the target word (sense inventory), this task is strictly related to that of word-sense disambiguation (WSD), which relies on a predefined sense inventory and aims to solve the ambiguity of words in context. "
    },
    {
        "id": "TASK_yield-mapping-in-apple-orchards",
        "name": "Yield Mapping In Apple Orchards",
        "description": "",
        "type": "Task",
        "probable_wikipedia": "Yield mapping",
        "score": "4.9574394",
        "probable_description": " Yield mapping or yield monitoring is a technique in agriculture of using GPS data to analyze variables such as crop yield and moisture content in a given field. It was developed in the 1990s and uses a combination of GPS technology and physical sensors, such as speedometers, to track crop yields, grain elevator speed, and combine speed.  This data produces a yield map that can be used to compare yield distribution within the field from year to year. This allows farmers to determine areas of the field that, for example, may need to be more heavily irrigated or are not yielding any crop at all. It also allows farmers to show the effects of a change in field-management techniques, to develop nutrient strategies for their fields, and as a record of crop yield to use in securing loans or renters.  "
    }
]